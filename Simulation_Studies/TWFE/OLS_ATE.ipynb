{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_did_data(\n",
    "    n_units=200,\n",
    "    num_x_covariates=5,\n",
    "    num_pre_periods=5,\n",
    "    num_post_periods=5,\n",
    "    linearity_degree=1, # 1: fully linear, 2: half X non-linear, 3: treatment + all X non-linear\n",
    "    pre_trend_bias_delta=0.2,\n",
    "    epsilon_scale=1,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates panel data for Difference-in-Differences analysis with controllable pre-trends and non-linearity.\n",
    "\n",
    "    Args:\n",
    "        n_units (int): Number of units (e.g., individuals, firms).\n",
    "        num_x_covariates (int): Number of control covariates (X) (not counting the two covariates W1 and W2 where W1 ~ Bernoulli(0.66)\n",
    "        and W2 takes the values 1,2,3,4 with the following probabilities 0.3, 0.1, 0.2, 0.4 respectively.\n",
    "        num_pre_periods (int): Number of periods before treatment.\n",
    "        num_post_periods (int): Number of periods after treatment.\n",
    "        treatment_effect_beta (float): True treatment effect size.\n",
    "        linearity_degree (int): Degree of linearity in the DGP:\n",
    "            1: Fully linear.\n",
    "            2: Half of X covariates have non-linear relationship with Y.\n",
    "            3: Treatment and all X covariates have non-linear relationship with Y.\n",
    "        pre_trend_bias_delta (float): Bias parameter to induce pre-trends in the treated group.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Generated panel data in long format.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # --- Set treatment_effect_beta based on linearity_degree (like R code) ---\n",
    "    if linearity_degree == 1 or linearity_degree == 2:\n",
    "        treatment_effect_beta = 3\n",
    "    elif linearity_degree == 3:\n",
    "        treatment_effect_beta = 5\n",
    "    else:\n",
    "        # Handle cases where linearity_degree is not 1, 2, or 3\n",
    "        print(f\"Warning: linearity_degree ({linearity_degree}) has an unexpected value. Setting treatment_effect_beta to NaN.\")\n",
    "        treatment_effect_beta = np.nan\n",
    "\n",
    "    periods = num_pre_periods + num_post_periods\n",
    "    unit_ids = range(n_units)\n",
    "    time_periods = range(periods)\n",
    "\n",
    "    # Create base data frame\n",
    "    data = pd.DataFrame({\n",
    "        'unit_id': np.repeat(unit_ids, periods),\n",
    "        'time': np.tile(time_periods, n_units)\n",
    "    })\n",
    "\n",
    "    # Treatment assignment (randomly assign half to treatment)\n",
    "    treated_units = np.random.choice(unit_ids, size=n_units // 2, replace=False) #TO DO: add more complex propensity score\n",
    "    data['treated_group'] = np.where(data['unit_id'].isin(treated_units), 1, 0)\n",
    "\n",
    "    # Time indicators\n",
    "    treatment_period = num_pre_periods # Period when treatment starts\n",
    "    data['post_treatment'] = np.where(data['time'] >= treatment_period, 1, 0)\n",
    "    data['time_trend'] = data['time'] # Simple linear time trend\n",
    "\n",
    "    X = np.random.normal(0, 1, size=(len(data), num_x_covariates))\n",
    "\n",
    "\n",
    "    # Add Bernoulli random variable with p=0.66\n",
    "    bernoulli_values = np.random.binomial(n=1, p=0.66, size=len(data))\n",
    "    # Add to both X matrix and dataframe\n",
    "    X = np.column_stack((bernoulli_values,X))\n",
    "\n",
    "   # Add categorical variable with values 1,2,3,4 with probabilities 0.3, 0.1, 0.2, 0.4\n",
    "    categories = [1, 2, 3, 4]\n",
    "    probabilities = [0.3, 0.1, 0.2, 0.4]\n",
    "    categorical_values = np.random.choice(categories, size=len(data), p=probabilities)\n",
    "    # Add to both X matrix and dataframe\n",
    "    X = np.column_stack((X, categorical_values))\n",
    "\n",
    "    for i in range(num_x_covariates+2):\n",
    "        data[f'X_{i+1}'] = X[:,i]\n",
    "\n",
    "    # Generate error term\n",
    "    data['epsilon'] = np.random.normal(scale=epsilon_scale,size=len(data))\n",
    "\n",
    "    # DGP parameters (can be adjusted for more complex DGPs)\n",
    "    beta_0 = -0.5 # Intercept\n",
    "    beta_treated = 0.75 # Main effect of treated group (alpha_i)\n",
    "    beta_time = 0.2 # Main effect of time trend (gamma_t)\n",
    "    beta_interaction = treatment_effect_beta # Treatment effect\n",
    "    beta_x = np.array([-0.75, 0.5, -0.5, -1.30, 1.8, 2.5, -1.0])\n",
    "\n",
    "\n",
    "    # Non-linear components based on linearity_degree\n",
    "    if linearity_degree == 1: # Half covariates non-linear\n",
    "        linear_x_contribution = np.sum([beta_x[i] * data[f'X_{i+1}'] for i in range(num_x_covariates+2)], axis=0)\n",
    "        data['Y'] = beta_0 + beta_treated * data['treated_group'] + beta_time * data['time_trend']+linear_x_contribution+ beta_interaction * data['treated_group'] * data['post_treatment']\n",
    "        data['CATE'] = beta_interaction * data['treated_group'] * data['post_treatment']\n",
    "\n",
    "    elif linearity_degree == 2: # Half covariates non-linear\n",
    "        half = num_x_covariates+2 // 2\n",
    "        cov_effect = (np.sum(beta_x[:int(half/2)] * (X[:, :int(half/2)] ** 2),axis=1) + np.sum(beta_x[int(half/2):half] * np.exp(X[:, int(half/2):half]),axis=1)+\n",
    "                              np.sum(beta_x[half:] * X[:, half:],axis=1))\n",
    "        data['Y'] = beta_0 + beta_treated * data['treated_group'] + beta_time * data['time_trend']+cov_effect+beta_interaction * data['treated_group'] * data['post_treatment']\n",
    "        data['CATE'] = beta_interaction * data['treated_group'] * data['post_treatment']\n",
    "\n",
    "    elif linearity_degree == 3: \n",
    "        half = num_x_covariates+2 // 2\n",
    "        cov_effect = (np.sum(beta_x[:int(half/2)] * (X[:, :int(half/2)] ** 2),axis=1) + np.sum(beta_x[int(half/2):half] * np.exp(X[:, int(half/2):half]),axis=1)+\n",
    "                              np.sum(beta_x[half:half+int(half/2)] * np.abs(X[:, half:half+int(half/2)]),axis=1) + np.sum(beta_x[half+int(half/2):] * np.sqrt(np.abs(X[:, half+int(half/2):])),axis=1))\n",
    "        data['Y'] = beta_0 + beta_treated * data['treated_group'] + beta_time * data['time_trend']**2+cov_effect+beta_interaction * data['treated_group'] * data['post_treatment']\n",
    "        data['CATE'] = beta_interaction * data['treated_group'] * data['post_treatment']\n",
    "\n",
    "\n",
    "    # Add pre-trend bias (differential trend for treated group in pre-treatment)\n",
    "    if pre_trend_bias_delta != 0:\n",
    "        if linearity_degree == 3:\n",
    "            # Example parameters for seasonality\n",
    "            seasonal_amplitude = 1.0  # Amplitude of the seasonal effect\n",
    "            seasonal_period = 4      # Period of the seasonal effect (e.g., 12 for monthly data)\n",
    "\n",
    "            # Calculate the seasonal effect\n",
    "            seasonal_effect = seasonal_amplitude * np.sin(2 * np.pi * data['time'] / seasonal_period)\n",
    "            data['Y'] += pre_trend_bias_delta * data['treated_group'] * seasonal_effect\n",
    "        else:\n",
    "            data['Y'] += pre_trend_bias_delta * data['treated_group'] * (data['time'] - treatment_period)\n",
    "        # (data['time'] - treatment_period) will be negative in pre-treatment, 0 at treatment period, and positive in post-treatment.\n",
    "        # (1 - data['post_treatment']) ensures this bias only applies in pre-treatment periods.\n",
    "\n",
    "\n",
    "    # Add error term\n",
    "    data['Y'] += data['epsilon']\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_p_values(model_complex, num_pre_periods):\n",
    "    \"\"\"\n",
    "    Extracts p-values for interaction terms in a statsmodels model,\n",
    "    counts the number of non-significant p-values, and returns them.\n",
    "\n",
    "    Args:\n",
    "        model_complex:  A statsmodels model results object.\n",
    "        num_pre_periods: The number of pre-treatment periods in the model.\n",
    "                       Determines the range of the loop.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - A list of extracted p-values.\n",
    "            - The count of p-values greater than 0.05.\n",
    "    \"\"\"\n",
    "\n",
    "    p_values = []\n",
    "    for i in range(1, num_pre_periods):  # Loop from 1 to num_pre_periods - 1\n",
    "        parameter_name = f\"C(time)[T.{i}]:treated_group\"  # Construct the parameter name dynamically\n",
    "        pval = model_complex.pvalues.get(parameter_name, np.nan)\n",
    "        p_values.append(pval)\n",
    "\n",
    "    count_non_significant = sum(pval > 0.05 for pval in p_values)  # Use a generator expression for efficiency\n",
    "\n",
    "    return count_non_significant\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_ate_parameters(model_complex, num_pre_periods, num_post_periods, estimated_ATE,iteration):\n",
    "    \"\"\"\n",
    "    Accumulates parameter estimates from a statsmodels model into an existing numpy array.\n",
    "\n",
    "    Args:\n",
    "        model_complex: A statsmodels model results object.\n",
    "        num_pre_periods: The number of pre-treatment periods.\n",
    "        num_post_periods: The number of post-treatment periods.\n",
    "        estimated_ATE: A numpy array of shape (num_post_periods, 100) to accumulate parameters into.\n",
    "                       Assumes this array has already been initialized with zeros.  This is crucial!\n",
    "    \"\"\"\n",
    "\n",
    "    for j in range(num_pre_periods, num_pre_periods + num_post_periods):  # Loop from num_pre to num_pre + num_post - 1\n",
    "        parameter_name = f\"C(time)[T.{j}]:treated_group\"\n",
    "        try:\n",
    "            parameter_value = model_complex.params[parameter_name]\n",
    "        except KeyError:\n",
    "            parameter_value = np.nan  # Handle missing parameters robustly\n",
    "\n",
    "        post_period_index = j - num_pre_periods  # Calculate the correct row index for estimated_ATE\n",
    "\n",
    "        # Check for nan before assignment. If it's nan, just move on to the next parameter.\n",
    "        if np.isnan(parameter_value):\n",
    "            print(f\"Warning: Parameter {parameter_name} is NaN. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Assign parameter value to all columns of the corresponding row\n",
    "        estimated_ATE[post_period_index, iteration] = parameter_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_ate_pvalues(model_complex, num_pre_periods, num_post_periods, accumulated_p_values, iteration):\n",
    "    \"\"\"\n",
    "    Accumulates p-values for ATE parameters from a statsmodels model into an existing numpy array.\n",
    "\n",
    "    Args:\n",
    "        model_complex: A statsmodels model results object.\n",
    "        num_pre_periods: The number of pre-treatment periods.\n",
    "        num_post_periods: The number of post-treatment periods.\n",
    "        accumulated_p_values: A numpy array of shape (num_post_periods, 100) to accumulate p-values into.\n",
    "                              Assumes this array has already been initialized with zeros or NaNs.\n",
    "        iteration: The current iteration number (column index in accumulated_p_values).\n",
    "    \"\"\"\n",
    "    for j in range(num_pre_periods, num_pre_periods + num_post_periods):  # Loop from num_pre to num_pre + num_post - 1\n",
    "        parameter_name = f\"C(time)[T.{j}]:treated_group\"\n",
    "        try:\n",
    "            # Access p-values instead of parameters\n",
    "            p_value = model_complex.pvalues[parameter_name]\n",
    "        except KeyError:\n",
    "            p_value = np.nan  # Handle missing parameters robustly\n",
    "        post_period_index = j - num_pre_periods  # Calculate the correct row index for accumulated_p_values\n",
    "\n",
    "        # Check for nan before assignment. If it's nan, just move on to the next parameter.\n",
    "        if np.isnan(p_value):\n",
    "            print(f\"Warning: P-value for parameter {parameter_name} is NaN. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Assign p-value to the specified column (iteration) and row (post-period)\n",
    "        accumulated_p_values[post_period_index, iteration] = p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_metrics(estimated_ATE, accumulated_p_values, true_ATE=0.5, suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Calculates RMSE, MAE, and MAPE for the estimated ATE values,\n",
    "    both overall and per time period (row).  Adds columns for each simulation's p-values\n",
    "    directly to the output CSV.\n",
    "\n",
    "    Args:\n",
    "        estimated_ATE: A numpy array of shape (num_post_periods, num_simulations)\n",
    "                       containing the estimated ATE values.\n",
    "        accumulated_p_values: A numpy array of shape (num_post_periods, num_simulations)\n",
    "                              containing the accumulated p-values.\n",
    "        true_ATE: The true ATE value (default: 0.5).\n",
    "        suffix: A suffix to add to the output CSV filename.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - overall_metrics: A dictionary containing overall RMSE, MAE, and MAPE for ATE.\n",
    "            - per_time_period_metrics: A dictionary where keys are time period indices\n",
    "              and values are dictionaries containing RMSE, MAE, and MAPE for ATE for that time period.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Calculate Metrics for Estimated ATE ---\n",
    "    # Overall metrics for ATE\n",
    "    overall_rmse_ate = np.sqrt(mean_squared_error(true_ATE * np.ones(estimated_ATE.size), estimated_ATE.flatten()))\n",
    "    overall_mae_ate = mean_absolute_error(true_ATE * np.ones(estimated_ATE.size), estimated_ATE.flatten())\n",
    "    overall_mape_ate = np.mean(np.abs((estimated_ATE.flatten() - true_ATE) / true_ATE)) * 100 if true_ATE != 0 else np.nan\n",
    "    std_rmse_ate = np.std(np.sqrt(np.mean((estimated_ATE-true_ATE)**2, axis=0)).flatten())\n",
    "    std_mae_ate = np.std(np.mean(np.abs(estimated_ATE-true_ATE), axis=0).flatten())\n",
    "    std_mape_ate = np.std(np.mean(np.abs((estimated_ATE - true_ATE) / true_ATE), axis=0).flatten() * 100) if true_ATE != 0 else np.nan\n",
    "\n",
    "\n",
    "    overall_metrics_ate = {\n",
    "        \"ATE_rmse\": overall_rmse_ate,\n",
    "        \"ATE_mae\": overall_mae_ate,\n",
    "        \"ATE_mape\": overall_mape_ate,\n",
    "        \"ATE_std_rmse\": std_rmse_ate,\n",
    "        \"ATE_std_mae\": std_mae_ate,\n",
    "        \"ATE_std_mape\": std_mape_ate\n",
    "    }\n",
    "\n",
    "    # Per-time-period metrics for ATE\n",
    "    per_time_period_metrics_ate = {}\n",
    "    for i in range(estimated_ATE.shape[0]):  # Iterate over rows (time periods)\n",
    "        rmse_ate = np.sqrt(mean_squared_error(true_ATE * np.ones(estimated_ATE[i,:].size), estimated_ATE[i,:]))\n",
    "        mae_ate = mean_absolute_error(true_ATE * np.ones(estimated_ATE[i,:].size), estimated_ATE[i,:])\n",
    "        mape_ate = np.mean(np.abs((estimated_ATE[i,:] - true_ATE) / true_ATE)) * 100 if true_ATE != 0 else np.nan\n",
    "        std_mse_ate = np.std((estimated_ATE[i,:]-true_ATE)**2)\n",
    "        std_mae_ate = np.std(np.abs(estimated_ATE[i,:]-true_ATE).flatten())\n",
    "        std_mape_ate = np.std(np.abs((estimated_ATE[i,:] - true_ATE) / true_ATE).flatten() * 100) if true_ATE != 0 else np.nan\n",
    "\n",
    "\n",
    "        per_time_period_metrics_ate[i] = {\n",
    "            \"ATE_rmse\": rmse_ate,\n",
    "            \"ATE_mae\": mae_ate,\n",
    "            \"ATE_mape\": mape_ate,\n",
    "            \"ATE_std_mse\": std_mse_ate,\n",
    "            \"ATE_std_mae\": std_mae_ate,\n",
    "            \"ATE_std_mape\": std_mape_ate\n",
    "        }\n",
    "\n",
    "\n",
    "    # Calculate the three specific metrics to save to CSV for ATE\n",
    "    rmse_values_ate = np.sqrt(np.mean((estimated_ATE-true_ATE)**2, axis=0)).flatten()\n",
    "    mae_values_ate = np.mean(np.abs(estimated_ATE-true_ATE), axis=0).flatten()\n",
    "    mape_values_ate = np.mean(np.abs((estimated_ATE - true_ATE) / true_ATE), axis=0).flatten() * 100 if true_ATE != 0 else np.nan\n",
    "\n",
    "\n",
    "    # Create DataFrame with the three rows for ATE metrics\n",
    "    df_ate_metrics = pd.DataFrame({\n",
    "        'ATE_RMSE': rmse_values_ate,\n",
    "        'ATE_MAE': mae_values_ate,\n",
    "        'ATE_MAPE': mape_values_ate,\n",
    "    }).T  # Transpose to have metrics as rows\n",
    "\n",
    "    # Create DataFrame for accumulated p-values, using column names like 'PValue_0', 'PValue_1', ...\n",
    "    p_value_columns = [f'PValue_{i}' for i in range(accumulated_p_values.shape[0])] # column names for p-values\n",
    "    df_p_values = pd.DataFrame(accumulated_p_values.T, columns=p_value_columns).T # Transpose so rows are periods, columns are iterations\n",
    "\n",
    "    df = pd.concat([df_ate_metrics, df_p_values], axis=0)\n",
    "\n",
    "    # Save to CSV with the specified filename format\n",
    "    filename = f\"OLS_ATE_and_PValues{suffix}.csv\" # Changed filename\n",
    "    df.to_csv(filename, header=False)\n",
    "\n",
    "    # Return only ATE metrics\n",
    "    return overall_metrics_ate, per_time_period_metrics_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/100 [00:00<?, ?iteration/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 100/100 [00:07<00:00, 14.02iteration/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations (out of 100) with at least two p-values above 0.05 (i.e., we assume that the Parallel Trend Assumption holds): 98\n",
      "Overall Metrics:\n",
      "  ATE_rmse: 0.5596\n",
      "  ATE_mae: 0.5275\n",
      "  ATE_mape: 17.5838\n",
      "  ATE_std_rmse: 0.1355\n",
      "  ATE_std_mae: 0.1374\n",
      "  ATE_std_mape: 4.5811\n",
      "\n",
      "Per Time Period Metrics:\n",
      "  Time Period 4:\n",
      "    ATE_rmse: 0.5549\n",
      "    ATE_mae: 0.5241\n",
      "    ATE_mape: 17.4703\n",
      "    ATE_std_mse: 0.1976\n",
      "    ATE_std_mae: 0.1822\n",
      "    ATE_std_mape: 6.0723\n",
      "  Time Period 5:\n",
      "    ATE_rmse: 0.5667\n",
      "    ATE_mae: 0.5359\n",
      "    ATE_mape: 17.8623\n",
      "    ATE_std_mse: 0.2148\n",
      "    ATE_std_mae: 0.1845\n",
      "    ATE_std_mape: 6.1498\n",
      "  Time Period 6:\n",
      "    ATE_rmse: 0.5675\n",
      "    ATE_mae: 0.5313\n",
      "    ATE_mape: 17.7105\n",
      "    ATE_std_mse: 0.2143\n",
      "    ATE_std_mae: 0.1994\n",
      "    ATE_std_mape: 6.6454\n",
      "  Time Period 7:\n",
      "    ATE_rmse: 0.5491\n",
      "    ATE_mae: 0.5188\n",
      "    ATE_mape: 17.2921\n",
      "    ATE_std_mse: 0.2033\n",
      "    ATE_std_mae: 0.1801\n",
      "    ATE_std_mape: 6.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the number of covariates as specified.\n",
    "num_x_covariates = 5\n",
    "linearity_degree=1\n",
    "\n",
    "# Set the number of iterations and initialize the counter.\n",
    "num_iterations = 100\n",
    "count_at_least_two_non_significant = 0\n",
    "\n",
    "num_pre_periods=4\n",
    "\n",
    "num_post_periods=4\n",
    "\n",
    "estimated_ATE=np.zeros([num_post_periods,num_iterations])\n",
    "accumulated_p_values=np.zeros([num_post_periods,num_iterations])\n",
    "\n",
    "epsilon_scale=1\n",
    "\n",
    "iterations_PTA=[]\n",
    "# Run the loop 100 times.\n",
    "for i in tqdm(range(num_iterations), desc=\"Progress\", unit=\"iteration\"):\n",
    "    # Generate a random seed for each iteration.\n",
    "    seed_val = i\n",
    "    \n",
    "    # Generate data with specified hyperparameters.\n",
    "    data_linear = generate_did_data(\n",
    "        linearity_degree=linearity_degree,\n",
    "        num_pre_periods=num_pre_periods,\n",
    "        num_post_periods=num_post_periods,\n",
    "        pre_trend_bias_delta=0,\n",
    "        num_x_covariates=num_x_covariates,\n",
    "        epsilon_scale=epsilon_scale,\n",
    "        seed=seed_val\n",
    "    )\n",
    "    \n",
    "    # Define the complex regression formula.\n",
    "    formula_complex = \"Y ~ treated_group + C(time) + C(time):treated_group + X_1 + X_2+X_3+X_4+ X_5 + X_6+X_7\"\n",
    "    \n",
    "    # Fit the model.\n",
    "    model_complex = smf.ols(formula=formula_complex, data=data_linear).fit()\n",
    "\n",
    "    \n",
    "    # Count how many of these p-values are above 0.05.\n",
    "    count_non_significant = analyze_p_values(model_complex, num_pre_periods)\n",
    "\n",
    "    accumulate_ate_parameters(model_complex, num_pre_periods, num_post_periods, estimated_ATE, i)\n",
    "    accumulate_ate_pvalues(model_complex, num_pre_periods, num_post_periods, accumulated_p_values, i)\n",
    "    \n",
    "    # If at least two p-values are above 0.05, increment the counter.\n",
    "    if count_non_significant >= 2:\n",
    "        count_at_least_two_non_significant += 1\n",
    "        iterations_PTA.append(i)\n",
    "\n",
    "if linearity_degree == 1 or linearity_degree == 2:\n",
    "    true_ATE = 3.0\n",
    "elif linearity_degree == 3:\n",
    "    true_ATE = 5.0\n",
    "\n",
    "overall_metrics, per_time_period_metrics = calculate_error_metrics(estimated_ATE,accumulated_p_values, true_ATE,\"_ATE_linearity=1\")\n",
    "\n",
    "print(\"Number of iterations (out of 100) with at least two p-values above 0.05 (i.e., we assume that the Parallel Trend Assumption holds):\", \n",
    "      count_at_least_two_non_significant)\n",
    "\n",
    "print(\"Overall Metrics:\")\n",
    "for metric, value in overall_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nPer Time Period Metrics:\")\n",
    "for time_period, metrics in per_time_period_metrics.items():\n",
    "    print(f\"  Time Period {time_period+num_pre_periods}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"    {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/100 [00:00<?, ?iteration/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 100/100 [00:05<00:00, 17.06iteration/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations (out of 100) with at least two p-values above 0.05 (i.e., we assume that the Parallel Trend Assumption holds): 98\n",
      "Overall Metrics:\n",
      "  ATE_rmse: 1.0243\n",
      "  ATE_mae: 0.8269\n",
      "  ATE_mape: 27.5623\n",
      "  ATE_std_rmse: 0.4133\n",
      "  ATE_std_mae: 0.3979\n",
      "  ATE_std_mape: 13.2642\n",
      "\n",
      "Per Time Period Metrics:\n",
      "  Time Period 4:\n",
      "    ATE_rmse: 1.0449\n",
      "    ATE_mae: 0.8087\n",
      "    ATE_mape: 26.9568\n",
      "    ATE_std_mse: 1.6951\n",
      "    ATE_std_mae: 0.6616\n",
      "    ATE_std_mape: 22.0544\n",
      "  Time Period 5:\n",
      "    ATE_rmse: 0.9834\n",
      "    ATE_mae: 0.7978\n",
      "    ATE_mape: 26.5933\n",
      "    ATE_std_mse: 1.2301\n",
      "    ATE_std_mae: 0.5749\n",
      "    ATE_std_mape: 19.1641\n",
      "  Time Period 6:\n",
      "    ATE_rmse: 1.1059\n",
      "    ATE_mae: 0.9135\n",
      "    ATE_mape: 30.4490\n",
      "    ATE_std_mse: 1.4799\n",
      "    ATE_std_mae: 0.6233\n",
      "    ATE_std_mape: 20.7767\n",
      "  Time Period 7:\n",
      "    ATE_rmse: 0.9565\n",
      "    ATE_mae: 0.7875\n",
      "    ATE_mape: 26.2503\n",
      "    ATE_std_mse: 1.1633\n",
      "    ATE_std_mae: 0.5428\n",
      "    ATE_std_mape: 18.0946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the number of covariates as specified.\n",
    "num_x_covariates = 5\n",
    "linearity_degree=2\n",
    "\n",
    "# Set the number of iterations and initialize the counter.\n",
    "num_iterations = 100\n",
    "count_at_least_two_non_significant = 0\n",
    "\n",
    "num_pre_periods=4\n",
    "\n",
    "num_post_periods=4\n",
    "\n",
    "estimated_ATE=np.zeros([num_post_periods,num_iterations])\n",
    "accumulated_p_values=np.zeros([num_post_periods,num_iterations])\n",
    "\n",
    "epsilon_scale=1\n",
    "\n",
    "iterations_PTA=[]\n",
    "# Run the loop 100 times.\n",
    "for i in tqdm(range(num_iterations), desc=\"Progress\", unit=\"iteration\"):\n",
    "    # Generate a random seed for each iteration.\n",
    "    seed_val = i\n",
    "    \n",
    "    # Generate data with specified hyperparameters.\n",
    "    data_linear = generate_did_data(\n",
    "        linearity_degree=linearity_degree,\n",
    "        num_pre_periods=num_pre_periods,\n",
    "        num_post_periods=num_post_periods,\n",
    "        pre_trend_bias_delta=0,\n",
    "        num_x_covariates=num_x_covariates,\n",
    "        epsilon_scale=epsilon_scale,\n",
    "        seed=seed_val\n",
    "    )\n",
    "    \n",
    "    # Define the complex regression formula.\n",
    "    formula_complex = \"Y ~ treated_group + C(time) + C(time):treated_group + X_1 + X_2+X_3+X_4+ X_5 + X_6+X_7\"\n",
    "    \n",
    "    # Fit the model.\n",
    "    model_complex = smf.ols(formula=formula_complex, data=data_linear).fit()\n",
    "\n",
    "    \n",
    "    # Count how many of these p-values are above 0.05.\n",
    "    count_non_significant = analyze_p_values(model_complex, num_pre_periods)\n",
    "\n",
    "    accumulate_ate_parameters(model_complex, num_pre_periods, num_post_periods, estimated_ATE, i)\n",
    "    accumulate_ate_pvalues(model_complex, num_pre_periods, num_post_periods, accumulated_p_values, i)\n",
    "    \n",
    "    # If at least two p-values are above 0.05, increment the counter.\n",
    "    if count_non_significant >= 2:\n",
    "        count_at_least_two_non_significant += 1\n",
    "        iterations_PTA.append(i)\n",
    "\n",
    "if linearity_degree == 1 or linearity_degree == 2:\n",
    "    true_ATE = 3.0\n",
    "elif linearity_degree == 3:\n",
    "    true_ATE = 5.0\n",
    "\n",
    "overall_metrics, per_time_period_metrics = calculate_error_metrics(estimated_ATE,accumulated_p_values, true_ATE,\"_ATE_linearity=2\")\n",
    "\n",
    "print(\"Number of iterations (out of 100) with at least two p-values above 0.05 (i.e., we assume that the Parallel Trend Assumption holds):\", \n",
    "      count_at_least_two_non_significant)\n",
    "\n",
    "print(\"Overall Metrics:\")\n",
    "for metric, value in overall_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nPer Time Period Metrics:\")\n",
    "for time_period, metrics in per_time_period_metrics.items():\n",
    "    print(f\"  Time Period {time_period+num_pre_periods}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"    {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/100 [00:00<?, ?iteration/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 100/100 [00:06<00:00, 15.10iteration/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations (out of 100) with at least two p-values above 0.05 (i.e., we assume that the Parallel Trend Assumption holds): 98\n",
      "Overall Metrics:\n",
      "  ATE_rmse: 2.6140\n",
      "  ATE_mae: 2.4520\n",
      "  ATE_mape: 49.0397\n",
      "  ATE_std_rmse: 0.6926\n",
      "  ATE_std_mae: 0.7001\n",
      "  ATE_std_mape: 14.0028\n",
      "\n",
      "Per Time Period Metrics:\n",
      "  Time Period 4:\n",
      "    ATE_rmse: 2.6189\n",
      "    ATE_mae: 2.4417\n",
      "    ATE_mape: 48.8337\n",
      "    ATE_std_mse: 5.1814\n",
      "    ATE_std_mae: 0.9469\n",
      "    ATE_std_mape: 18.9386\n",
      "  Time Period 5:\n",
      "    ATE_rmse: 2.6427\n",
      "    ATE_mae: 2.5042\n",
      "    ATE_mape: 50.0841\n",
      "    ATE_std_mse: 4.2558\n",
      "    ATE_std_mae: 0.8443\n",
      "    ATE_std_mape: 16.8854\n",
      "  Time Period 6:\n",
      "    ATE_rmse: 2.6605\n",
      "    ATE_mae: 2.4805\n",
      "    ATE_mape: 49.6102\n",
      "    ATE_std_mse: 4.7376\n",
      "    ATE_std_mae: 0.9619\n",
      "    ATE_std_mape: 19.2378\n",
      "  Time Period 7:\n",
      "    ATE_rmse: 2.5319\n",
      "    ATE_mae: 2.3816\n",
      "    ATE_mape: 47.6310\n",
      "    ATE_std_mse: 4.1585\n",
      "    ATE_std_mae: 0.8596\n",
      "    ATE_std_mape: 17.1914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the number of covariates as specified.\n",
    "num_x_covariates = 5\n",
    "linearity_degree=3\n",
    "\n",
    "# Set the number of iterations and initialize the counter.\n",
    "num_iterations = 100\n",
    "count_at_least_two_non_significant = 0\n",
    "\n",
    "num_pre_periods=4\n",
    "\n",
    "num_post_periods=4\n",
    "\n",
    "estimated_ATE=np.zeros([num_post_periods,num_iterations])\n",
    "accumulated_p_values=np.zeros([num_post_periods,num_iterations])\n",
    "\n",
    "epsilon_scale=1\n",
    "\n",
    "iterations_PTA=[]\n",
    "# Run the loop 100 times.\n",
    "for i in tqdm(range(num_iterations), desc=\"Progress\", unit=\"iteration\"):\n",
    "    # Generate a random seed for each iteration.\n",
    "    seed_val = i\n",
    "    \n",
    "    # Generate data with specified hyperparameters.\n",
    "    data_linear = generate_did_data(\n",
    "        linearity_degree=linearity_degree,\n",
    "        num_pre_periods=num_pre_periods,\n",
    "        num_post_periods=num_post_periods,\n",
    "        pre_trend_bias_delta=0,\n",
    "        num_x_covariates=num_x_covariates,\n",
    "        epsilon_scale=epsilon_scale,\n",
    "        seed=seed_val\n",
    "    )\n",
    "    \n",
    "    # Define the complex regression formula.\n",
    "    formula_complex = \"Y ~ treated_group + C(time) + C(time):treated_group + X_1 + X_2+X_3+X_4+ X_5 + X_6+X_7\"\n",
    "    \n",
    "    # Fit the model.\n",
    "    model_complex = smf.ols(formula=formula_complex, data=data_linear).fit()\n",
    "\n",
    "    \n",
    "    # Count how many of these p-values are above 0.05.\n",
    "    count_non_significant = analyze_p_values(model_complex, num_pre_periods)\n",
    "\n",
    "    accumulate_ate_parameters(model_complex, num_pre_periods, num_post_periods, estimated_ATE, i)\n",
    "    accumulate_ate_pvalues(model_complex, num_pre_periods, num_post_periods, accumulated_p_values, i)\n",
    "    \n",
    "    # If at least two p-values are above 0.05, increment the counter.\n",
    "    if count_non_significant >= 2:\n",
    "        count_at_least_two_non_significant += 1\n",
    "        iterations_PTA.append(i)\n",
    "\n",
    "if linearity_degree == 1 or linearity_degree == 2:\n",
    "    true_ATE = 3.0\n",
    "elif linearity_degree == 3:\n",
    "    true_ATE = 5.0\n",
    "\n",
    "overall_metrics, per_time_period_metrics = calculate_error_metrics(estimated_ATE,accumulated_p_values, true_ATE,\"_ATE_linearity=3\")\n",
    "\n",
    "print(\"Number of iterations (out of 100) with at least two p-values above 0.05 (i.e., we assume that the Parallel Trend Assumption holds):\", \n",
    "      count_at_least_two_non_significant)\n",
    "\n",
    "print(\"Overall Metrics:\")\n",
    "for metric, value in overall_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nPer Time Period Metrics:\")\n",
    "for time_period, metrics in per_time_period_metrics.items():\n",
    "    print(f\"  Time Period {time_period+num_pre_periods}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"    {metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
