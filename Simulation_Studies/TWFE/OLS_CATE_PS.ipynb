{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_did_data(\n",
    "    n_units=200,\n",
    "    num_pre_periods=5,\n",
    "    num_post_periods=5,\n",
    "    linearity_degree=1, # 1: fully linear, 2: half X non-linear, 3: treatment + all X non-linear\n",
    "    propensity_coeffs={'intercept': 0.0, 'X1': 0.5, 'X7': -0.5}, # Coefficients for propensity score (using static X1 and X7)\n",
    "    pre_trend_bias_delta=0.2,\n",
    "    epsilon_scale=1,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates panel data for Difference-in-Differences analysis with controllable pre-trends,\n",
    "    non-linearity, conditional treatment effects, and propensity score based treatment assignment.\n",
    "\n",
    "    Generates exactly 8 covariates:\n",
    "    - X1: Bernoulli(p=0.66) - Static (unit-level)\n",
    "    - X2: Bernoulli(p=0.45) - Time-varying\n",
    "    - X3: Normal(0,1) - Time-varying (Used in CATE)\n",
    "    - X4: Normal(0,1) - Time-varying\n",
    "    - X5: Normal(0,1) - Time-varying\n",
    "    - X6: Normal(0,1) - Time-varying\n",
    "    - X7: Normal(0,1) - Static (unit-level, Used in Propensity Score)\n",
    "    - X8: Categorical{1,2,3,4} probs {0.3, 0.1, 0.2, 0.4} - Time-varying (Used in CATE)\n",
    "\n",
    "    Args:\n",
    "        n_units (int): Number of units (e.g., individuals, firms).\n",
    "        num_pre_periods (int): Number of periods before treatment.\n",
    "        num_post_periods (int): Number of periods after treatment.\n",
    "        linearity_degree (int): Degree of linearity in the DGP:\n",
    "            1: Fully linear Y, conditional treatment effect based on X3/X8.\n",
    "            2: X1,X2,X3,X4 non-linear, conditional treatment effect based on X3/X8.\n",
    "            3: All X covariates non-linear, conditional treatment effect based on X3/X8.\n",
    "        propensity_coeffs (dict): Dictionary with coefficients for the propensity score calculation.\n",
    "                                  Expected keys: 'intercept', 'X1', 'X7'.\n",
    "                                  p = sigmoid(intercept + X1*coeff_X1 + X7*coeff_X7)\n",
    "        pre_trend_bias_delta (float): Bias parameter to induce pre-trends in the treated group.\n",
    "        epsilon_scale (float): Scale (standard deviation) of the error term.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Generated panel data in long format with 'Y', covariates (X1-X8), 'treated_group',\n",
    "                      'post_treatment', 'CATE', and 'propensity_score'.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    total_periods = num_pre_periods + num_post_periods\n",
    "    total_observations = n_units * total_periods\n",
    "    unit_ids = np.arange(n_units)\n",
    "    time_periods = np.arange(total_periods)\n",
    "\n",
    "    # --- Fixed Covariate Betas ---\n",
    "    beta_x = np.array([-0.75, 0.5, -0.5, -1.30, 1.8, 2.5, -1.0, 0.3])\n",
    "    if len(beta_x) != 8:\n",
    "        raise ValueError(\"beta_x must have exactly 8 elements.\")\n",
    "\n",
    "    # --- Set base treatment_effect_beta based on linearity_degree ---\n",
    "    if linearity_degree == 1 or linearity_degree == 2:\n",
    "        treatment_effect_beta = 3.0\n",
    "    elif linearity_degree == 3:\n",
    "        treatment_effect_beta = 5.0\n",
    "    else:\n",
    "        raise ValueError(f\"linearity_degree ({linearity_degree}) must be 1, 2, or 3.\")\n",
    "\n",
    "    # --- Generate Static Covariates (Unit-Level) ---\n",
    "    X1_unit = np.random.binomial(n=1, p=0.66, size=n_units) # Static Bernoulli for Propensity\n",
    "    X7_unit = np.random.normal(0, 1, size=n_units)          # Static Numerical for Propensity\n",
    "\n",
    "    # --- Propensity Score Calculation and Treatment Assignment ---\n",
    "    z_unit = (propensity_coeffs['intercept'] +\n",
    "              propensity_coeffs['X1'] * X1_unit +\n",
    "              propensity_coeffs['X7'] * X7_unit)\n",
    "    propensity_scores_unit = 1 / (1 + np.exp(-z_unit))\n",
    "    treatment_assignment_random = np.random.uniform(0, 1, size=n_units)\n",
    "    treated_units_mask = treatment_assignment_random < propensity_scores_unit\n",
    "    treated_units = unit_ids[treated_units_mask]\n",
    "\n",
    "    # --- Create Base DataFrame and Map Static Data ---\n",
    "    data = pd.DataFrame({\n",
    "        'unit_id': np.repeat(unit_ids, total_periods),\n",
    "        'time': np.tile(time_periods, n_units)\n",
    "    })\n",
    "    unit_map = data['unit_id'].values # Index mapper from panel row to unit\n",
    "\n",
    "    data['X1'] = X1_unit[unit_map]\n",
    "    data['X7'] = X7_unit[unit_map]\n",
    "    data['propensity_score'] = propensity_scores_unit[unit_map]\n",
    "    data['treated_group'] = np.isin(data['unit_id'], treated_units).astype(int)\n",
    "\n",
    "    # --- Generate Time-Varying Covariates (Panel-Level) ---\n",
    "    data['X2'] = np.random.binomial(n=1, p=0.45, size=total_observations) # Time-varying Bernoulli\n",
    "    data['X3'] = np.random.normal(0, 1, size=total_observations)         # Time-varying Numerical (for CATE)\n",
    "    data['X4'] = np.random.normal(0, 1, size=total_observations)         # Time-varying Numerical\n",
    "    data['X5'] = np.random.normal(0, 1, size=total_observations)         # Time-varying Numerical\n",
    "    data['X6'] = np.random.normal(0, 1, size=total_observations)         # Time-varying Numerical\n",
    "\n",
    "    # Time-varying Categorical (for CATE)\n",
    "    cat_categories = [1, 2, 3, 4]\n",
    "    cat_probabilities = [0.3, 0.1, 0.2, 0.4]\n",
    "    data['X8'] = np.random.choice(cat_categories, size=total_observations, p=cat_probabilities)\n",
    "\n",
    "    # --- Time indicators ---\n",
    "    treatment_period = num_pre_periods\n",
    "    data['post_treatment'] = np.where(data['time'] >= treatment_period, 1, 0)\n",
    "    data['time_trend'] = data['time']\n",
    "\n",
    "    # --- Generate error term ---\n",
    "    data['epsilon'] = np.random.normal(scale=epsilon_scale, size=total_observations)\n",
    "\n",
    "    # --- DGP parameters ---\n",
    "    beta_0 = -0.5 # Intercept\n",
    "    beta_treated = 0.75 # Main effect of treated group (alpha_i)\n",
    "    beta_time = 0.2 # Main effect of time trend (gamma_t)\n",
    "\n",
    "    # --- Calculate Conditional Treatment Effect (CATE) ---\n",
    "    # Depends on X3 (first numerical, time-varying) and X8 (categorical, time-varying)\n",
    "    sqrt_abs_X3 = np.sqrt(np.abs(data['X3']))\n",
    "    cate_conditions = [\n",
    "        (data['X8'] == 1) | (data['X8'] == 3),\n",
    "        (data['X8'] == 2),\n",
    "        (data['X8'] == 4)\n",
    "    ]\n",
    "    cate_choices = [\n",
    "        treatment_effect_beta + 1.5 * sqrt_abs_X3,\n",
    "        treatment_effect_beta,\n",
    "        treatment_effect_beta - 0.5 * sqrt_abs_X3\n",
    "    ]\n",
    "    potential_cate = np.select(cate_conditions, cate_choices, default=treatment_effect_beta)\n",
    "\n",
    "\n",
    "\n",
    "    actual_cate_contribution = potential_cate * data['treated_group'] * data['post_treatment']\n",
    "    data['CATE'] = potential_cate # Store potential effect magnitude\n",
    "\n",
    "    # --- Calculate Outcome Y based on Linearity Degree ---\n",
    "    covariate_names = [f'X{i+1}' for i in range(8)]\n",
    "\n",
    "    # Define non-linear functions for flexibility\n",
    "    def nl_func1(x): return x**2\n",
    "    def nl_func2(x): return np.exp(x / 2) # Scaled exp\n",
    "    def nl_func3(x): return np.abs(x)\n",
    "    def nl_func4(x): return np.sqrt(np.abs(x))\n",
    "\n",
    "    cov_effect = 0\n",
    "\n",
    "    if linearity_degree == 1: # Fully linear\n",
    "        for i in range(8):\n",
    "            cov_effect += beta_x[i] * data[covariate_names[i]]\n",
    "        time_term = beta_time * data['time_trend']\n",
    "\n",
    "    elif linearity_degree == 2: # X1, X2, X3, X4 non-linear\n",
    "        # Apply specific non-linear functions to first 4 covariates\n",
    "        cov_effect += beta_x[0] * nl_func1(data['X1']) # X1^2 (still 0 or 1)\n",
    "        cov_effect += beta_x[1] * nl_func2(data['X2']) # exp(X2/2) (more distinct for 0/1)\n",
    "        cov_effect += beta_x[2] * nl_func3(data['X3']) # abs(X3)\n",
    "        cov_effect += beta_x[3] * nl_func4(data['X4']) # sqrt(abs(X4))\n",
    "        # Linear term for remaining covariates\n",
    "        for i in range(4, 8):\n",
    "            cov_effect += beta_x[i] * data[covariate_names[i]]\n",
    "        time_term = beta_time * data['time_trend']\n",
    "\n",
    "    elif linearity_degree == 3: \n",
    "        # Apply different non-linear functions across all covariates\n",
    "        nl_funcs = [nl_func1, nl_func2, nl_func3, nl_func4, nl_func1, nl_func2, nl_func3, nl_func4] # Example pattern\n",
    "        for i in range(8):\n",
    "            cov_effect += beta_x[i] * nl_funcs[i](data[covariate_names[i]])\n",
    "        # Non-linear time trend\n",
    "        time_term = beta_time * (data['time_trend']**2)\n",
    "\n",
    "    # Combine all components for Y\n",
    "    data['Y'] = (beta_0 + beta_treated * data['treated_group'] +\n",
    "                 time_term + cov_effect +\n",
    "                 actual_cate_contribution)\n",
    "\n",
    "    # --- Add pre-trend bias ---\n",
    "    if pre_trend_bias_delta != 0:\n",
    "        pre_trend_effect = pre_trend_bias_delta * data['treated_group'] * (data['time'] - treatment_period) * (1 - data['post_treatment'])\n",
    "        data['Y'] += pre_trend_effect\n",
    "\n",
    "    # --- Add final error term ---\n",
    "    data['Y'] += data['epsilon']\n",
    "\n",
    "    # --- Final Touches ---\n",
    "    # Reorder columns for clarity\n",
    "    cols_order = ['unit_id', 'time', 'treated_group', 'post_treatment', 'propensity_score'] + \\\n",
    "                 covariate_names + ['CATE', 'Y']\n",
    "    data = data[cols_order]\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/100 [00:00<?, ?iteration/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 100/100 [00:16<00:00,  6.25iteration/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE for 100 simulations: 0.8983647183003943\n",
      "Standard Deviation RMSE for 100 simulations: 0.032354195944998575\n",
      "Mean MAE for 100 simulations: 0.7938258838359326\n",
      "Standard Deviation MAE for 100 simulations: 0.026502446790157112\n",
      "Mean MAPE for 100 simulations: 0.2397658328118938\n",
      "Standard Deviation MAPE for 100 simulations: 0.015627411643064432\n",
      "Mean RMSE for 100 simulations for post-treatment period 1: 0.8988381081140762\n",
      "Standard Deviation RMSE for 100 simulations for post-treatment period 1: 0.048861067727973405\n",
      "Mean MAE for 100 simulations for post-treatment period 1: 0.7976668674207193\n",
      "Standard Deviation MAE for 100 simulations for post-treatment period 1: 0.045033149320408083\n",
      "Mean MAPE for 100 simulations for post-treatment period 1: 0.2403722317919322\n",
      "Standard Deviation MAPE for 100 simulations for post-treatment period 1: 0.021204101640252323\n",
      "Mean RMSE for 100 simulations for post-treatment period 2: 0.8935123314195486\n",
      "Standard Deviation RMSE for 100 simulations for post-treatment period 2: 0.04977682185919933\n",
      "Mean MAE for 100 simulations for post-treatment period 2: 0.7909961074879878\n",
      "Standard Deviation MAE for 100 simulations for post-treatment period 2: 0.047044234281519076\n",
      "Mean MAPE for 100 simulations for post-treatment period 2: 0.23941573829977048\n",
      "Standard Deviation MAPE for 100 simulations for post-treatment period 2: 0.021670688910373225\n",
      "Mean RMSE for 100 simulations for post-treatment period 3: 0.8986735663033578\n",
      "Standard Deviation RMSE for 100 simulations for post-treatment period 3: 0.046797003146714186\n",
      "Mean MAE for 100 simulations for post-treatment period 3: 0.796192113217665\n",
      "Standard Deviation MAE for 100 simulations for post-treatment period 3: 0.04506514625023028\n",
      "Mean MAPE for 100 simulations for post-treatment period 3: 0.2412596141701356\n",
      "Standard Deviation MAPE for 100 simulations for post-treatment period 3: 0.02209978141818719\n",
      "Mean RMSE for 100 simulations for post-treatment period 4: 0.8992196655240648\n",
      "Standard Deviation RMSE for 100 simulations for post-treatment period 4: 0.053705979712196136\n",
      "Mean MAE for 100 simulations for post-treatment period 4: 0.7904484472173579\n",
      "Standard Deviation MAE for 100 simulations for post-treatment period 4: 0.04994796774890267\n",
      "Mean MAPE for 100 simulations for post-treatment period 4: 0.23801574698573685\n",
      "Standard Deviation MAPE for 100 simulations for post-treatment period 4: 0.024415298184185325\n",
      "Metrics data saved to sheet 'Metrics' in OLS_CATE_PS_and_PValues_linearity=1.xlsx\n",
      "P-Values data saved to sheet 'P_Values' in OLS_CATE_PS_and_PValues_linearity=1.xlsx\n"
     ]
    }
   ],
   "source": [
    "num_x_covariates = 6\n",
    "linearity_degree=1\n",
    "\n",
    "# Set the number of iterations and initialize the counter.\n",
    "num_iterations = 100\n",
    "count_at_least_two_non_significant = 0\n",
    "\n",
    "num_pre_periods=4\n",
    "\n",
    "num_post_periods=4\n",
    "\n",
    "epsilon_scale=1\n",
    "\n",
    "RMSE_per_period=np.zeros([num_iterations,num_post_periods])\n",
    "MAE_per_period=np.zeros([num_iterations,num_post_periods])\n",
    "MAPE_per_period=np.zeros([num_iterations,num_post_periods])\n",
    "\n",
    "list_accumulated_p_values=[]\n",
    "\n",
    "RMSE_overall=np.zeros([num_iterations])\n",
    "MAE_overall=np.zeros([num_iterations])\n",
    "MAPE_overall=np.zeros([num_iterations])\n",
    "\n",
    "for i in tqdm(range(num_iterations), desc=\"Progress\", unit=\"iteration\"):\n",
    "    # Generate a random seed for each iteration.\n",
    "    seed_val = i\n",
    "\n",
    "    # Generate data with specified hyperparameters.\n",
    "    data_linear = generate_did_data(\n",
    "        n_units=200,\n",
    "        linearity_degree=linearity_degree,\n",
    "        num_pre_periods=num_pre_periods,\n",
    "        num_post_periods=num_post_periods,\n",
    "        pre_trend_bias_delta=0,\n",
    "        epsilon_scale=epsilon_scale,\n",
    "        seed=seed_val\n",
    "    )\n",
    "\n",
    "    # Define the complex regression formula.\n",
    "    formula_complex = \"Y ~ treated_group + C(time) + C(time):treated_group + X1 + X2+X3+X4+ X5 + X6+X7+X8\"\n",
    "    \n",
    "    # Fit the model.\n",
    "    model_complex = smf.ols(formula=formula_complex, data=data_linear).fit()\n",
    "\n",
    "    CATE_indexes=data_linear[(data_linear['treated_group']==1) & (data_linear['post_treatment']==1)]['CATE'].index\n",
    "    Pre_indexes=data_linear[(data_linear['treated_group']==1) & (data_linear['post_treatment']==0)]['CATE'].index\n",
    "    true_CATE=data_linear[(data_linear['treated_group']==1) & (data_linear['post_treatment']==1)]['CATE']\n",
    "    estimated_CATE=np.array(model_complex.params[[f'C(time)[T.{t}]:treated_group' for t in range(num_pre_periods, num_pre_periods + num_post_periods)]])\n",
    "    final_CATE=np.zeros([int(len(Pre_indexes)/4),num_post_periods])\n",
    "\n",
    "    for k in range(int(len(Pre_indexes)/4)):\n",
    "        final_CATE[k,:]=estimated_CATE\n",
    "\n",
    "    true_CATE=np.array(true_CATE).reshape(final_CATE.shape)\n",
    "    accumulated_p_values=np.zeros([num_post_periods,int(len(Pre_indexes)/4)])\n",
    "\n",
    "    for k in range(int(len(Pre_indexes)/4)):\n",
    "        accumulated_p_values[:,k]=np.array(model_complex.pvalues[[f'C(time)[T.{t}]:treated_group' for t in range(num_pre_periods, num_pre_periods + num_post_periods)]])\n",
    "\n",
    "    accumulated_p_values=accumulated_p_values.reshape(accumulated_p_values.shape[0]*accumulated_p_values.shape[1])\n",
    "    list_accumulated_p_values.append(accumulated_p_values)\n",
    "\n",
    "\n",
    "    RMSE_overall[i]=np.sqrt(np.mean((final_CATE-true_CATE)**2))\n",
    "    MAE_overall[i]=np.mean(np.abs(final_CATE-true_CATE))\n",
    "    MAPE_overall[i]=np.mean(np.abs((final_CATE-true_CATE)/true_CATE))\n",
    "\n",
    "    for h in range(num_post_periods):\n",
    "        RMSE_per_period[i,h]=np.sqrt(np.mean((final_CATE[:,h]-true_CATE[:,h])**2))\n",
    "        MAE_per_period[i,h]=np.mean(np.abs(final_CATE[:,h]-true_CATE[:,h]))\n",
    "        MAPE_per_period[i,h]=np.mean(np.abs((final_CATE[:,h]-true_CATE[:,h])/true_CATE[:,h]))\n",
    "\n",
    "\n",
    "mean_RMSE_overall=np.mean(RMSE_overall)\n",
    "mean_MAE_overall=np.mean(MAE_overall)\n",
    "mean_MAPE_overall=np.mean(MAPE_overall)\n",
    "std_RMSE_overall=np.std(RMSE_overall)\n",
    "std_MAE_overall=np.std(MAE_overall)\n",
    "std_MAPE_overall=np.std(MAPE_overall)\n",
    "\n",
    "print(f\"Mean RMSE for {num_iterations} simulations: {mean_RMSE_overall}\")\n",
    "print(f\"Standard Deviation RMSE for {num_iterations} simulations: {std_RMSE_overall}\")\n",
    "print(f\"Mean MAE for {num_iterations} simulations: {mean_MAE_overall}\")\n",
    "print(f\"Standard Deviation MAE for {num_iterations} simulations: {std_MAE_overall}\")\n",
    "print(f\"Mean MAPE for {num_iterations} simulations: {mean_MAPE_overall}\")\n",
    "print(f\"Standard Deviation MAPE for {num_iterations} simulations: {std_MAPE_overall}\")\n",
    "\n",
    "for h in range(num_post_periods):\n",
    "    print(f\"Mean RMSE for {num_iterations} simulations for post-treatment period {h+1}: {np.mean(RMSE_per_period[:,h])}\")\n",
    "    print(f\"Standard Deviation RMSE for {num_iterations} simulations for post-treatment period {h+1}: {np.std(RMSE_per_period[:,h])}\")\n",
    "    print(f\"Mean MAE for {num_iterations} simulations for post-treatment period {h+1}: {np.mean(MAE_per_period[:,h])}\")\n",
    "    print(f\"Standard Deviation MAE for {num_iterations} simulations for post-treatment period {h+1}: {np.std(MAE_per_period[:,h])}\")\n",
    "    print(f\"Mean MAPE for {num_iterations} simulations for post-treatment period {h+1}: {np.mean(MAPE_per_period[:,h])}\")\n",
    "    print(f\"Standard Deviation MAPE for {num_iterations} simulations for post-treatment period {h+1}: {np.std(MAPE_per_period[:,h])}\")\n",
    "\n",
    "# === Sheet 1: Metrics Data ===\n",
    "\n",
    "# 1. Prepare data dictionary for metrics (overall and per_period)\n",
    "metrics_data_dict = {\n",
    "    'RMSE_overall': RMSE_overall,\n",
    "    'MAE_overall': MAE_overall,\n",
    "    'MAPE_overall': MAPE_overall,\n",
    "}\n",
    "\n",
    "# 2. Flatten the _per_period arrays into columns\n",
    "for i in range(num_post_periods):\n",
    "    metrics_data_dict[f'RMSE_period_{i}'] = RMSE_per_period[:, i]\n",
    "    metrics_data_dict[f'MAE_period_{i}'] = MAE_per_period[:, i]\n",
    "    metrics_data_dict[f'MAPE_period_{i}'] = MAPE_per_period[:, i]\n",
    "\n",
    "# 3. Create the first DataFrame for metrics\n",
    "df_metrics = pd.DataFrame(metrics_data_dict)\n",
    "\n",
    "# === Sheet 2: P-Values Data (Handling Variable Lengths) ===\n",
    "\n",
    "df_p_values = None # Initialize in case the list is empty\n",
    "\n",
    "if not list_accumulated_p_values:\n",
    "    print(\"Warning: 'list_accumulated_p_values' is empty. P-Value sheet will not be created.\")\n",
    "else:\n",
    "    # 1. Find the maximum length of the p-value vectors\n",
    "    max_len = 0\n",
    "    for vec in list_accumulated_p_values:\n",
    "        # Check if the element is actually a numpy array or list-like\n",
    "        if hasattr(vec, '__len__'):\n",
    "             max_len = max(max_len, len(vec))\n",
    "        # else: handle potential non-iterable elements if necessary\n",
    "\n",
    "    if max_len == 0 and list_accumulated_p_values:\n",
    "         print(\"Warning: list_accumulated_p_values contains elements but none have length > 0.\")\n",
    "         # Decide how to handle this - maybe create an empty df?\n",
    "\n",
    "    # 2. Create padded data\n",
    "    padded_p_values = []\n",
    "    for i, vec in enumerate(list_accumulated_p_values):\n",
    "         # Ensure vec is treated as an iterable, default to empty if not applicable\n",
    "        current_vec = []\n",
    "        if hasattr(vec, '__len__'):\n",
    "            current_vec = list(vec) # Convert numpy array to list for easy padding\n",
    "\n",
    "        # Create a padded row with NaN for missing values\n",
    "        padded_row = current_vec + [np.nan] * (max_len - len(current_vec))\n",
    "        padded_p_values.append(padded_row)\n",
    "\n",
    "    # 3. Create column names for the p-values sheet\n",
    "    p_value_columns = [f'p_value_{i}' for i in range(max_len)]\n",
    "\n",
    "    # 4. Create the second DataFrame for p-values\n",
    "    df_p_values = pd.DataFrame(padded_p_values, columns=p_value_columns)\n",
    "\n",
    "# === Save to Excel File with Multiple Sheets ===\n",
    "\n",
    "output_filename_excel = 'OLS_CATE_PS_and_PValues_linearity=1.xlsx'\n",
    "\n",
    "# Use ExcelWriter to write multiple DataFrames to different sheets\n",
    "try:\n",
    "    with pd.ExcelWriter(output_filename_excel, engine='openpyxl') as writer:\n",
    "        # Write the metrics DataFrame to the first sheet\n",
    "        df_metrics.to_excel(writer, sheet_name='Metrics', index=False, float_format='%.6f')\n",
    "        print(f\"Metrics data saved to sheet 'Metrics' in {output_filename_excel}\")\n",
    "\n",
    "        # Write the p-values DataFrame to the second sheet (if it exists)\n",
    "        if df_p_values is not None:\n",
    "            df_p_values.to_excel(writer, sheet_name='P_Values', index=False, float_format='%.6f')\n",
    "            print(f\"P-Values data saved to sheet 'P_Values' in {output_filename_excel}\")\n",
    "        else:\n",
    "             # Optionally create an empty sheet or just skip\n",
    "             print(\"P-Values sheet was not created as the source list was empty or contained no vectors.\")\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nError: Cannot write Excel file. Please install the 'openpyxl' library.\")\n",
    "    print(\"You can install it using: pip install openpyxl\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while writing the Excel file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 100/100 [00:14<00:00,  6.74iteration/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE for 100 simulations: 0.9054549390429928\n",
      "Standard Deviation RMSE for 100 simulations: 0.03764203411725024\n",
      "Mean MAE for 100 simulations: 0.7961892599525594\n",
      "Standard Deviation MAE for 100 simulations: 0.029187512899409226\n",
      "Mean MAPE for 100 simulations: 0.2403755948640254\n",
      "Standard Deviation MAPE for 100 simulations: 0.018686878907407355\n",
      "Mean RMSE for 100 simulations for post-treatment period 1: 0.9050465051864512\n",
      "Standard Deviation RMSE for 100 simulations for post-treatment period 1: 0.05470091423519172\n",
      "Mean MAE for 100 simulations for post-treatment period 1: 0.7997262658936043\n",
      "Standard Deviation MAE for 100 simulations for post-treatment period 1: 0.0462378352377123\n",
      "Mean MAPE for 100 simulations for post-treatment period 1: 0.24069594895824128\n",
      "Standard Deviation MAPE for 100 simulations for post-treatment period 1: 0.022623030765579505\n",
      "Mean RMSE for 100 simulations for post-treatment period 2: 0.900143772790812\n",
      "Standard Deviation RMSE for 100 simulations for post-treatment period 2: 0.048887380616300824\n",
      "Mean MAE for 100 simulations for post-treatment period 2: 0.7921567161360766\n",
      "Standard Deviation MAE for 100 simulations for post-treatment period 2: 0.04618899873879205\n",
      "Mean MAPE for 100 simulations for post-treatment period 2: 0.23940352120822925\n",
      "Standard Deviation MAPE for 100 simulations for post-treatment period 2: 0.023170070808543258\n",
      "Mean RMSE for 100 simulations for post-treatment period 3: 0.9081420839476287\n",
      "Standard Deviation RMSE for 100 simulations for post-treatment period 3: 0.05604923029620374\n",
      "Mean MAE for 100 simulations for post-treatment period 3: 0.8004446210222912\n",
      "Standard Deviation MAE for 100 simulations for post-treatment period 3: 0.048227486777914184\n",
      "Mean MAPE for 100 simulations for post-treatment period 3: 0.2426882516859817\n",
      "Standard Deviation MAPE for 100 simulations for post-treatment period 3: 0.025861344248727346\n",
      "Mean RMSE for 100 simulations for post-treatment period 4: 0.9050198904116781\n",
      "Standard Deviation RMSE for 100 simulations for post-treatment period 4: 0.05820275198868205\n",
      "Mean MAE for 100 simulations for post-treatment period 4: 0.7924294367582657\n",
      "Standard Deviation MAE for 100 simulations for post-treatment period 4: 0.049763382559778184\n",
      "Mean MAPE for 100 simulations for post-treatment period 4: 0.23871465760364952\n",
      "Standard Deviation MAPE for 100 simulations for post-treatment period 4: 0.026024178262205776\n",
      "Metrics data saved to sheet 'Metrics' in OLS_CATE_PS_and_PValues_linearity=2.xlsx\n",
      "P-Values data saved to sheet 'P_Values' in OLS_CATE_PS_and_PValues_linearity=2.xlsx\n"
     ]
    }
   ],
   "source": [
    "num_x_covariates = 6\n",
    "linearity_degree=2\n",
    "\n",
    "# Set the number of iterations and initialize the counter.\n",
    "num_iterations = 100\n",
    "count_at_least_two_non_significant = 0\n",
    "\n",
    "num_pre_periods=4\n",
    "\n",
    "num_post_periods=4\n",
    "\n",
    "epsilon_scale=1\n",
    "\n",
    "RMSE_per_period=np.zeros([num_iterations,num_post_periods])\n",
    "MAE_per_period=np.zeros([num_iterations,num_post_periods])\n",
    "MAPE_per_period=np.zeros([num_iterations,num_post_periods])\n",
    "\n",
    "list_accumulated_p_values=[]\n",
    "\n",
    "RMSE_overall=np.zeros([num_iterations])\n",
    "MAE_overall=np.zeros([num_iterations])\n",
    "MAPE_overall=np.zeros([num_iterations])\n",
    "\n",
    "for i in tqdm(range(num_iterations), desc=\"Progress\", unit=\"iteration\"):\n",
    "    # Generate a random seed for each iteration.\n",
    "    seed_val = i\n",
    "\n",
    "    # Generate data with specified hyperparameters.\n",
    "    data_linear = generate_did_data(\n",
    "        n_units=200,\n",
    "        linearity_degree=linearity_degree,\n",
    "        num_pre_periods=num_pre_periods,\n",
    "        num_post_periods=num_post_periods,\n",
    "        pre_trend_bias_delta=0,\n",
    "        epsilon_scale=epsilon_scale,\n",
    "        seed=seed_val\n",
    "    )\n",
    "\n",
    "    # Define the complex regression formula.\n",
    "    formula_complex = \"Y ~ treated_group + C(time) + C(time):treated_group + X1 + X2+X3+X4+ X5 + X6+X7+X8\"\n",
    "    \n",
    "    # Fit the model.\n",
    "    model_complex = smf.ols(formula=formula_complex, data=data_linear).fit()\n",
    "\n",
    "    CATE_indexes=data_linear[(data_linear['treated_group']==1) & (data_linear['post_treatment']==1)]['CATE'].index\n",
    "    Pre_indexes=data_linear[(data_linear['treated_group']==1) & (data_linear['post_treatment']==0)]['CATE'].index\n",
    "    true_CATE=data_linear[(data_linear['treated_group']==1) & (data_linear['post_treatment']==1)]['CATE']\n",
    "    estimated_CATE=np.array(model_complex.params[[f'C(time)[T.{t}]:treated_group' for t in range(num_pre_periods, num_pre_periods + num_post_periods)]])\n",
    "    final_CATE=np.zeros([int(len(Pre_indexes)/4),num_post_periods])\n",
    "\n",
    "    for k in range(int(len(Pre_indexes)/4)):\n",
    "        final_CATE[k,:]=estimated_CATE\n",
    "\n",
    "    true_CATE=np.array(true_CATE).reshape(final_CATE.shape)\n",
    "    accumulated_p_values=np.zeros([num_post_periods,int(len(Pre_indexes)/4)])\n",
    "\n",
    "    for k in range(int(len(Pre_indexes)/4)):\n",
    "        accumulated_p_values[:,k]=np.array(model_complex.pvalues[[f'C(time)[T.{t}]:treated_group' for t in range(num_pre_periods, num_pre_periods + num_post_periods)]])\n",
    "\n",
    "    accumulated_p_values=accumulated_p_values.reshape(accumulated_p_values.shape[0]*accumulated_p_values.shape[1])\n",
    "    list_accumulated_p_values.append(accumulated_p_values)\n",
    "\n",
    "\n",
    "    RMSE_overall[i]=np.sqrt(np.mean((final_CATE-true_CATE)**2))\n",
    "    MAE_overall[i]=np.mean(np.abs(final_CATE-true_CATE))\n",
    "    MAPE_overall[i]=np.mean(np.abs((final_CATE-true_CATE)/true_CATE))\n",
    "\n",
    "    for h in range(num_post_periods):\n",
    "        RMSE_per_period[i,h]=np.sqrt(np.mean((final_CATE[:,h]-true_CATE[:,h])**2))\n",
    "        MAE_per_period[i,h]=np.mean(np.abs(final_CATE[:,h]-true_CATE[:,h]))\n",
    "        MAPE_per_period[i,h]=np.mean(np.abs((final_CATE[:,h]-true_CATE[:,h])/true_CATE[:,h]))\n",
    "\n",
    "\n",
    "mean_RMSE_overall=np.mean(RMSE_overall)\n",
    "mean_MAE_overall=np.mean(MAE_overall)\n",
    "mean_MAPE_overall=np.mean(MAPE_overall)\n",
    "std_RMSE_overall=np.std(RMSE_overall)\n",
    "std_MAE_overall=np.std(MAE_overall)\n",
    "std_MAPE_overall=np.std(MAPE_overall)\n",
    "\n",
    "print(f\"Mean RMSE for {num_iterations} simulations: {mean_RMSE_overall}\")\n",
    "print(f\"Standard Deviation RMSE for {num_iterations} simulations: {std_RMSE_overall}\")\n",
    "print(f\"Mean MAE for {num_iterations} simulations: {mean_MAE_overall}\")\n",
    "print(f\"Standard Deviation MAE for {num_iterations} simulations: {std_MAE_overall}\")\n",
    "print(f\"Mean MAPE for {num_iterations} simulations: {mean_MAPE_overall}\")\n",
    "print(f\"Standard Deviation MAPE for {num_iterations} simulations: {std_MAPE_overall}\")\n",
    "\n",
    "for h in range(num_post_periods):\n",
    "    print(f\"Mean RMSE for {num_iterations} simulations for post-treatment period {h+1}: {np.mean(RMSE_per_period[:,h])}\")\n",
    "    print(f\"Standard Deviation RMSE for {num_iterations} simulations for post-treatment period {h+1}: {np.std(RMSE_per_period[:,h])}\")\n",
    "    print(f\"Mean MAE for {num_iterations} simulations for post-treatment period {h+1}: {np.mean(MAE_per_period[:,h])}\")\n",
    "    print(f\"Standard Deviation MAE for {num_iterations} simulations for post-treatment period {h+1}: {np.std(MAE_per_period[:,h])}\")\n",
    "    print(f\"Mean MAPE for {num_iterations} simulations for post-treatment period {h+1}: {np.mean(MAPE_per_period[:,h])}\")\n",
    "    print(f\"Standard Deviation MAPE for {num_iterations} simulations for post-treatment period {h+1}: {np.std(MAPE_per_period[:,h])}\")\n",
    "\n",
    "# === Sheet 1: Metrics Data ===\n",
    "\n",
    "# 1. Prepare data dictionary for metrics (overall and per_period)\n",
    "metrics_data_dict = {\n",
    "    'RMSE_overall': RMSE_overall,\n",
    "    'MAE_overall': MAE_overall,\n",
    "    'MAPE_overall': MAPE_overall,\n",
    "}\n",
    "\n",
    "# 2. Flatten the _per_period arrays into columns\n",
    "for i in range(num_post_periods):\n",
    "    metrics_data_dict[f'RMSE_period_{i}'] = RMSE_per_period[:, i]\n",
    "    metrics_data_dict[f'MAE_period_{i}'] = MAE_per_period[:, i]\n",
    "    metrics_data_dict[f'MAPE_period_{i}'] = MAPE_per_period[:, i]\n",
    "\n",
    "# 3. Create the first DataFrame for metrics\n",
    "df_metrics = pd.DataFrame(metrics_data_dict)\n",
    "\n",
    "# === Sheet 2: P-Values Data (Handling Variable Lengths) ===\n",
    "\n",
    "df_p_values = None # Initialize in case the list is empty\n",
    "\n",
    "if not list_accumulated_p_values:\n",
    "    print(\"Warning: 'list_accumulated_p_values' is empty. P-Value sheet will not be created.\")\n",
    "else:\n",
    "    # 1. Find the maximum length of the p-value vectors\n",
    "    max_len = 0\n",
    "    for vec in list_accumulated_p_values:\n",
    "        # Check if the element is actually a numpy array or list-like\n",
    "        if hasattr(vec, '__len__'):\n",
    "             max_len = max(max_len, len(vec))\n",
    "        # else: handle potential non-iterable elements if necessary\n",
    "\n",
    "    if max_len == 0 and list_accumulated_p_values:\n",
    "         print(\"Warning: list_accumulated_p_values contains elements but none have length > 0.\")\n",
    "         # Decide how to handle this - maybe create an empty df?\n",
    "\n",
    "    # 2. Create padded data\n",
    "    padded_p_values = []\n",
    "    for i, vec in enumerate(list_accumulated_p_values):\n",
    "         # Ensure vec is treated as an iterable, default to empty if not applicable\n",
    "        current_vec = []\n",
    "        if hasattr(vec, '__len__'):\n",
    "            current_vec = list(vec) # Convert numpy array to list for easy padding\n",
    "\n",
    "        # Create a padded row with NaN for missing values\n",
    "        padded_row = current_vec + [np.nan] * (max_len - len(current_vec))\n",
    "        padded_p_values.append(padded_row)\n",
    "\n",
    "    # 3. Create column names for the p-values sheet\n",
    "    p_value_columns = [f'p_value_{i}' for i in range(max_len)]\n",
    "\n",
    "    # 4. Create the second DataFrame for p-values\n",
    "    df_p_values = pd.DataFrame(padded_p_values, columns=p_value_columns)\n",
    "\n",
    "# === Save to Excel File with Multiple Sheets ===\n",
    "\n",
    "output_filename_excel = 'OLS_CATE_PS_and_PValues_linearity=2.xlsx'\n",
    "\n",
    "# Use ExcelWriter to write multiple DataFrames to different sheets\n",
    "try:\n",
    "    with pd.ExcelWriter(output_filename_excel, engine='openpyxl') as writer:\n",
    "        # Write the metrics DataFrame to the first sheet\n",
    "        df_metrics.to_excel(writer, sheet_name='Metrics', index=False, float_format='%.6f')\n",
    "        print(f\"Metrics data saved to sheet 'Metrics' in {output_filename_excel}\")\n",
    "\n",
    "        # Write the p-values DataFrame to the second sheet (if it exists)\n",
    "        if df_p_values is not None:\n",
    "            df_p_values.to_excel(writer, sheet_name='P_Values', index=False, float_format='%.6f')\n",
    "            print(f\"P-Values data saved to sheet 'P_Values' in {output_filename_excel}\")\n",
    "        else:\n",
    "             # Optionally create an empty sheet or just skip\n",
    "             print(\"P-Values sheet was not created as the source list was empty or contained no vectors.\")\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nError: Cannot write Excel file. Please install the 'openpyxl' library.\")\n",
    "    print(\"You can install it using: pip install openpyxl\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while writing the Excel file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/100 [00:00<?, ?iteration/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 100/100 [00:14<00:00,  7.10iteration/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE for 100 simulations: 1.0190804986939925\n",
      "Standard Deviation RMSE for 100 simulations: 0.0993013606867084\n",
      "Mean MAE for 100 simulations: 0.8582972158593382\n",
      "Standard Deviation MAE for 100 simulations: 0.0726645166091538\n",
      "Mean MAPE for 100 simulations: 0.15966982289456502\n",
      "Standard Deviation MAPE for 100 simulations: 0.019930555842114075\n",
      "Mean RMSE for 100 simulations for post-treatment period 1: 0.9984831008427849\n",
      "Standard Deviation RMSE for 100 simulations for post-treatment period 1: 0.142171428101317\n",
      "Mean MAE for 100 simulations for post-treatment period 1: 0.8481632030053012\n",
      "Standard Deviation MAE for 100 simulations for post-treatment period 1: 0.11109213313216994\n",
      "Mean MAPE for 100 simulations for post-treatment period 1: 0.15674030533032426\n",
      "Standard Deviation MAPE for 100 simulations for post-treatment period 1: 0.027008459582042756\n",
      "Mean RMSE for 100 simulations for post-treatment period 2: 1.0280313676529738\n",
      "Standard Deviation RMSE for 100 simulations for post-treatment period 2: 0.17412860780351194\n",
      "Mean MAE for 100 simulations for post-treatment period 2: 0.8712151734118322\n",
      "Standard Deviation MAE for 100 simulations for post-treatment period 2: 0.12983177788630498\n",
      "Mean MAPE for 100 simulations for post-treatment period 2: 0.16315462028167174\n",
      "Standard Deviation MAPE for 100 simulations for post-treatment period 2: 0.030386890639227702\n",
      "Mean RMSE for 100 simulations for post-treatment period 3: 0.9773407744253043\n",
      "Standard Deviation RMSE for 100 simulations for post-treatment period 3: 0.12475885455102236\n",
      "Mean MAE for 100 simulations for post-treatment period 3: 0.8385263099634858\n",
      "Standard Deviation MAE for 100 simulations for post-treatment period 3: 0.09522249002631615\n",
      "Mean MAPE for 100 simulations for post-treatment period 3: 0.15680329855263372\n",
      "Standard Deviation MAPE for 100 simulations for post-treatment period 3: 0.025526258891064878\n",
      "Mean RMSE for 100 simulations for post-treatment period 4: 1.039173772277821\n",
      "Standard Deviation RMSE for 100 simulations for post-treatment period 4: 0.19634388074558667\n",
      "Mean MAE for 100 simulations for post-treatment period 4: 0.875284177056734\n",
      "Standard Deviation MAE for 100 simulations for post-treatment period 4: 0.15567126254464042\n",
      "Mean MAPE for 100 simulations for post-treatment period 4: 0.16198106741363041\n",
      "Standard Deviation MAPE for 100 simulations for post-treatment period 4: 0.036279971982633254\n",
      "Metrics data saved to sheet 'Metrics' in OLS_CATE_PS_and_PValues_linearity=4.xlsx\n",
      "P-Values data saved to sheet 'P_Values' in OLS_CATE_PS_and_PValues_linearity=4.xlsx\n"
     ]
    }
   ],
   "source": [
    "num_x_covariates = 6\n",
    "linearity_degree=3\n",
    "\n",
    "# Set the number of iterations and initialize the counter.\n",
    "num_iterations = 100\n",
    "count_at_least_two_non_significant = 0\n",
    "\n",
    "num_pre_periods=4\n",
    "\n",
    "num_post_periods=4\n",
    "\n",
    "epsilon_scale=1\n",
    "\n",
    "RMSE_per_period=np.zeros([num_iterations,num_post_periods])\n",
    "MAE_per_period=np.zeros([num_iterations,num_post_periods])\n",
    "MAPE_per_period=np.zeros([num_iterations,num_post_periods])\n",
    "\n",
    "list_accumulated_p_values=[]\n",
    "\n",
    "RMSE_overall=np.zeros([num_iterations])\n",
    "MAE_overall=np.zeros([num_iterations])\n",
    "MAPE_overall=np.zeros([num_iterations])\n",
    "\n",
    "for i in tqdm(range(num_iterations), desc=\"Progress\", unit=\"iteration\"):\n",
    "    # Generate a random seed for each iteration.\n",
    "    seed_val = i\n",
    "\n",
    "    # Generate data with specified hyperparameters.\n",
    "    data_linear = generate_did_data(\n",
    "        n_units=200,\n",
    "        linearity_degree=linearity_degree,\n",
    "        num_pre_periods=num_pre_periods,\n",
    "        num_post_periods=num_post_periods,\n",
    "        pre_trend_bias_delta=0,\n",
    "        epsilon_scale=epsilon_scale,\n",
    "        seed=seed_val\n",
    "    )\n",
    "\n",
    "    # Define the complex regression formula.\n",
    "    formula_complex = \"Y ~ treated_group + C(time) + C(time):treated_group + X1 + X2+X3+X4+ X5 + X6+X7+X8\"\n",
    "    \n",
    "    # Fit the model.\n",
    "    model_complex = smf.ols(formula=formula_complex, data=data_linear).fit()\n",
    "\n",
    "    CATE_indexes=data_linear[(data_linear['treated_group']==1) & (data_linear['post_treatment']==1)]['CATE'].index\n",
    "    Pre_indexes=data_linear[(data_linear['treated_group']==1) & (data_linear['post_treatment']==0)]['CATE'].index\n",
    "    true_CATE=data_linear[(data_linear['treated_group']==1) & (data_linear['post_treatment']==1)]['CATE']\n",
    "    estimated_CATE=np.array(model_complex.params[[f'C(time)[T.{t}]:treated_group' for t in range(num_pre_periods, num_pre_periods + num_post_periods)]])\n",
    "    final_CATE=np.zeros([int(len(Pre_indexes)/4),num_post_periods])\n",
    "\n",
    "    for k in range(int(len(Pre_indexes)/4)):\n",
    "        final_CATE[k,:]=estimated_CATE\n",
    "\n",
    "    true_CATE=np.array(true_CATE).reshape(final_CATE.shape)\n",
    "    accumulated_p_values=np.zeros([num_post_periods,int(len(Pre_indexes)/4)])\n",
    "\n",
    "    for k in range(int(len(Pre_indexes)/4)):\n",
    "        accumulated_p_values[:,k]=np.array(model_complex.pvalues[[f'C(time)[T.{t}]:treated_group' for t in range(num_pre_periods, num_pre_periods + num_post_periods)]])\n",
    "\n",
    "    accumulated_p_values=accumulated_p_values.reshape(accumulated_p_values.shape[0]*accumulated_p_values.shape[1])\n",
    "    list_accumulated_p_values.append(accumulated_p_values)\n",
    "\n",
    "\n",
    "    RMSE_overall[i]=np.sqrt(np.mean((final_CATE-true_CATE)**2))\n",
    "    MAE_overall[i]=np.mean(np.abs(final_CATE-true_CATE))\n",
    "    MAPE_overall[i]=np.mean(np.abs((final_CATE-true_CATE)/true_CATE))\n",
    "\n",
    "    for h in range(num_post_periods):\n",
    "        RMSE_per_period[i,h]=np.sqrt(np.mean((final_CATE[:,h]-true_CATE[:,h])**2))\n",
    "        MAE_per_period[i,h]=np.mean(np.abs(final_CATE[:,h]-true_CATE[:,h]))\n",
    "        MAPE_per_period[i,h]=np.mean(np.abs((final_CATE[:,h]-true_CATE[:,h])/true_CATE[:,h]))\n",
    "\n",
    "\n",
    "mean_RMSE_overall=np.mean(RMSE_overall)\n",
    "mean_MAE_overall=np.mean(MAE_overall)\n",
    "mean_MAPE_overall=np.mean(MAPE_overall)\n",
    "std_RMSE_overall=np.std(RMSE_overall)\n",
    "std_MAE_overall=np.std(MAE_overall)\n",
    "std_MAPE_overall=np.std(MAPE_overall)\n",
    "\n",
    "print(f\"Mean RMSE for {num_iterations} simulations: {mean_RMSE_overall}\")\n",
    "print(f\"Standard Deviation RMSE for {num_iterations} simulations: {std_RMSE_overall}\")\n",
    "print(f\"Mean MAE for {num_iterations} simulations: {mean_MAE_overall}\")\n",
    "print(f\"Standard Deviation MAE for {num_iterations} simulations: {std_MAE_overall}\")\n",
    "print(f\"Mean MAPE for {num_iterations} simulations: {mean_MAPE_overall}\")\n",
    "print(f\"Standard Deviation MAPE for {num_iterations} simulations: {std_MAPE_overall}\")\n",
    "\n",
    "for h in range(num_post_periods):\n",
    "    print(f\"Mean RMSE for {num_iterations} simulations for post-treatment period {h+1}: {np.mean(RMSE_per_period[:,h])}\")\n",
    "    print(f\"Standard Deviation RMSE for {num_iterations} simulations for post-treatment period {h+1}: {np.std(RMSE_per_period[:,h])}\")\n",
    "    print(f\"Mean MAE for {num_iterations} simulations for post-treatment period {h+1}: {np.mean(MAE_per_period[:,h])}\")\n",
    "    print(f\"Standard Deviation MAE for {num_iterations} simulations for post-treatment period {h+1}: {np.std(MAE_per_period[:,h])}\")\n",
    "    print(f\"Mean MAPE for {num_iterations} simulations for post-treatment period {h+1}: {np.mean(MAPE_per_period[:,h])}\")\n",
    "    print(f\"Standard Deviation MAPE for {num_iterations} simulations for post-treatment period {h+1}: {np.std(MAPE_per_period[:,h])}\")\n",
    "\n",
    "# === Sheet 1: Metrics Data ===\n",
    "\n",
    "# 1. Prepare data dictionary for metrics (overall and per_period)\n",
    "metrics_data_dict = {\n",
    "    'RMSE_overall': RMSE_overall,\n",
    "    'MAE_overall': MAE_overall,\n",
    "    'MAPE_overall': MAPE_overall,\n",
    "}\n",
    "\n",
    "# 2. Flatten the _per_period arrays into columns\n",
    "for i in range(num_post_periods):\n",
    "    metrics_data_dict[f'RMSE_period_{i}'] = RMSE_per_period[:, i]\n",
    "    metrics_data_dict[f'MAE_period_{i}'] = MAE_per_period[:, i]\n",
    "    metrics_data_dict[f'MAPE_period_{i}'] = MAPE_per_period[:, i]\n",
    "\n",
    "# 3. Create the first DataFrame for metrics\n",
    "df_metrics = pd.DataFrame(metrics_data_dict)\n",
    "\n",
    "# === Sheet 2: P-Values Data (Handling Variable Lengths) ===\n",
    "\n",
    "df_p_values = None # Initialize in case the list is empty\n",
    "\n",
    "if not list_accumulated_p_values:\n",
    "    print(\"Warning: 'list_accumulated_p_values' is empty. P-Value sheet will not be created.\")\n",
    "else:\n",
    "    # 1. Find the maximum length of the p-value vectors\n",
    "    max_len = 0\n",
    "    for vec in list_accumulated_p_values:\n",
    "        # Check if the element is actually a numpy array or list-like\n",
    "        if hasattr(vec, '__len__'):\n",
    "             max_len = max(max_len, len(vec))\n",
    "        # else: handle potential non-iterable elements if necessary\n",
    "\n",
    "    if max_len == 0 and list_accumulated_p_values:\n",
    "         print(\"Warning: list_accumulated_p_values contains elements but none have length > 0.\")\n",
    "         # Decide how to handle this - maybe create an empty df?\n",
    "\n",
    "    # 2. Create padded data\n",
    "    padded_p_values = []\n",
    "    for i, vec in enumerate(list_accumulated_p_values):\n",
    "         # Ensure vec is treated as an iterable, default to empty if not applicable\n",
    "        current_vec = []\n",
    "        if hasattr(vec, '__len__'):\n",
    "            current_vec = list(vec) # Convert numpy array to list for easy padding\n",
    "\n",
    "        # Create a padded row with NaN for missing values\n",
    "        padded_row = current_vec + [np.nan] * (max_len - len(current_vec))\n",
    "        padded_p_values.append(padded_row)\n",
    "\n",
    "    # 3. Create column names for the p-values sheet\n",
    "    p_value_columns = [f'p_value_{i}' for i in range(max_len)]\n",
    "\n",
    "    # 4. Create the second DataFrame for p-values\n",
    "    df_p_values = pd.DataFrame(padded_p_values, columns=p_value_columns)\n",
    "\n",
    "# === Save to Excel File with Multiple Sheets ===\n",
    "\n",
    "output_filename_excel = 'OLS_CATE_PS_and_PValues_linearity=3.xlsx'\n",
    "\n",
    "# Use ExcelWriter to write multiple DataFrames to different sheets\n",
    "try:\n",
    "    with pd.ExcelWriter(output_filename_excel, engine='openpyxl') as writer:\n",
    "        # Write the metrics DataFrame to the first sheet\n",
    "        df_metrics.to_excel(writer, sheet_name='Metrics', index=False, float_format='%.6f')\n",
    "        print(f\"Metrics data saved to sheet 'Metrics' in {output_filename_excel}\")\n",
    "\n",
    "        # Write the p-values DataFrame to the second sheet (if it exists)\n",
    "        if df_p_values is not None:\n",
    "            df_p_values.to_excel(writer, sheet_name='P_Values', index=False, float_format='%.6f')\n",
    "            print(f\"P-Values data saved to sheet 'P_Values' in {output_filename_excel}\")\n",
    "        else:\n",
    "             # Optionally create an empty sheet or just skip\n",
    "             print(\"P-Values sheet was not created as the source list was empty or contained no vectors.\")\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nError: Cannot write Excel file. Please install the 'openpyxl' library.\")\n",
    "    print(\"You can install it using: pip install openpyxl\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while writing the Excel file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
