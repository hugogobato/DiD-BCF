{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p76dXXNQWYB",
        "outputId": "c91a1af1-63fc-4e38-d453-d9c86505ce77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/StochasticTree/stochtree.git\n",
            "  Cloning https://github.com/StochasticTree/stochtree.git to /tmp/pip-req-build-90rj8fh4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/StochasticTree/stochtree.git /tmp/pip-req-build-90rj8fh4\n",
            "  Resolved https://github.com/StochasticTree/stochtree.git to commit 795b1e80729f9067034a312105b3bc348b2843d8\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stochtree\n",
            "  Building wheel for stochtree (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stochtree: filename=stochtree-0.1.0-cp311-cp311-linux_x86_64.whl size=875692 sha256=eaa785e0d9db5dcfd556480e150a27087202525b3df8aee80376ce27b810bb31\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wmc3bxd9/wheels/6b/16/bb/b09e1d07fb9c44bfd19200620859a0fdda75287afaa4a076bf\n",
            "Successfully built stochtree\n",
            "Installing collected packages: stochtree\n",
            "Successfully installed stochtree-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/StochasticTree/stochtree.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VgjqGpE_QWYH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hg0viKgUQWYI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_did_data(\n",
        "    n_units=200,\n",
        "    num_x_covariates=5,\n",
        "    num_pre_periods=5,\n",
        "    num_post_periods=5,\n",
        "    linearity_degree=1, # 1: fully linear, 2: half X non-linear, 3: treatment + all X non-linear\n",
        "    pre_trend_bias_delta=0.2,\n",
        "    epsilon_scale=1,\n",
        "    seed=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates panel data for Difference-in-Differences analysis with controllable pre-trends and non-linearity.\n",
        "\n",
        "    Args:\n",
        "        n_units (int): Number of units (e.g., individuals, firms).\n",
        "        num_x_covariates (int): Number of control covariates (X) (not counting the two covariates W1 and W2 where W1 ~ Bernoulli(0.66)\n",
        "        and W2 takes the values 1,2,3,4 with the following probabilities 0.3, 0.1, 0.2, 0.4 respectively.\n",
        "        num_pre_periods (int): Number of periods before treatment.\n",
        "        num_post_periods (int): Number of periods after treatment.\n",
        "        treatment_effect_beta (float): True treatment effect size.\n",
        "        linearity_degree (int): Degree of linearity in the DGP:\n",
        "            1: Fully linear.\n",
        "            2: Half of X covariates have non-linear relationship with Y.\n",
        "            3: Treatment and all X covariates have non-linear relationship with Y.\n",
        "        pre_trend_bias_delta (float): Bias parameter to induce pre-trends in the treated group.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Generated panel data in long format.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # --- Set treatment_effect_beta based on linearity_degree (like R code) ---\n",
        "    if linearity_degree == 1 or linearity_degree == 2:\n",
        "        treatment_effect_beta = 0\n",
        "    elif linearity_degree == 3:\n",
        "        treatment_effect_beta = 0\n",
        "    else:\n",
        "        # Handle cases where linearity_degree is not 1, 2, or 3\n",
        "        print(f\"Warning: linearity_degree ({linearity_degree}) has an unexpected value. Setting treatment_effect_beta to NaN.\")\n",
        "        treatment_effect_beta = np.nan\n",
        "\n",
        "    periods = num_pre_periods + num_post_periods\n",
        "    unit_ids = range(n_units)\n",
        "    time_periods = range(periods)\n",
        "\n",
        "    # Create base data frame\n",
        "    data = pd.DataFrame({\n",
        "        'unit_id': np.repeat(unit_ids, periods),\n",
        "        'time': np.tile(time_periods, n_units)\n",
        "    })\n",
        "\n",
        "    # Treatment assignment (randomly assign half to treatment)\n",
        "    treated_units = np.random.choice(unit_ids, size=n_units // 2, replace=False) #TO DO: add more complex propensity score\n",
        "    data['treated_group'] = np.where(data['unit_id'].isin(treated_units), 1, 0)\n",
        "\n",
        "    # Time indicators\n",
        "    treatment_period = num_pre_periods # Period when treatment starts\n",
        "    data['post_treatment'] = np.where(data['time'] >= treatment_period, 1, 0)\n",
        "    data['time_trend'] = data['time'] # Simple linear time trend\n",
        "\n",
        "    X = np.random.normal(0, 1, size=(len(data), num_x_covariates))\n",
        "\n",
        "\n",
        "    # Add Bernoulli random variable with p=0.66\n",
        "    bernoulli_values = np.random.binomial(n=1, p=0.66, size=len(data))\n",
        "    # Add to both X matrix and dataframe\n",
        "    X = np.column_stack((bernoulli_values,X))\n",
        "\n",
        "   # Add categorical variable with values 1,2,3,4 with probabilities 0.3, 0.1, 0.2, 0.4\n",
        "    categories = [1, 2, 3, 4]\n",
        "    probabilities = [0.3, 0.1, 0.2, 0.4]\n",
        "    categorical_values = np.random.choice(categories, size=len(data), p=probabilities)\n",
        "    # Add to both X matrix and dataframe\n",
        "    X = np.column_stack((X, categorical_values))\n",
        "\n",
        "    for i in range(num_x_covariates+2):\n",
        "        data[f'X_{i+1}'] = X[:,i]\n",
        "\n",
        "    # Generate error term\n",
        "    data['epsilon'] = np.random.normal(scale=epsilon_scale,size=len(data))\n",
        "\n",
        "    # DGP parameters (can be adjusted for more complex DGPs)\n",
        "    beta_0 = -0.5 # Intercept\n",
        "    beta_treated = 0.75 # Main effect of treated group (alpha_i)\n",
        "    beta_time = 0.2 # Main effect of time trend (gamma_t)\n",
        "    beta_interaction = treatment_effect_beta # Treatment effect\n",
        "    beta_x = np.array([-0.75, 0.5, -0.5, -1.30, 1.8, 2.5, -1.0])\n",
        "\n",
        "\n",
        "    # Non-linear components based on linearity_degree\n",
        "    if linearity_degree == 1: # Half covariates non-linear\n",
        "        linear_x_contribution = np.sum([beta_x[i] * data[f'X_{i+1}'] for i in range(num_x_covariates+2)], axis=0)\n",
        "        data['Y'] = beta_0 + beta_treated * data['treated_group'] + beta_time * data['time_trend']+linear_x_contribution+ beta_interaction * data['treated_group'] * data['post_treatment']\n",
        "        data['CATE'] = beta_interaction * data['treated_group'] * data['post_treatment']\n",
        "\n",
        "    elif linearity_degree == 2: # Half covariates non-linear\n",
        "        half = num_x_covariates+2 // 2\n",
        "        cov_effect = (np.sum(beta_x[:int(half/2)] * (X[:, :int(half/2)] ** 2),axis=1) + np.sum(beta_x[int(half/2):half] * np.exp(X[:, int(half/2):half]),axis=1)+\n",
        "                              np.sum(beta_x[half:] * X[:, half:],axis=1))\n",
        "        data['Y'] = beta_0 + beta_treated * data['treated_group'] + beta_time * data['time_trend']+cov_effect+beta_interaction * data['treated_group'] * data['post_treatment']\n",
        "        data['CATE'] = beta_interaction * data['treated_group'] * data['post_treatment']\n",
        "\n",
        "    elif linearity_degree == 3: \n",
        "        half = num_x_covariates+2 // 2\n",
        "        cov_effect = (np.sum(beta_x[:int(half/2)] * (X[:, :int(half/2)] ** 2),axis=1) + np.sum(beta_x[int(half/2):half] * np.exp(X[:, int(half/2):half]),axis=1)+\n",
        "                              np.sum(beta_x[half:half+int(half/2)] * np.abs(X[:, half:half+int(half/2)]),axis=1) + np.sum(beta_x[half+int(half/2):] * np.sqrt(np.abs(X[:, half+int(half/2):])),axis=1))\n",
        "        data['Y'] = beta_0 + beta_treated * data['treated_group'] + beta_time * data['time_trend']**2+cov_effect+beta_interaction * data['treated_group'] * data['post_treatment']\n",
        "        data['CATE'] = beta_interaction * data['treated_group'] * data['post_treatment']\n",
        "\n",
        "\n",
        "    # Add pre-trend bias (differential trend for treated group in pre-treatment)\n",
        "    if pre_trend_bias_delta != 0:\n",
        "        if linearity_degree == 3:\n",
        "            # Example parameters for seasonality\n",
        "            seasonal_amplitude = 1.0  # Amplitude of the seasonal effect\n",
        "            seasonal_period = 4      # Period of the seasonal effect (e.g., 12 for monthly data)\n",
        "\n",
        "            # Calculate the seasonal effect\n",
        "            seasonal_effect = seasonal_amplitude * np.sin(2 * np.pi * data['time'] / seasonal_period)\n",
        "            data['Y'] += pre_trend_bias_delta * data['treated_group'] * seasonal_effect\n",
        "        else:\n",
        "            data['Y'] += pre_trend_bias_delta * data['treated_group'] * (data['time'] - treatment_period)\n",
        "        # (data['time'] - treatment_period) will be negative in pre-treatment, 0 at treatment period, and positive in post-treatment.\n",
        "        # (1 - data['post_treatment']) ensures this bias only applies in pre-treatment periods.\n",
        "\n",
        "\n",
        "    # Add error term\n",
        "    data['Y'] += data['epsilon']\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mRsSJjwMQWYK"
      },
      "outputs": [],
      "source": [
        "from stochtree import BCFModel\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1Ax6IhACQWYK"
      },
      "outputs": [],
      "source": [
        "def calculate_error_metrics(estimated_ATE, accumulated_p_values, true_ATE=0.5, suffix=\"\"):\n",
        "    \"\"\"\n",
        "    Calculates RMSE, MAE, and MAPE for the estimated ATE values,\n",
        "    both overall and per time period (row).  Adds columns for each simulation's p-values\n",
        "    directly to the output CSV.\n",
        "\n",
        "    Args:\n",
        "        estimated_ATE: A numpy array of shape (num_post_periods, num_simulations)\n",
        "                       containing the estimated ATE values.\n",
        "        accumulated_p_values: A numpy array of shape (num_post_periods, num_simulations)\n",
        "                              containing the accumulated p-values.\n",
        "        true_ATE: The true ATE value (default: 0.5).\n",
        "        suffix: A suffix to add to the output CSV filename.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - overall_metrics: A dictionary containing overall RMSE, MAE, and MAPE for ATE.\n",
        "            - per_time_period_metrics: A dictionary where keys are time period indices\n",
        "              and values are dictionaries containing RMSE, MAE, and MAPE for ATE for that time period.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Calculate Metrics for Estimated ATE ---\n",
        "    # Overall metrics for ATE\n",
        "    overall_rmse_ate = np.sqrt(mean_squared_error(true_ATE * np.ones(estimated_ATE.size), estimated_ATE.flatten()))\n",
        "    overall_mae_ate = mean_absolute_error(true_ATE * np.ones(estimated_ATE.size), estimated_ATE.flatten())\n",
        "    overall_mape_ate = np.mean(np.abs((estimated_ATE.flatten() - true_ATE) / true_ATE)) * 100 if true_ATE != 0 else np.nan\n",
        "    std_rmse_ate = np.std(np.sqrt(np.mean((estimated_ATE-true_ATE)**2, axis=0)).flatten())\n",
        "    std_mae_ate = np.std(np.mean(np.abs(estimated_ATE-true_ATE), axis=0).flatten())\n",
        "    std_mape_ate = np.std(np.mean(np.abs((estimated_ATE - true_ATE) / true_ATE), axis=0).flatten() * 100) if true_ATE != 0 else np.nan\n",
        "\n",
        "\n",
        "    overall_metrics_ate = {\n",
        "        \"ATE_rmse\": overall_rmse_ate,\n",
        "        \"ATE_mae\": overall_mae_ate,\n",
        "        \"ATE_mape\": overall_mape_ate,\n",
        "        \"ATE_std_rmse\": std_rmse_ate,\n",
        "        \"ATE_std_mae\": std_mae_ate,\n",
        "        \"ATE_std_mape\": std_mape_ate\n",
        "    }\n",
        "\n",
        "    # Per-time-period metrics for ATE\n",
        "    per_time_period_metrics_ate = {}\n",
        "    for i in range(estimated_ATE.shape[0]):  # Iterate over rows (time periods)\n",
        "        rmse_ate = np.sqrt(mean_squared_error(true_ATE * np.ones(estimated_ATE[i,:].size), estimated_ATE[i,:]))\n",
        "        mae_ate = mean_absolute_error(true_ATE * np.ones(estimated_ATE[i,:].size), estimated_ATE[i,:])\n",
        "        mape_ate = np.mean(np.abs((estimated_ATE[i,:] - true_ATE) / true_ATE)) * 100 if true_ATE != 0 else np.nan\n",
        "        std_mse_ate = np.std((estimated_ATE[i,:]-true_ATE)**2)\n",
        "        std_mae_ate = np.std(np.abs(estimated_ATE[i,:]-true_ATE).flatten())\n",
        "        std_mape_ate = np.std(np.abs((estimated_ATE[i,:] - true_ATE) / true_ATE).flatten() * 100) if true_ATE != 0 else np.nan\n",
        "\n",
        "\n",
        "        per_time_period_metrics_ate[i] = {\n",
        "            \"ATE_rmse\": rmse_ate,\n",
        "            \"ATE_mae\": mae_ate,\n",
        "            \"ATE_mape\": mape_ate,\n",
        "            \"ATE_std_mse\": std_mse_ate,\n",
        "            \"ATE_std_mae\": std_mae_ate,\n",
        "            \"ATE_std_mape\": std_mape_ate\n",
        "        }\n",
        "\n",
        "\n",
        "    # Calculate the three specific metrics to save to CSV for ATE\n",
        "    rmse_values_ate = np.sqrt(np.mean((estimated_ATE-true_ATE)**2, axis=0)).flatten()\n",
        "    mae_values_ate = np.mean(np.abs(estimated_ATE-true_ATE), axis=0).flatten()\n",
        "    mape_values_ate = np.mean(np.abs((estimated_ATE - true_ATE) / true_ATE), axis=0).flatten() * 100 if true_ATE != 0 else np.nan\n",
        "\n",
        "\n",
        "    # Create DataFrame with the three rows for ATE metrics\n",
        "    df_ate_metrics = pd.DataFrame({\n",
        "        'ATE_RMSE': rmse_values_ate,\n",
        "        'ATE_MAE': mae_values_ate,\n",
        "        'ATE_MAPE': mape_values_ate,\n",
        "    }).T  # Transpose to have metrics as rows\n",
        "\n",
        "    # Create DataFrame for accumulated p-values, using column names like 'PValue_0', 'PValue_1', ...\n",
        "    p_value_columns = [f'PValue_{i}' for i in range(accumulated_p_values.shape[0])] # column names for p-values\n",
        "    df_p_values = pd.DataFrame(accumulated_p_values.T, columns=p_value_columns).T # Transpose so rows are periods, columns are iterations\n",
        "\n",
        "    df = pd.concat([df_ate_metrics, df_p_values], axis=0)\n",
        "\n",
        "    # Save to CSV with the specified filename format\n",
        "    filename = f\"DiD_BCF_ATE_and_PValues{suffix}.csv\" # Changed filename\n",
        "    df.to_csv(filename, header=False)\n",
        "\n",
        "    # Return only ATE metrics\n",
        "    return overall_metrics_ate, per_time_period_metrics_ate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zR60BzSxQWYL"
      },
      "outputs": [],
      "source": [
        "def accumulate_ate_pvalues(model_complex, num_pre_periods, num_post_periods, accumulated_p_values, iteration):\n",
        "    \"\"\"\n",
        "    Accumulates p-values for ATE parameters from a statsmodels model into an existing numpy array.\n",
        "\n",
        "    Args:\n",
        "        model_complex: A statsmodels model results object.\n",
        "        num_pre_periods: The number of pre-treatment periods.\n",
        "        num_post_periods: The number of post-treatment periods.\n",
        "        accumulated_p_values: A numpy array of shape (num_post_periods, 100) to accumulate p-values into.\n",
        "                              Assumes this array has already been initialized with zeros or NaNs.\n",
        "        iteration: The current iteration number (column index in accumulated_p_values).\n",
        "    \"\"\"\n",
        "    for j in range(num_pre_periods, num_pre_periods + num_post_periods):  # Loop from num_pre to num_pre + num_post - 1\n",
        "        parameter_name = f\"C(time)[T.{j}]:treated_group\"\n",
        "        try:\n",
        "            # Access p-values instead of parameters\n",
        "            p_value = model_complex.pvalues[parameter_name]\n",
        "        except KeyError:\n",
        "            p_value = np.nan  # Handle missing parameters robustly\n",
        "        post_period_index = j - num_pre_periods  # Calculate the correct row index for accumulated_p_values\n",
        "\n",
        "        # Check for nan before assignment. If it's nan, just move on to the next parameter.\n",
        "        if np.isnan(p_value):\n",
        "            print(f\"Warning: P-value for parameter {parameter_name} is NaN. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Assign p-value to the specified column (iteration) and row (post-period)\n",
        "        accumulated_p_values[post_period_index, iteration] = p_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIL86pDqQWYP"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNmg9UeXQWYP",
        "outputId": "4c7fbf7e-1ff4-470e-ec54-d48a90fad66a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Progress: 100%|██████████| 100/100 [2:20:46<00:00, 84.46s/iteration]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Metrics:\n",
            "  ATE_rmse: 0.1189\n",
            "  ATE_mae: 0.0777\n",
            "  ATE_mape: nan\n",
            "  ATE_std_rmse: 0.0639\n",
            "  ATE_std_mae: 0.0410\n",
            "  ATE_std_mape: nan\n",
            "\n",
            "Per Time Period Metrics:\n",
            "  Time Period 4:\n",
            "    ATE_rmse: 0.0805\n",
            "    ATE_mae: 0.0612\n",
            "    ATE_mape: nan\n",
            "    ATE_std_mse: 0.0103\n",
            "    ATE_std_mae: 0.0523\n",
            "    ATE_std_mape: nan\n",
            "  Time Period 5:\n",
            "    ATE_rmse: 0.1930\n",
            "    ATE_mae: 0.1280\n",
            "    ATE_mape: nan\n",
            "    ATE_std_mse: 0.0799\n",
            "    ATE_std_mae: 0.1444\n",
            "    ATE_std_mape: nan\n",
            "  Time Period 6:\n",
            "    ATE_rmse: 0.0877\n",
            "    ATE_mae: 0.0687\n",
            "    ATE_mape: nan\n",
            "    ATE_std_mse: 0.0129\n",
            "    ATE_std_mae: 0.0546\n",
            "    ATE_std_mape: nan\n",
            "  Time Period 7:\n",
            "    ATE_rmse: 0.0715\n",
            "    ATE_mae: 0.0528\n",
            "    ATE_mape: nan\n",
            "    ATE_std_mse: 0.0083\n",
            "    ATE_std_mae: 0.0482\n",
            "    ATE_std_mape: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Set the number of covariates as specified.\n",
        "num_x_covariates = 5\n",
        "linearity_degree=2\n",
        "\n",
        "# Set the number of iterations and initialize the counter.\n",
        "num_iterations = 100\n",
        "count_at_least_two_non_significant = 0\n",
        "\n",
        "num_pre_periods=4\n",
        "\n",
        "num_post_periods=4\n",
        "\n",
        "estimated_ATE_subset=np.empty((0,num_post_periods))\n",
        "accumulated_p_values=np.empty((0,num_post_periods))\n",
        "\n",
        "epsilon_scale=1\n",
        "\n",
        "iterations_PTA=[]\n",
        "# Run the loop 100 times.\n",
        "for i in tqdm(range(num_iterations), desc=\"Progress\", unit=\"iteration\"):\n",
        "    # Generate a random seed for each iteration.\n",
        "    seed_val = i\n",
        "\n",
        "    # Generate data with specified hyperparameters.\n",
        "    data_linear = generate_did_data(\n",
        "        n_units=200,\n",
        "        linearity_degree=linearity_degree,\n",
        "        num_pre_periods=num_pre_periods,\n",
        "        num_post_periods=num_post_periods,\n",
        "        pre_trend_bias_delta=0,\n",
        "        num_x_covariates=num_x_covariates,\n",
        "        epsilon_scale=epsilon_scale,\n",
        "        seed=seed_val\n",
        "    )\n",
        "    data_linear['pi_hat']=0.5\n",
        "\n",
        "    data_linear['D']=data_linear['post_treatment']*data_linear['treated_group']\n",
        "    x_columns = [f\"X_{i}\" for i in range(1, num_x_covariates + 1+2)]\n",
        "    X = np.array(data_linear[[\"treated_group\"] + x_columns +[\"time\"]])\n",
        "\n",
        "    Z=np.array(data_linear[\"D\"])\n",
        "    y=np.array(data_linear[\"Y\"])\n",
        "\n",
        "    bcf_model = BCFModel()\n",
        "    general_params = {\"keep_every\": 5, \"num_chains\": 3}\n",
        "    prognostic_forest_params = {\"keep_vars\": np.array([0, 1] + list(range(2, num_x_covariates + 3))+[num_x_covariates + 3])}\n",
        "    treatment_effect_forest_params = {\"keep_vars\": np.array([num_x_covariates + 3])}\n",
        "    bcf_model.sample(X_train=X, Z_train=Z, y_train=y, pi_train=np.array(data_linear['pi_hat']), num_gfr=50, num_mcmc=500, general_params=general_params, prognostic_forest_params=prognostic_forest_params,\n",
        "                treatment_effect_forest_params=treatment_effect_forest_params)\n",
        "\n",
        "\n",
        "    p_values=[]\n",
        "    for i in range(num_pre_periods,num_post_periods+num_pre_periods):\n",
        "        mean_values=bcf_model.tau_hat_train[num_pre_periods:num_post_periods+num_pre_periods,:]\n",
        "        above_zero = np.sum(mean_values > 0)\n",
        "        below_zero = np.sum(mean_values < 0)\n",
        "        total_points = mean_values.size\n",
        "        percentage_above_zero = (above_zero / total_points)\n",
        "        percentage_below_zero = (below_zero / total_points)\n",
        "        #print(min(percentage_above_zero, percentage_below_zero))\n",
        "        p_values.append(min(percentage_above_zero, percentage_below_zero))\n",
        "        #plt.figure(figsize=(10, 6))\n",
        "        #plt.hist(mean_values, bins=20, edgecolor='black', alpha=0.7)\n",
        "        #plt.title('Histogram of Mean Values')\n",
        "        #plt.xlabel('Mean Value')\n",
        "        #plt.ylabel('Frequency')\n",
        "        #plt.grid(True)\n",
        "        #plt.show()\n",
        "    new_row = bcf_model.tau_hat_train.mean(axis=1)[num_pre_periods:num_post_periods+num_pre_periods]\n",
        "    estimated_ATE_subset= np.vstack((estimated_ATE_subset, new_row))\n",
        "    #print(p_values)\n",
        "    new_row = np.array([p_values])\n",
        "    accumulated_p_values= np.vstack((accumulated_p_values, new_row))\n",
        "\n",
        "if linearity_degree == 1 or linearity_degree == 2:\n",
        "    true_ATE = 0\n",
        "elif linearity_degree == 3:\n",
        "    true_ATE = np.array([10,3.8674102,1.4956862,0.5784432])\n",
        "elif linearity_degree == 4:\n",
        "    true_ATE = 0\n",
        "\n",
        "estimated_ATE=estimated_ATE_subset.reshape([estimated_ATE_subset.shape[1],estimated_ATE_subset.shape[0]])\n",
        "accumulated_p_values=accumulated_p_values.reshape([accumulated_p_values.shape[1],accumulated_p_values.shape[0]])\n",
        "\n",
        "try:\n",
        "    overall_metrics, per_time_period_metrics = calculate_error_metrics(estimated_ATE,accumulated_p_values, true_ATE,\"_ATE_linearity=1\")\n",
        "    print(\"Overall Metrics:\")\n",
        "    for metric, value in overall_metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    print(\"\\nPer Time Period Metrics:\")\n",
        "    for time_period, metrics in per_time_period_metrics.items():\n",
        "        print(f\"  Time Period {time_period+num_pre_periods}:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"    {metric}: {value:.4f}\")\n",
        "except:\n",
        "  print(\"No simulation has been passed the pre-trends test\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP6gnm_xQme3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
