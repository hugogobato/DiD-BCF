{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p76dXXNQWYB",
        "outputId": "f6373fba-2ca1-4ae4-ec87-14c0f6539f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/StochasticTree/stochtree.git\n",
            "  Cloning https://github.com/StochasticTree/stochtree.git to /tmp/pip-req-build-n52g6t7d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/StochasticTree/stochtree.git /tmp/pip-req-build-n52g6t7d\n",
            "  Resolved https://github.com/StochasticTree/stochtree.git to commit f55bbb47b57ef6160964084650ab81f557c9559c\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stochtree\n",
            "  Building wheel for stochtree (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stochtree: filename=stochtree-0.1.0-cp311-cp311-linux_x86_64.whl size=871294 sha256=d0109bc2a85e428a87126697ce947e3039542abd26a21d45b95cd767d1a4dc11\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0fbjbxcz/wheels/6b/16/bb/b09e1d07fb9c44bfd19200620859a0fdda75287afaa4a076bf\n",
            "Successfully built stochtree\n",
            "Installing collected packages: stochtree\n",
            "Successfully installed stochtree-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/StochasticTree/stochtree.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VgjqGpE_QWYH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hg0viKgUQWYI"
      },
      "outputs": [],
      "source": [
        "def generate_staggered_did_data_fixed_X(\n",
        "    n_units=200,\n",
        "    num_pre_periods=5,\n",
        "    num_post_periods=5,\n",
        "    linearity_degree=1,\n",
        "    pre_trend_bias_delta=0.2,\n",
        "    propensity_noise_scale=0.5, # Scale of noise added to utility for assignment randomness\n",
        "    epsilon_scale=1,\n",
        "    seed=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates panel data for DiD with staggered adoption based on propensity scores,\n",
        "    using a fixed set of 8 covariates with mixed static/dynamic properties.\n",
        "\n",
        "    Covariates (8 total):\n",
        "    - X1: Bernoulli(p=0.66) - STATIC, influences propensity\n",
        "    - X2: Bernoulli(p=0.45) - Time-varying\n",
        "    - X3: Categorical({1,2,3,4} p={0.3,0.1,0.2,0.4}) - Time-varying\n",
        "    - X4-X7: Numerical (Normal(0,1)) - Time-varying\n",
        "    - X8: Numerical (Normal(0,1)) - STATIC, influences propensity\n",
        "\n",
        "    Assigns units to 4 groups based on STATIC covariates (X1, X8):\n",
        "    - Group 0: Never treated (Control) - Baseline\n",
        "    - Group 1: Treated starting at num_pre_periods (T0)\n",
        "    - Group 2: Treated starting at num_pre_periods + 1 (T1)\n",
        "    - Group 3: Treated starting at num_pre_periods + 2 (T2)\n",
        "\n",
        "    Args:\n",
        "        n_units (int): Total number of units.\n",
        "        num_pre_periods (int): Periods before the *earliest* treatment.\n",
        "        num_post_periods (int): Periods after the *earliest* treatment.\n",
        "        linearity_degree (int): Degree of linearity in the DGP (1-4).\n",
        "        pre_trend_bias_delta (float): Bias for pre-trends in eventually treated groups.\n",
        "        propensity_noise_scale (float): Std deviation of noise added to group utility\n",
        "                                        before assignment. Higher -> more random assignment.\n",
        "        epsilon_scale (float): Std deviation of the outcome error term.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Generated panel data including propensity-based group assignment\n",
        "                      and mixed static/time-varying covariates.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    total_covariates = 8 # Fixed number of covariates\n",
        "\n",
        "    # --- 1. Generate STATIC Unit-Level Covariates (for Propensity Score) ---\n",
        "    unit_ids = np.arange(n_units)\n",
        "    # X1: Static Bernoulli(p=0.66)\n",
        "    unit_X1_bern = np.random.binomial(n=1, p=0.66, size=n_units)\n",
        "    # X8: Static Numerical (Normal(0,1))\n",
        "    unit_X8_num = np.random.normal(0, 1, size=n_units)\n",
        "\n",
        "    # --- 2. Calculate Group Utilities and Assign Groups based on STATIC X1, X8 ---\n",
        "    # Define coefficients linking ONLY static covariates (X1, X8) to utility\n",
        "    coeffs = {\n",
        "        # Group 1 (T0) - Example: Higher utility if X1=1 and high X8\n",
        "        'g1': {'intercept': 0.1, 'x1_bern': 0.8, 'x8_num': 0.6},\n",
        "        # Group 2 (T1) - Example: Higher utility if X1=0 and low X8\n",
        "        'g2': {'intercept': 0.0, 'x1_bern': -0.5, 'x8_num': -0.7},\n",
        "        # Group 3 (T2) - Example: Mild preference for X1=1, sensitive to X8\n",
        "        'g3': {'intercept': -0.1, 'x1_bern': 0.3, 'x8_num': 0.4}\n",
        "    }\n",
        "\n",
        "    # Calculate systematic utility part (V = X*beta) for each group\n",
        "    V0 = np.zeros(n_units) # Utility for Group 0 (Control) is baseline 0\n",
        "    V1 = coeffs['g1']['intercept'] + coeffs['g1']['x1_bern'] * unit_X1_bern + coeffs['g1']['x8_num'] * unit_X8_num\n",
        "    V2 = coeffs['g2']['intercept'] + coeffs['g2']['x1_bern'] * unit_X1_bern + coeffs['g2']['x8_num'] * unit_X8_num\n",
        "    V3 = coeffs['g3']['intercept'] + coeffs['g3']['x1_bern'] * unit_X1_bern + coeffs['g3']['x8_num'] * unit_X8_num\n",
        "\n",
        "    # Add random noise\n",
        "    noise = np.random.normal(0, propensity_noise_scale, size=(n_units, 4))\n",
        "    U = np.column_stack((V0, V1, V2, V3)) + noise\n",
        "\n",
        "    # Assign unit to group with highest utility\n",
        "    unit_treatment_group = np.argmax(U, axis=1)\n",
        "\n",
        "    # --- 3. Create Panel DataFrame and Merge STATIC Unit-Level Info ---\n",
        "    periods = num_pre_periods + num_post_periods\n",
        "    time_periods = np.arange(periods)\n",
        "\n",
        "    data = pd.DataFrame({\n",
        "        'unit_id': np.repeat(unit_ids, periods),\n",
        "        'time': np.tile(time_periods, n_units)\n",
        "    })\n",
        "\n",
        "    # Create a temporary DataFrame for unit-level data\n",
        "    df_unit_static = pd.DataFrame({\n",
        "        'unit_id': unit_ids,\n",
        "        'treatment_group': unit_treatment_group,\n",
        "        'X1': unit_X1_bern, # Static Bernoulli\n",
        "        'X8': unit_X8_num   # Static Numerical\n",
        "    })\n",
        "\n",
        "    # Merge static unit-level data into the main panel DataFrame\n",
        "    data = pd.merge(data, df_unit_static, on='unit_id', how='left')\n",
        "\n",
        "    # --- 4. Generate Time-Varying Covariates ---\n",
        "    n_observations = len(data)\n",
        "    # X2: Time-varying Bernoulli(p=0.45)\n",
        "    data['X2'] = np.random.binomial(n=1, p=0.45, size=n_observations)\n",
        "    # X3: Time-varying Categorical\n",
        "    cat_choices = [1, 2, 3, 4]\n",
        "    cat_probs = [0.3, 0.1, 0.2, 0.4]\n",
        "    data['X3'] = np.random.choice(cat_choices, size=n_observations, p=cat_probs)\n",
        "    # X4-X7: Time-varying Numerical (Normal(0,1)) - 4 covariates\n",
        "    X_num_time_varying = np.random.normal(0, 1, size=(n_observations, 4))\n",
        "    data['X4'] = X_num_time_varying[:, 0]\n",
        "    data['X5'] = X_num_time_varying[:, 1]\n",
        "    data['X6'] = X_num_time_varying[:, 2]\n",
        "    data['X7'] = X_num_time_varying[:, 3]\n",
        "\n",
        "    # --- 5. Define Treatment Timing and Indicators ---\n",
        "    earliest_treatment_period = num_pre_periods\n",
        "    conditions = [\n",
        "        data['treatment_group'] == 0, data['treatment_group'] == 1,\n",
        "        data['treatment_group'] == 2, data['treatment_group'] == 3\n",
        "    ]\n",
        "    choices = [ np.inf, earliest_treatment_period, earliest_treatment_period + 1, earliest_treatment_period + 2 ]\n",
        "    data['first_treat_period'] = np.select(conditions, choices, default=np.nan)\n",
        "    data['post_treatment'] = (data['time'] >= num_pre_periods).astype(int)\n",
        "    data['eventually_treated'] = (data['treatment_group'] > 0).astype(int)\n",
        "    data['D'] = (data['time'] >= data['first_treat_period']).astype(int)\n",
        "    data['time_trend'] = data['time']\n",
        "\n",
        "    # --- 6. Generate Outcome Variable (Y) using FIXED beta_x ---\n",
        "    if linearity_degree == 1 or linearity_degree == 2: treatment_effect_beta = 0\n",
        "    elif linearity_degree == 3: treatment_effect_beta = 0\n",
        "    else: treatment_effect_beta = np.nan\n",
        "\n",
        "    data['epsilon'] = np.random.normal(scale=epsilon_scale, size=len(data))\n",
        "\n",
        "    # DGP parameters\n",
        "    beta_0 = -0.5 # Intercept\n",
        "    beta_group_effect = 0.75 # Main effect of treated group (alpha_i)\n",
        "    beta_time = 0.2 # Main effect of time trend (gamma_t)\n",
        "    beta_interaction = treatment_effect_beta # Treatment effect magnitude\n",
        "\n",
        "    # FIXED coefficients for the 8 covariates\n",
        "    beta_x = np.array([-0.75, 0.5, -0.5, -1.30, 1.8, 2.5, -1.0, 0.3])\n",
        "    if len(beta_x) != total_covariates:\n",
        "        raise ValueError(f\"Length of fixed beta_x ({len(beta_x)}) does not match total_covariates ({total_covariates})\")\n",
        "\n",
        "\n",
        "    # Prepare covariate matrix X from DataFrame columns in order X1 to X8\n",
        "    X_cols = [f'X{i}' for i in range(1, total_covariates + 1)]\n",
        "    X = data[X_cols].values # Shape (n_observations, 8)\n",
        "\n",
        "    # --- Calculate Y based on linearity_degree ---\n",
        "    Y_base = (beta_0 + beta_group_effect * data['eventually_treated'] + beta_time * data['time_trend'])\n",
        "    half = total_covariates // 2 # half = 4\n",
        "\n",
        "    if linearity_degree == 1: # Fully Linear\n",
        "        Y_covariates = np.sum(beta_x * X, axis=1)\n",
        "        Y_treatment = beta_interaction * data['D']\n",
        "        data['CATE'] = beta_interaction * data['D']\n",
        "\n",
        "    elif linearity_degree == 2: # Half X non-linear\n",
        "        # beta_x indices: [0,1] [2,3] [4,5,6,7]\n",
        "        # X columns    :  0,1   2,3   4,5,6,7\n",
        "        cov_effect = (np.sum(beta_x[:2] * (X[:, :2] ** 2), axis=1) +        # First 2 X's squared\n",
        "                      np.sum(beta_x[2:4] * np.exp(X[:, 2:4]), axis=1) +     # Next 2 X's exp\n",
        "                      np.sum(beta_x[4:] * X[:, 4:], axis=1))               # Last 4 X's linear\n",
        "        Y_covariates = cov_effect\n",
        "        Y_treatment = beta_interaction * data['D']\n",
        "        data['CATE'] = beta_interaction * data['D']\n",
        "\n",
        "    elif linearity_degree == 3: \n",
        "        Y_base = (beta_0 + beta_group_effect * data['eventually_treated'] + beta_time * data['time_trend']**2) # Non-linear time\n",
        "        # beta_x indices: [0,1] [2,3] [4,5] [6,7]\n",
        "        # X columns    :  0,1   2,3   4,5   6,7\n",
        "        cov_effect = (np.sum(beta_x[:2] * (X[:, :2] ** 2), axis=1) +         # First 2 X's squared\n",
        "                      np.sum(beta_x[2:4] * np.exp(X[:, 2:4]), axis=1) +      # Next 2 X's exp\n",
        "                      np.sum(beta_x[4:6] * np.abs(X[:, 4:6]), axis=1) +      # Next 2 X's abs\n",
        "                      np.sum(beta_x[6:] * np.sqrt(np.abs(X[:, 6:])), axis=1))# Last 2 X's sqrt(abs)\n",
        "        Y_covariates = cov_effect\n",
        "        Y_treatment = beta_interaction * data['D'] # Linear treatment\n",
        "        data['CATE'] = beta_interaction * data['D']\n",
        "    else:\n",
        "         Y_covariates = 0\n",
        "         Y_treatment = 0\n",
        "         data['CATE'] = 0\n",
        "\n",
        "    data['Y'] = Y_base + Y_covariates + Y_treatment\n",
        "\n",
        "    # --- Add pre-trend bias ---\n",
        "    if pre_trend_bias_delta != 0:\n",
        "        pre_period_mask = data['time'] < earliest_treatment_period\n",
        "        bias_mask = pre_period_mask & (data['eventually_treated'] == 1)\n",
        "        if linearity_degree == 3: \n",
        "            seasonal_amplitude = 1.0\n",
        "            seasonal_period = 4\n",
        "            seasonal_effect = seasonal_amplitude * np.sin(2 * np.pi * data['time'] / seasonal_period)\n",
        "            data.loc[bias_mask, 'Y'] += pre_trend_bias_delta * seasonal_effect[bias_mask]\n",
        "        else:\n",
        "            time_diff = data['time'] - earliest_treatment_period\n",
        "            data.loc[bias_mask, 'Y'] += pre_trend_bias_delta * time_diff[bias_mask]\n",
        "\n",
        "    # Add final error term\n",
        "    data['Y'] += data['epsilon']\n",
        "\n",
        "    # --- 7. Finalize DataFrame ---\n",
        "    # Reorder columns for clarity (optional)\n",
        "    final_cols = (['unit_id', 'time', 'treatment_group', 'first_treat_period', 'eventually_treated', 'D','post_treatment'] +\n",
        "                   X_cols + ['Y', 'CATE', 'time_trend', 'epsilon'])\n",
        "    # Ensure all columns exist before selecting\n",
        "    final_cols = [col for col in final_cols if col in data.columns]\n",
        "    data = data[final_cols]\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R9xVDSM3cnE7"
      },
      "outputs": [],
      "source": [
        "def find_first_treatment_indexes_array(df, min_time=4, eventually_treated=1):\n",
        "    \"\"\"\n",
        "    Finds the indexes of the first row for each treatment group (0, 1, 2, 3)\n",
        "    after filtering the DataFrame by time and eventually_treated, and returns them as a NumPy array.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame.\n",
        "        min_time: The minimum time value.\n",
        "        eventually_treated: The desired eventually_treated value.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array containing the first row indexes for each treatment group (0, 1, 2, 3),\n",
        "        or None if no rows meet the criteria. Returns -1 if a treatment group does not appear in the filtered data.\n",
        "    \"\"\"\n",
        "\n",
        "    filtered_df = df[(df['time'] >= min_time) & (df['eventually_treated'] == eventually_treated)]\n",
        "\n",
        "    if filtered_df.empty:\n",
        "        return None  # Return None if no rows match the time and eventually_treated criteria.\n",
        "\n",
        "    indexes = []\n",
        "    for group in [1, 2, 3]:\n",
        "        group_df = filtered_df[filtered_df['treatment_group'] == group]\n",
        "        if not group_df.empty:\n",
        "            indexes.append(group_df.index[0])  # Get the first index\n",
        "        else:\n",
        "            indexes.append(-1) #Return -1 if the treatment group does not appear in the filtered data.\n",
        "\n",
        "    return np.array(indexes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mRsSJjwMQWYK"
      },
      "outputs": [],
      "source": [
        "from stochtree import BCFModel\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1Ax6IhACQWYK"
      },
      "outputs": [],
      "source": [
        "def calculate_error_metrics_grouped_hybrid( # Renamed slightly for clarity\n",
        "    true_ATE,\n",
        "    estimated_ATE,\n",
        "    accumulated_p_values,\n",
        "    suffix=\"\"\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Calculates both per-iteration and summary metrics (RMSE, MAE, MAPE).\n",
        "\n",
        "    1. Saves per-iteration results to an Excel file named\n",
        "       \"BCF_GATE_and_PValues{suffix}.xlsx\". The file has multiple sheets,\n",
        "       with each row representing a simulation iteration:\n",
        "       - 'Overall_Metrics': Contains overall RMSE, MAE, MAPE per iteration.\n",
        "       - 'Group_X': One sheet per group, containing the group's RMSE, MAE, MAPE\n",
        "                    per iteration, alongside the raw p-values for that group.\n",
        "\n",
        "    2. Returns dictionaries containing summary statistics (mean and standard\n",
        "       deviation of metrics aggregated across all iterations).\n",
        "\n",
        "    Args:\n",
        "        true_ATE: A numpy array of shape (num_iterations, num_post_periods, number_of_groups)\n",
        "                  containing the true ATE values.\n",
        "        estimated_ATE: A numpy array of shape (num_iterations, num_post_periods, number_of_groups)\n",
        "                       containing the estimated ATE values.\n",
        "        accumulated_p_values: A numpy array of shape (num_iterations, num_post_periods, number_of_groups)\n",
        "                              containing the p-values.\n",
        "        suffix (str): An optional suffix to append to the base filename\n",
        "                      \"BCF_GATE_and_PValues\". Defaults to \"\".\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - overall_metrics: Dictionary with overall summary statistics\n",
        "              (mean RMSE, mean MAE, mean MAPE, std RMSE, std MAE, std MAPE).\n",
        "            - per_group_metrics: Dictionary where keys are group indices and\n",
        "              values are dictionaries with summary statistics for that group.\n",
        "    \"\"\"\n",
        "\n",
        "    # Input validation\n",
        "    if not (true_ATE.shape == estimated_ATE.shape == accumulated_p_values.shape):\n",
        "        raise ValueError(\"Shapes of true_ATE, estimated_ATE, and accumulated_p_values must match.\")\n",
        "    if true_ATE.ndim != 3:\n",
        "         raise ValueError(\"Input arrays must have 3 dimensions: (iterations, time, groups).\")\n",
        "\n",
        "    num_iterations, num_post_periods, number_of_groups = true_ATE.shape\n",
        "    iteration_index = pd.RangeIndex(num_iterations, name='Iteration')\n",
        "\n",
        "    # Construct filename using suffix\n",
        "    filename = f\"detrend_unbiased_BCF_GATE_PS_and_PValues{suffix}.xlsx\"\n",
        "\n",
        "    # Calculate element-wise errors\n",
        "    errors = estimated_ATE - true_ATE # Shape: (iterations, time, groups)\n",
        "    abs_errors = np.abs(errors)       # Shape: (iterations, time, groups)\n",
        "\n",
        "    # --- Calculate PER-ITERATION Metrics (used for both Excel and summaries) ---\n",
        "\n",
        "    # Overall per-iteration metrics\n",
        "    overall_rmse_per_iteration = np.sqrt(np.mean(errors**2, axis=(1, 2)))\n",
        "    overall_mae_per_iteration = np.mean(abs_errors, axis=(1, 2))\n",
        "    overall_mape_per_iteration = np.zeros(num_iterations) * np.nan\n",
        "    for i in range(num_iterations):\n",
        "         true_ate_i = true_ATE[i, :, :]\n",
        "         errors_i = errors[i, :, :]\n",
        "         valid_mask_i = true_ate_i != 0\n",
        "         if np.any(valid_mask_i):\n",
        "             abs_perc_errors_i = np.abs(errors_i[valid_mask_i] / true_ate_i[valid_mask_i])\n",
        "             overall_mape_per_iteration[i] = np.mean(abs_perc_errors_i) * 100\n",
        "\n",
        "    # Create Overall DataFrame for Excel\n",
        "    df_overall = pd.DataFrame({\n",
        "        'Overall_RMSE': overall_rmse_per_iteration,\n",
        "        'Overall_MAE': overall_mae_per_iteration,\n",
        "        'Overall_MAPE': overall_mape_per_iteration\n",
        "    }, index=iteration_index)\n",
        "\n",
        "    # --- Calculate SUMMARY Overall Metrics (for return value) ---\n",
        "    overall_rmse = np.sqrt(np.mean(errors**2))\n",
        "    overall_mae = np.mean(abs_errors)\n",
        "    valid_mape_mask = true_ATE != 0\n",
        "    abs_perc_errors = np.full_like(errors, fill_value=np.nan)\n",
        "    abs_perc_errors[valid_mape_mask] = np.abs(errors[valid_mape_mask] / true_ATE[valid_mape_mask])\n",
        "    overall_mape = np.nanmean(abs_perc_errors) * 100\n",
        "\n",
        "    summary_overall_std_rmse = np.nanstd(overall_rmse_per_iteration)\n",
        "    summary_overall_std_mae = np.nanstd(overall_mae_per_iteration)\n",
        "    summary_overall_std_mape = np.nanstd(overall_mape_per_iteration)\n",
        "\n",
        "    overall_metrics = { # Dictionary for return value\n",
        "        \"Overall_RMSE\": overall_rmse,\n",
        "        \"Overall_MAE\": overall_mae,\n",
        "        \"Overall_MAPE\": overall_mape,\n",
        "        \"Overall_Std_RMSE\": summary_overall_std_rmse,\n",
        "        \"Overall_Std_MAE\": summary_overall_std_mae,\n",
        "        \"Overall_Std_MAPE\": summary_overall_std_mape,\n",
        "    }\n",
        "\n",
        "    # --- Process Per-Group Data (for both Excel and summaries) ---\n",
        "    group_combined_dfs = {} # For Excel sheets\n",
        "    per_group_metrics = {}  # For return value summaries\n",
        "\n",
        "    for g in range(number_of_groups):\n",
        "        # Slice data for the current group\n",
        "        true_ATE_g = true_ATE[:, :, g]  # Shape: (iterations, time)\n",
        "        errors_g = errors[:, :, g]      # Shape: (iterations, time)\n",
        "        abs_errors_g = abs_errors[:, :, g]# Shape: (iterations, time)\n",
        "        p_values_g = accumulated_p_values[:, :, g] # Shape: (iterations, time)\n",
        "\n",
        "        # Calculate per-iteration metrics for group g\n",
        "        group_rmse_per_iter = np.sqrt(np.mean(errors_g**2, axis=1))\n",
        "        group_mae_per_iter = np.mean(abs_errors_g, axis=1)\n",
        "        group_mape_per_iter = np.zeros(num_iterations) * np.nan\n",
        "        for i in range(num_iterations):\n",
        "            true_ate_gi = true_ATE_g[i, :]\n",
        "            errors_gi = errors_g[i, :]\n",
        "            valid_mask_gi = true_ate_gi != 0\n",
        "            if np.any(valid_mask_gi):\n",
        "                abs_perc_errors_gi = np.abs(errors_gi[valid_mask_gi] / true_ate_gi[valid_mask_gi])\n",
        "                group_mape_per_iter[i] = np.mean(abs_perc_errors_gi) * 100\n",
        "\n",
        "        # --- Calculate SUMMARY Stats for Group g (for return dict) ---\n",
        "        group_rmse = np.sqrt(np.mean(errors_g**2))\n",
        "        group_mae = np.mean(abs_errors_g)\n",
        "        valid_mape_mask_g = true_ATE_g != 0\n",
        "        abs_perc_errors_g = np.full_like(errors_g, fill_value=np.nan)\n",
        "        abs_perc_errors_g[valid_mape_mask_g] = np.abs(errors_g[valid_mape_mask_g] / true_ATE_g[valid_mape_mask_g])\n",
        "        group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
        "\n",
        "        summary_group_std_rmse = np.nanstd(group_rmse_per_iter)\n",
        "        summary_group_std_mae = np.nanstd(group_mae_per_iter)\n",
        "        summary_group_std_mape = np.nanstd(group_mape_per_iter)\n",
        "\n",
        "        per_group_metrics[g] = { # Populate return dictionary for group g\n",
        "             f\"Group_{g}_RMSE\": group_rmse,\n",
        "             f\"Group_{g}_MAE\": group_mae,\n",
        "             f\"Group_{g}_MAPE\": group_mape,\n",
        "             f\"Group_{g}_Std_RMSE\": summary_group_std_rmse,\n",
        "             f\"Group_{g}_Std_MAE\": summary_group_std_mae,\n",
        "             f\"Group_{g}_Std_MAPE\": summary_group_std_mape,\n",
        "        }\n",
        "\n",
        "        # --- Create DataFrames for Excel Sheet for Group g ---\n",
        "        df_metrics_g = pd.DataFrame({\n",
        "            f'Group_{g}_RMSE': group_rmse_per_iter, # Use the per-iter arrays\n",
        "            f'Group_{g}_MAE': group_mae_per_iter,\n",
        "            f'Group_{g}_MAPE': group_mape_per_iter\n",
        "        }, index=iteration_index)\n",
        "\n",
        "        p_value_columns = [f'PValue_Time_{t}' for t in range(num_post_periods)]\n",
        "        df_pvals_g = pd.DataFrame(p_values_g,\n",
        "                                  index=iteration_index,\n",
        "                                  columns=p_value_columns)\n",
        "\n",
        "        # Combine metrics and p-values for the group's Excel sheet\n",
        "        group_combined_dfs[g] = pd.concat([df_metrics_g, df_pvals_g], axis=1)\n",
        "\n",
        "\n",
        "    # --- Write Per-Iteration Data to Excel ---\n",
        "    try:\n",
        "        with pd.ExcelWriter(filename) as writer:\n",
        "            # Write Overall Metrics Sheet\n",
        "            df_overall.to_excel(writer, sheet_name='Overall_Metrics', header=True, index=True)\n",
        "            # Write Per-Group Sheets\n",
        "            for g in range(number_of_groups):\n",
        "                sheet_name_g = f'Group_{g}'\n",
        "                group_combined_dfs[g].to_excel(writer, sheet_name=sheet_name_g, header=True, index=True)\n",
        "\n",
        "        print(f\"Per-iteration metrics and p-values successfully saved to '{filename}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving per-iteration metrics to Excel file '{filename}': {e}\")\n",
        "\n",
        "    # --- Return Summary Dictionaries ---\n",
        "    return overall_metrics, per_group_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIL86pDqQWYP"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNmg9UeXQWYP",
        "outputId": "b85b697b-baab-4ac7-c091-f5d94157d10d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Progress: 100%|██████████| 100/100 [2:22:00<00:00, 85.21s/iteration]\n",
            "<ipython-input-6-3c81f95709e8>:81: RuntimeWarning: Mean of empty slice\n",
            "  overall_mape = np.nanmean(abs_perc_errors) * 100\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
            "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "<ipython-input-6-3c81f95709e8>:125: RuntimeWarning: Mean of empty slice\n",
            "  group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
            "<ipython-input-6-3c81f95709e8>:125: RuntimeWarning: Mean of empty slice\n",
            "  group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
            "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "<ipython-input-6-3c81f95709e8>:125: RuntimeWarning: Mean of empty slice\n",
            "  group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
            "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Per-iteration metrics and p-values successfully saved to 'detrend_unbiased_BCF_GATE_PS_and_PValues_linearity=2.xlsx'\n",
            "\n",
            "--- Overall Metrics (Dictionary) ---\n",
            "{'Overall_RMSE': np.float64(0.1983609319231804), 'Overall_MAE': np.float64(0.13200967103521336), 'Overall_MAPE': np.float64(nan), 'Overall_Std_RMSE': np.float64(0.11889876900243643), 'Overall_Std_MAE': np.float64(0.10275309789020365), 'Overall_Std_MAPE': np.float64(nan)}\n",
            "\n",
            "--- Per Group Metrics (Dictionary) ---\n",
            "Group 0:\n",
            "{'Group_0_RMSE': np.float64(0.23311440718349086), 'Group_0_MAE': np.float64(0.1755016649684547), 'Group_0_MAPE': np.float64(nan), 'Group_0_Std_RMSE': np.float64(0.14644076349391233), 'Group_0_Std_MAE': np.float64(0.14704093995045917), 'Group_0_Std_MAPE': np.float64(nan)}\n",
            "Group 1:\n",
            "{'Group_1_RMSE': np.float64(0.1994532911811355), 'Group_1_MAE': np.float64(0.13486382006127434), 'Group_1_MAPE': np.float64(nan), 'Group_1_Std_RMSE': np.float64(0.11988809473614313), 'Group_1_Std_MAE': np.float64(0.10389729858600937), 'Group_1_Std_MAPE': np.float64(nan)}\n",
            "Group 2:\n",
            "{'Group_2_RMSE': np.float64(0.15465198266042576), 'Group_2_MAE': np.float64(0.085663528075911), 'Group_2_MAPE': np.float64(nan), 'Group_2_Std_RMSE': np.float64(0.09409375237506583), 'Group_2_Std_MAE': np.float64(0.0667219820889841), 'Group_2_Std_MAPE': np.float64(nan)}\n"
          ]
        }
      ],
      "source": [
        "# Set the number of covariates as specified.\n",
        "num_x_covariates = 6\n",
        "linearity_degree=2\n",
        "\n",
        "# Set the number of iterations and initialize the counter.\n",
        "num_iterations = 100\n",
        "count_at_least_two_non_significant = 0\n",
        "\n",
        "num_pre_periods=4\n",
        "\n",
        "num_post_periods=4\n",
        "\n",
        "number_of_groups=3\n",
        "true_ATE=np.zeros([num_iterations,num_post_periods,number_of_groups])\n",
        "estimated_ATE_subset=np.zeros([num_iterations,num_post_periods,number_of_groups])\n",
        "accumulated_p_values=np.zeros([num_iterations,num_post_periods,number_of_groups])\n",
        "\n",
        "epsilon_scale=1\n",
        "\n",
        "# Run the loop 100 times.\n",
        "for i in tqdm(range(num_iterations), desc=\"Progress\", unit=\"iteration\"):\n",
        "    # Generate a random seed for each iteration.\n",
        "    seed_val = i\n",
        "\n",
        "    # Generate data with specified hyperparameters.\n",
        "    data_linear = generate_staggered_did_data_fixed_X(\n",
        "        n_units=200,\n",
        "        linearity_degree=linearity_degree,\n",
        "        num_pre_periods=num_pre_periods,\n",
        "        num_post_periods=num_post_periods,\n",
        "        pre_trend_bias_delta=0,\n",
        "        epsilon_scale=epsilon_scale,\n",
        "        seed=seed_val\n",
        "    )\n",
        "    indexes = find_first_treatment_indexes_array(data_linear)\n",
        "\n",
        "    x_columns = [f\"X{i}\" for i in range(1, num_x_covariates + 1+2)]\n",
        "    X = np.array(data_linear[[\"eventually_treated\"] + x_columns +[\"post_treatment\"]+[\"time\"]+[\"treatment_group\"]])\n",
        "    Z=np.array(data_linear[\"D\"])\n",
        "    y=np.array(data_linear[\"Y\"])\n",
        "\n",
        "    bcf_model = BCFModel()\n",
        "    general_params = {\"keep_every\": 5, \"num_chains\": 3}\n",
        "    prognostic_forest_params = {\"keep_vars\": np.array([0, 1] + list(range(2, num_x_covariates + 3))+[num_x_covariates + 4])}\n",
        "    treatment_effect_forest_params = {\"keep_vars\": np.array([num_x_covariates + 3,num_x_covariates + 4,num_x_covariates + 5])}\n",
        "    bcf_model.sample(X_train=X, Z_train=Z, y_train=y, num_gfr=50, num_mcmc=500, general_params=general_params, prognostic_forest_params=prognostic_forest_params,\n",
        "                treatment_effect_forest_params=treatment_effect_forest_params)\n",
        "\n",
        "    for j in range(len(indexes)):\n",
        "      true_ATE[i,:,j]=np.array(data_linear[(data_linear['time'] >= num_pre_periods) & data_linear['eventually_treated'] == 1][\"CATE\"].loc[indexes[j]:indexes[j]+num_post_periods])\n",
        "      estimated_ATE_subset[i,:,j]=bcf_model.tau_hat_train.mean(axis=1)[indexes[j]:indexes[j]+num_post_periods]*Z[indexes[j]:indexes[j]+num_post_periods]-np.mean(bcf_model.tau_hat_train.mean(axis=1)[indexes[j]-4:indexes[j]]*Z[indexes[j]-4:indexes[j]])\n",
        "\n",
        "    for h in range(num_pre_periods,num_post_periods+num_pre_periods):\n",
        "      for j in range(len(indexes)):\n",
        "        mean_values=(bcf_model.tau_hat_train[indexes[j]:indexes[j]+num_post_periods,:]*Z[indexes[j]:indexes[j]+num_post_periods].reshape(-1, 1)-0*bcf_model.tau_hat_train[indexes[j]-4:indexes[j],:].mean(axis=0))[h-num_pre_periods,:]\n",
        "        above_zero = np.sum(mean_values > 0)\n",
        "        below_zero = np.sum(mean_values < 0)\n",
        "        total_points = mean_values.size\n",
        "        percentage_above_zero = (above_zero / total_points)\n",
        "        percentage_below_zero = (below_zero / total_points)\n",
        "        accumulated_p_values[i,h-num_pre_periods,j]=min(percentage_above_zero, percentage_below_zero)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "simulation_suffix = \"_linearity=2\" # Example suffix\n",
        "overall_metrics, per_group_metrics = calculate_error_metrics_grouped_hybrid(\n",
        "    true_ATE,\n",
        "    estimated_ATE_subset,\n",
        "    accumulated_p_values,\n",
        "    suffix=simulation_suffix # Pass the suffix here\n",
        ")\n",
        "\n",
        "print(\"\\n--- Overall Metrics (Dictionary) ---\")\n",
        "print(overall_metrics)\n",
        "\n",
        "print(\"\\n--- Per Group Metrics (Dictionary) ---\")\n",
        "for group_idx, metrics in per_group_metrics.items():\n",
        "    print(f\"Group {group_idx}:\")\n",
        "    print(metrics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lZgpfpnnPmk3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
