{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_staggered_did_data(\n",
    "    n_units=200,\n",
    "    num_x_covariates=5,\n",
    "    num_pre_periods=5,\n",
    "    num_post_periods=5,\n",
    "    linearity_degree=1, # 1: fully linear, 2: half X non-linear, 3: treatment + all X non-linear\n",
    "    pre_trend_bias_delta=0.2,\n",
    "    epsilon_scale=1,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates panel data for Difference-in-Differences analysis with staggered adoption,\n",
    "    controllable pre-trends, and non-linearity.\n",
    "\n",
    "    Creates 4 groups:\n",
    "    - Group 0: Never treated (Control)\n",
    "    - Group 1: Treated starting at num_pre_periods (First Treatment Time)\n",
    "    - Group 2: Treated starting at num_pre_periods + 1\n",
    "    - Group 3: Treated starting at num_pre_periods + 2\n",
    "\n",
    "    Args:\n",
    "        n_units (int): Total number of units (e.g., individuals, firms). Should be divisible by 4 ideally.\n",
    "        num_x_covariates (int): Number of control covariates (X) (not counting W1 and W2).\n",
    "        num_pre_periods (int): Number of periods before the *earliest* treatment.\n",
    "        num_post_periods (int): Number of periods after the *earliest* treatment.\n",
    "        linearity_degree (int): Degree of linearity in the DGP:\n",
    "            1: Fully linear.\n",
    "            2: Half of X covariates have non-linear relationship with Y.\n",
    "            3: Treatment and half of X covariates have non-linear relationship with Y.\n",
    "            4: Treatment and all X covariates have non-linear relationship with Y.\n",
    "        pre_trend_bias_delta (float): Bias parameter to induce pre-trends in eventually treated groups.\n",
    "        epsilon_scale (float): Standard deviation of the error term.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Generated panel data in long format. Includes columns:\n",
    "            'unit_id': Unique identifier for each unit.\n",
    "            'time': Time period index.\n",
    "            'treatment_group': Integer indicating the unit's group (0: Control, 1: T0, 2: T0+1, 3: T0+2).\n",
    "            'first_treat_period': The period when treatment starts for the unit (np.inf for control).\n",
    "            'eventually_treated': Binary indicator (1 if unit belongs to groups 1, 2, or 3, 0 otherwise).\n",
    "            'D': Binary treatment indicator (1 if unit is treated *in the current period*, 0 otherwise).\n",
    "            'X_1', 'X_2', ...: Covariates.\n",
    "            'Y': Outcome variable.\n",
    "            'CATE': True Conditional Average Treatment Effect for the unit-period.\n",
    "            'epsilon': Error term component.\n",
    "            'time_trend': Linear time trend index.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # --- Set treatment_effect_beta based on linearity_degree ---\n",
    "    if linearity_degree == 1 or linearity_degree == 2:\n",
    "        treatment_effect_beta = 0\n",
    "    elif linearity_degree == 3:\n",
    "        treatment_effect_beta = 0\n",
    "    else:\n",
    "        print(f\"Warning: linearity_degree ({linearity_degree}) has an unexpected value. Setting treatment_effect_beta to NaN.\")\n",
    "        treatment_effect_beta = np.nan\n",
    "\n",
    "    periods = num_pre_periods + num_post_periods\n",
    "    unit_ids = np.arange(n_units)\n",
    "    time_periods = np.arange(periods)\n",
    "\n",
    "    # Create base data frame\n",
    "    data = pd.DataFrame({\n",
    "        'unit_id': np.repeat(unit_ids, periods),\n",
    "        'time': np.tile(time_periods, n_units)\n",
    "    })\n",
    "\n",
    "    # --- Staggered Treatment Assignment ---\n",
    "    # Divide units into 4 roughly equal groups\n",
    "    shuffled_unit_ids = np.random.permutation(unit_ids)\n",
    "    group_size = n_units // 4\n",
    "    group_assignments = {}\n",
    "    group_assignments[0] = shuffled_unit_ids[0 * group_size : 1 * group_size] # Control\n",
    "    group_assignments[1] = shuffled_unit_ids[1 * group_size : 2 * group_size] # Treat at T0\n",
    "    group_assignments[2] = shuffled_unit_ids[2 * group_size : 3 * group_size] # Treat at T0 + 1\n",
    "    # Assign remaining units (if n_units % 4 != 0) to the last group\n",
    "    group_assignments[3] = shuffled_unit_ids[3 * group_size :] # Treat at T0 + 2\n",
    "\n",
    "    # Map unit_id to treatment group\n",
    "    unit_to_group = {}\n",
    "    for group_id, units_in_group in group_assignments.items():\n",
    "        for unit in units_in_group:\n",
    "            unit_to_group[unit] = group_id\n",
    "    data['treatment_group'] = data['unit_id'].map(unit_to_group)\n",
    "\n",
    "    # Determine the first treatment period for each unit\n",
    "    earliest_treatment_period = num_pre_periods # Period when the *first* group gets treated (Group 1)\n",
    "    conditions = [\n",
    "        data['treatment_group'] == 0,\n",
    "        data['treatment_group'] == 1,\n",
    "        data['treatment_group'] == 2,\n",
    "        data['treatment_group'] == 3\n",
    "    ]\n",
    "    choices = [\n",
    "        np.inf, # Never treated\n",
    "        earliest_treatment_period,\n",
    "        earliest_treatment_period + 1,\n",
    "        earliest_treatment_period + 2\n",
    "    ]\n",
    "    data['first_treat_period'] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "    # Indicator for being *eventually* treated (used for pre-trend bias)\n",
    "    data['eventually_treated'] = (data['treatment_group'] > 0).astype(int)\n",
    "    data['post_treatment'] = (data['time'] >= num_pre_periods).astype(int)\n",
    "\n",
    "    # Dynamic treatment indicator 'D': 1 if treated in the current period, 0 otherwise\n",
    "    data['D'] = (data['time'] >= data['first_treat_period']).astype(int)\n",
    "\n",
    "    # --- Covariates ---\n",
    "    data['time_trend'] = data['time'] # Simple linear time trend\n",
    "\n",
    "    # Generate X covariates\n",
    "    X_numeric = np.random.normal(0, 1, size=(len(data), num_x_covariates))\n",
    "    bernoulli_values = np.random.binomial(n=1, p=0.66, size=len(data))\n",
    "    categories = [1, 2, 3, 4]\n",
    "    probabilities = [0.3, 0.1, 0.2, 0.4]\n",
    "    categorical_values = np.random.choice(categories, size=len(data), p=probabilities)\n",
    "\n",
    "    # Combine all covariates into a single matrix X for easier processing later\n",
    "    X = np.column_stack((bernoulli_values, X_numeric, categorical_values))\n",
    "\n",
    "    # Add covariates to the DataFrame with names X_1, X_2, ...\n",
    "    total_covariates = num_x_covariates + 2\n",
    "    for i in range(total_covariates):\n",
    "        data[f'X_{i+1}'] = X[:, i]\n",
    "\n",
    "    # --- Generate Outcome Variable (Y) ---\n",
    "    data['epsilon'] = np.random.normal(scale=epsilon_scale, size=len(data))\n",
    "\n",
    "    # DGP parameters\n",
    "    beta_0 = -0.5 # Intercept\n",
    "    beta_group_effect = 0.75 # Main effect of treated group (alpha_i)\n",
    "    beta_time = 0.2 # Main effect of time trend (gamma_t)\n",
    "    beta_interaction = treatment_effect_beta # Treatment effect magnitude\n",
    "    # Ensure beta_x has the correct length\n",
    "    beta_x = np.array([-0.75, 0.5, -0.5, -1.30, 1.8, 2.5, -1.0])[:total_covariates] # Adjust length if num_x_covariates changes\n",
    "\n",
    "    # Baseline Y components (common across linearity degrees)\n",
    "    Y_base = (beta_0 +\n",
    "              beta_group_effect * data['eventually_treated'] + # Group fixed effect for those eventually treated\n",
    "              beta_time * data['time_trend']) # Common time trend\n",
    "\n",
    "    # Covariate effects\n",
    "    if linearity_degree == 1:\n",
    "        linear_x_contribution = np.sum([beta_x[i] * data[f'X_{i+1}'] for i in range(total_covariates)], axis=0)\n",
    "        Y_covariates = linear_x_contribution\n",
    "        Y_treatment = beta_interaction * data['D']\n",
    "        data['CATE'] = beta_interaction * data['D']\n",
    "\n",
    "    elif linearity_degree == 2:\n",
    "        half = total_covariates // 2\n",
    "        cov_effect = (np.sum(beta_x[:int(half/2)] * (X[:, :int(half/2)] ** 2),axis=1) +\n",
    "                      np.sum(beta_x[int(half/2):half] * np.exp(X[:, int(half/2):half]),axis=1)+\n",
    "                      np.sum(beta_x[half:] * X[:, half:],axis=1))\n",
    "        Y_covariates = cov_effect\n",
    "        Y_treatment = beta_interaction * data['D']\n",
    "        data['CATE'] = beta_interaction * data['D']\n",
    "\n",
    "    elif linearity_degree == 3:\n",
    "        half = total_covariates // 2\n",
    "        # Non-linear time trend and covariates\n",
    "        Y_base = (beta_0 +\n",
    "                  beta_group_effect * data['eventually_treated'] +\n",
    "                  beta_time * data['time_trend']**2) # Non-linear time trend\n",
    "        cov_effect = (np.sum(beta_x[:int(half/2)] * (X[:, :int(half/2)] ** 2),axis=1) +\n",
    "                      np.sum(beta_x[int(half/2):half] * np.exp(X[:, int(half/2):half]),axis=1)+\n",
    "                      np.sum(beta_x[half:half+int(half/2)] * np.abs(X[:, half:half+int(half/2)]),axis=1) +\n",
    "                      np.sum(beta_x[half+int(half/2):] * np.sqrt(np.abs(X[:, half+int(half/2):])),axis=1))\n",
    "        Y_covariates = cov_effect\n",
    "        # Linear treatment effect (as per original code structure for degree 4)\n",
    "        Y_treatment = beta_interaction * data['D']\n",
    "        data['CATE'] = beta_interaction * data['D']\n",
    "    else: # Handle unexpected linearity_degree\n",
    "         Y_covariates = 0\n",
    "         Y_treatment = 0\n",
    "         data['CATE'] = 0\n",
    "\n",
    "    data['Y'] = Y_base + Y_covariates + Y_treatment\n",
    "\n",
    "    # --- Add pre-trend bias ---\n",
    "    # Apply bias to *eventually treated* units during the pre-period *relative to the first treatment time*\n",
    "    if pre_trend_bias_delta != 0:\n",
    "        # Apply bias only before the *earliest* treatment period\n",
    "        pre_period_mask = data['time'] < earliest_treatment_period\n",
    "        # Apply bias only to units that will eventually be treated\n",
    "        bias_mask = pre_period_mask & (data['eventually_treated'] == 1)\n",
    "\n",
    "        if linearity_degree == 3: # Non-linear pre-trend (e.g., seasonal)\n",
    "            seasonal_amplitude = 1.0\n",
    "            seasonal_period = 4\n",
    "            seasonal_effect = seasonal_amplitude * np.sin(2 * np.pi * data['time'] / seasonal_period)\n",
    "            data.loc[bias_mask, 'Y'] += pre_trend_bias_delta * seasonal_effect[bias_mask]\n",
    "        else: # Linear pre-trend bias\n",
    "             # Difference relative to the earliest treatment time\n",
    "            time_diff = data['time'] - earliest_treatment_period\n",
    "            data.loc[bias_mask, 'Y'] += pre_trend_bias_delta * time_diff[bias_mask]\n",
    "\n",
    "    # Add final error term\n",
    "    data['Y'] += data['epsilon']\n",
    "\n",
    "    # Remove intermediate columns if desired, or keep for clarity\n",
    "    # data = data.drop(columns=['epsilon'])\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_treatment_indexes_array(df, min_time=4, eventually_treated=1):\n",
    "    \"\"\"\n",
    "    Finds the indexes of the first row for each treatment group (0, 1, 2, 3)\n",
    "    after filtering the DataFrame by time and eventually_treated, and returns them as a NumPy array.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame.\n",
    "        min_time: The minimum time value.\n",
    "        eventually_treated: The desired eventually_treated value.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array containing the first row indexes for each treatment group (0, 1, 2, 3),\n",
    "        or None if no rows meet the criteria. Returns -1 if a treatment group does not appear in the filtered data.\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_df = df[(df['time'] >= min_time) & (df['eventually_treated'] == eventually_treated)]\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        return None  # Return None if no rows match the time and eventually_treated criteria.\n",
    "\n",
    "    indexes = []\n",
    "    for group in [1, 2, 3]:\n",
    "        group_df = filtered_df[filtered_df['treatment_group'] == group]\n",
    "        if not group_df.empty:\n",
    "            indexes.append(group_df.index[0])  # Get the first index\n",
    "        else:\n",
    "            indexes.append(-1) #Return -1 if the treatment group does not appear in the filtered data.\n",
    "\n",
    "    return np.array(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_metrics_grouped_hybrid( # Renamed slightly for clarity\n",
    "    true_ATE,\n",
    "    estimated_ATE,\n",
    "    accumulated_p_values,\n",
    "    suffix=\"\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculates both per-iteration and summary metrics (RMSE, MAE, MAPE).\n",
    "\n",
    "    1. Saves per-iteration results to an Excel file named\n",
    "       \"BCF_GATE_and_PValues{suffix}.xlsx\". The file has multiple sheets,\n",
    "       with each row representing a simulation iteration:\n",
    "       - 'Overall_Metrics': Contains overall RMSE, MAE, MAPE per iteration.\n",
    "       - 'Group_X': One sheet per group, containing the group's RMSE, MAE, MAPE\n",
    "                    per iteration, alongside the raw p-values for that group.\n",
    "\n",
    "    2. Returns dictionaries containing summary statistics (mean and standard\n",
    "       deviation of metrics aggregated across all iterations).\n",
    "\n",
    "    Args:\n",
    "        true_ATE: A numpy array of shape (num_iterations, num_post_periods, number_of_groups)\n",
    "                  containing the true ATE values.\n",
    "        estimated_ATE: A numpy array of shape (num_iterations, num_post_periods, number_of_groups)\n",
    "                       containing the estimated ATE values.\n",
    "        accumulated_p_values: A numpy array of shape (num_iterations, num_post_periods, number_of_groups)\n",
    "                              containing the p-values.\n",
    "        suffix (str): An optional suffix to append to the base filename\n",
    "                      \"BCF_GATE_and_PValues\". Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - overall_metrics: Dictionary with overall summary statistics\n",
    "              (mean RMSE, mean MAE, mean MAPE, std RMSE, std MAE, std MAPE).\n",
    "            - per_group_metrics: Dictionary where keys are group indices and\n",
    "              values are dictionaries with summary statistics for that group.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    if not (true_ATE.shape == estimated_ATE.shape == accumulated_p_values.shape):\n",
    "        raise ValueError(\"Shapes of true_ATE, estimated_ATE, and accumulated_p_values must match.\")\n",
    "    if true_ATE.ndim != 3:\n",
    "         raise ValueError(\"Input arrays must have 3 dimensions: (iterations, time, groups).\")\n",
    "\n",
    "    num_iterations, num_post_periods, number_of_groups = true_ATE.shape\n",
    "    iteration_index = pd.RangeIndex(num_iterations, name='Iteration')\n",
    "\n",
    "    # Construct filename using suffix\n",
    "    filename = f\"OLS_GATE_and_PValues{suffix}.xlsx\"\n",
    "\n",
    "    # Calculate element-wise errors\n",
    "    errors = estimated_ATE - true_ATE # Shape: (iterations, time, groups)\n",
    "    abs_errors = np.abs(errors)       # Shape: (iterations, time, groups)\n",
    "\n",
    "    # --- Calculate PER-ITERATION Metrics (used for both Excel and summaries) ---\n",
    "\n",
    "    # Overall per-iteration metrics\n",
    "    overall_rmse_per_iteration = np.sqrt(np.mean(errors**2, axis=(1, 2)))\n",
    "    overall_mae_per_iteration = np.mean(abs_errors, axis=(1, 2))\n",
    "    overall_mape_per_iteration = np.zeros(num_iterations) * np.nan\n",
    "    for i in range(num_iterations):\n",
    "         true_ate_i = true_ATE[i, :, :]\n",
    "         errors_i = errors[i, :, :]\n",
    "         valid_mask_i = true_ate_i != 0\n",
    "         if np.any(valid_mask_i):\n",
    "             abs_perc_errors_i = np.abs(errors_i[valid_mask_i] / true_ate_i[valid_mask_i])\n",
    "             overall_mape_per_iteration[i] = np.mean(abs_perc_errors_i) * 100\n",
    "\n",
    "    # Create Overall DataFrame for Excel\n",
    "    df_overall = pd.DataFrame({\n",
    "        'Overall_RMSE': overall_rmse_per_iteration,\n",
    "        'Overall_MAE': overall_mae_per_iteration,\n",
    "        'Overall_MAPE': overall_mape_per_iteration\n",
    "    }, index=iteration_index)\n",
    "\n",
    "    # --- Calculate SUMMARY Overall Metrics (for return value) ---\n",
    "    overall_rmse = np.sqrt(np.mean(errors**2))\n",
    "    overall_mae = np.mean(abs_errors)\n",
    "    valid_mape_mask = true_ATE != 0\n",
    "    abs_perc_errors = np.full_like(errors, fill_value=np.nan)\n",
    "    abs_perc_errors[valid_mape_mask] = np.abs(errors[valid_mape_mask] / true_ATE[valid_mape_mask])\n",
    "    overall_mape = np.nanmean(abs_perc_errors) * 100\n",
    "\n",
    "    summary_overall_std_rmse = np.nanstd(overall_rmse_per_iteration)\n",
    "    summary_overall_std_mae = np.nanstd(overall_mae_per_iteration)\n",
    "    summary_overall_std_mape = np.nanstd(overall_mape_per_iteration)\n",
    "\n",
    "    overall_metrics = { # Dictionary for return value\n",
    "        \"Overall_RMSE\": overall_rmse,\n",
    "        \"Overall_MAE\": overall_mae,\n",
    "        \"Overall_MAPE\": overall_mape,\n",
    "        \"Overall_Std_RMSE\": summary_overall_std_rmse,\n",
    "        \"Overall_Std_MAE\": summary_overall_std_mae,\n",
    "        \"Overall_Std_MAPE\": summary_overall_std_mape,\n",
    "    }\n",
    "\n",
    "    # --- Process Per-Group Data (for both Excel and summaries) ---\n",
    "    group_combined_dfs = {} # For Excel sheets\n",
    "    per_group_metrics = {}  # For return value summaries\n",
    "\n",
    "    for g in range(number_of_groups):\n",
    "        # Slice data for the current group\n",
    "        true_ATE_g = true_ATE[:, :, g]  # Shape: (iterations, time)\n",
    "        errors_g = errors[:, :, g]      # Shape: (iterations, time)\n",
    "        abs_errors_g = abs_errors[:, :, g]# Shape: (iterations, time)\n",
    "        p_values_g = accumulated_p_values[:, :, g] # Shape: (iterations, time)\n",
    "\n",
    "        # Calculate per-iteration metrics for group g\n",
    "        group_rmse_per_iter = np.sqrt(np.mean(errors_g**2, axis=1))\n",
    "        group_mae_per_iter = np.mean(abs_errors_g, axis=1)\n",
    "        group_mape_per_iter = np.zeros(num_iterations) * np.nan\n",
    "        for i in range(num_iterations):\n",
    "            true_ate_gi = true_ATE_g[i, :]\n",
    "            errors_gi = errors_g[i, :]\n",
    "            valid_mask_gi = true_ate_gi != 0\n",
    "            if np.any(valid_mask_gi):\n",
    "                abs_perc_errors_gi = np.abs(errors_gi[valid_mask_gi] / true_ate_gi[valid_mask_gi])\n",
    "                group_mape_per_iter[i] = np.mean(abs_perc_errors_gi) * 100\n",
    "\n",
    "        # --- Calculate SUMMARY Stats for Group g (for return dict) ---\n",
    "        group_rmse = np.sqrt(np.mean(errors_g**2))\n",
    "        group_mae = np.mean(abs_errors_g)\n",
    "        valid_mape_mask_g = true_ATE_g != 0\n",
    "        abs_perc_errors_g = np.full_like(errors_g, fill_value=np.nan)\n",
    "        abs_perc_errors_g[valid_mape_mask_g] = np.abs(errors_g[valid_mape_mask_g] / true_ATE_g[valid_mape_mask_g])\n",
    "        group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
    "\n",
    "        summary_group_std_rmse = np.nanstd(group_rmse_per_iter)\n",
    "        summary_group_std_mae = np.nanstd(group_mae_per_iter)\n",
    "        summary_group_std_mape = np.nanstd(group_mape_per_iter)\n",
    "\n",
    "        per_group_metrics[g] = { # Populate return dictionary for group g\n",
    "             f\"Group_{g}_RMSE\": group_rmse,\n",
    "             f\"Group_{g}_MAE\": group_mae,\n",
    "             f\"Group_{g}_MAPE\": group_mape,\n",
    "             f\"Group_{g}_Std_RMSE\": summary_group_std_rmse,\n",
    "             f\"Group_{g}_Std_MAE\": summary_group_std_mae,\n",
    "             f\"Group_{g}_Std_MAPE\": summary_group_std_mape,\n",
    "        }\n",
    "\n",
    "        # --- Create DataFrames for Excel Sheet for Group g ---\n",
    "        df_metrics_g = pd.DataFrame({\n",
    "            f'Group_{g}_RMSE': group_rmse_per_iter, # Use the per-iter arrays\n",
    "            f'Group_{g}_MAE': group_mae_per_iter,\n",
    "            f'Group_{g}_MAPE': group_mape_per_iter\n",
    "        }, index=iteration_index)\n",
    "\n",
    "        p_value_columns = [f'PValue_Time_{t}' for t in range(num_post_periods)]\n",
    "        df_pvals_g = pd.DataFrame(p_values_g,\n",
    "                                  index=iteration_index,\n",
    "                                  columns=p_value_columns)\n",
    "\n",
    "        # Combine metrics and p-values for the group's Excel sheet\n",
    "        group_combined_dfs[g] = pd.concat([df_metrics_g, df_pvals_g], axis=1)\n",
    "\n",
    "\n",
    "    # --- Write Per-Iteration Data to Excel ---\n",
    "    try:\n",
    "        with pd.ExcelWriter(filename) as writer:\n",
    "            # Write Overall Metrics Sheet\n",
    "            df_overall.to_excel(writer, sheet_name='Overall_Metrics', header=True, index=True)\n",
    "            # Write Per-Group Sheets\n",
    "            for g in range(number_of_groups):\n",
    "                sheet_name_g = f'Group_{g}'\n",
    "                group_combined_dfs[g].to_excel(writer, sheet_name=sheet_name_g, header=True, index=True)\n",
    "\n",
    "        print(f\"Per-iteration metrics and p-values successfully saved to '{filename}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving per-iteration metrics to Excel file '{filename}': {e}\")\n",
    "\n",
    "    # --- Return Summary Dictionaries ---\n",
    "    return overall_metrics, per_group_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 100/100 [00:08<00:00, 12.11iteration/s]\n",
      "/tmp/ipykernel_15953/394917134.py:81: RuntimeWarning: Mean of empty slice\n",
      "  overall_mape = np.nanmean(abs_perc_errors) * 100\n",
      "/home/hugo.souto/.local/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_15953/394917134.py:125: RuntimeWarning: Mean of empty slice\n",
      "  group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
      "/tmp/ipykernel_15953/394917134.py:125: RuntimeWarning: Mean of empty slice\n",
      "  group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
      "/home/hugo.souto/.local/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_15953/394917134.py:125: RuntimeWarning: Mean of empty slice\n",
      "  group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
      "/home/hugo.souto/.local/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-iteration metrics and p-values successfully saved to 'OLS_GATE_and_PValues_linearity=1.xlsx'\n",
      "\n",
      "--- Overall Metrics (Dictionary) ---\n",
      "{'Overall_RMSE': np.float64(0.220670341731245), 'Overall_MAE': np.float64(0.17710817753886587), 'Overall_MAPE': np.float64(nan), 'Overall_Std_RMSE': np.float64(0.08421684782444672), 'Overall_Std_MAE': np.float64(0.0818431966517216), 'Overall_Std_MAPE': np.float64(nan)}\n",
      "\n",
      "--- Per Group Metrics (Dictionary) ---\n",
      "Group 0:\n",
      "{'Group_0_RMSE': np.float64(0.220670341731245), 'Group_0_MAE': np.float64(0.17710817753886587), 'Group_0_MAPE': np.float64(nan), 'Group_0_Std_RMSE': np.float64(0.08421684782444672), 'Group_0_Std_MAE': np.float64(0.08184319665172159), 'Group_0_Std_MAPE': np.float64(nan)}\n",
      "Group 1:\n",
      "{'Group_1_RMSE': np.float64(0.220670341731245), 'Group_1_MAE': np.float64(0.17710817753886587), 'Group_1_MAPE': np.float64(nan), 'Group_1_Std_RMSE': np.float64(0.08421684782444672), 'Group_1_Std_MAE': np.float64(0.08184319665172159), 'Group_1_Std_MAPE': np.float64(nan)}\n",
      "Group 2:\n",
      "{'Group_2_RMSE': np.float64(0.220670341731245), 'Group_2_MAE': np.float64(0.17710817753886587), 'Group_2_MAPE': np.float64(nan), 'Group_2_Std_RMSE': np.float64(0.08421684782444672), 'Group_2_Std_MAE': np.float64(0.08184319665172159), 'Group_2_Std_MAPE': np.float64(nan)}\n"
     ]
    }
   ],
   "source": [
    "linearity_degree=1\n",
    "num_x_covariates = 5\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "num_pre_periods=4\n",
    "\n",
    "num_post_periods=4\n",
    "\n",
    "number_of_groups=3\n",
    "true_ATE=np.zeros([num_iterations,num_post_periods,number_of_groups])\n",
    "estimated_ATE_subset=np.zeros([num_iterations,num_post_periods,number_of_groups])\n",
    "accumulated_p_values=np.zeros([num_iterations,num_post_periods,number_of_groups])\n",
    "\n",
    "epsilon_scale=1\n",
    "\n",
    "for i in tqdm(range(num_iterations), desc=\"Progress\", unit=\"iteration\"):\n",
    "    # Generate a random seed for each iteration.\n",
    "    seed_val = i\n",
    "\n",
    "    # Generate data with specified hyperparameters.\n",
    "    data_linear = generate_staggered_did_data(\n",
    "        n_units=200,\n",
    "        linearity_degree=linearity_degree,\n",
    "        num_pre_periods=num_pre_periods,\n",
    "        num_post_periods=num_post_periods,\n",
    "        pre_trend_bias_delta=0,\n",
    "        num_x_covariates=num_x_covariates,\n",
    "        epsilon_scale=epsilon_scale,\n",
    "        seed=seed_val\n",
    "    )\n",
    "    indexes = find_first_treatment_indexes_array(data_linear)\n",
    "\n",
    "    # Define the complex regression formula.\n",
    "    formula_complex = \"Y ~ eventually_treated + C(time) + C(time):eventually_treated + X_1 + X_2+X_3+X_4+ X_5 + X_6+X_7\"\n",
    "    \n",
    "    # Fit the model.\n",
    "    model_complex = smf.ols(formula=formula_complex, data=data_linear).fit()\n",
    "\n",
    "    for j in range(len(indexes)):\n",
    "      true_ATE[i,:,j]=np.array(data_linear[(data_linear['time'] >= num_pre_periods) & data_linear['eventually_treated'] == 1][\"CATE\"].loc[indexes[j]:indexes[j]+num_post_periods])\n",
    "      estimated_ATE_subset[i,:,j]=np.array(model_complex.params[[f'C(time)[T.{t}]:eventually_treated' for t in range(num_pre_periods, num_pre_periods + num_post_periods)]])\n",
    "      accumulated_p_values[i,:,j]=np.array(model_complex.pvalues[[f'C(time)[T.{t}]:eventually_treated' for t in range(num_pre_periods, num_pre_periods + num_post_periods)]])\n",
    "\n",
    "\n",
    "simulation_suffix = \"_linearity=1\" # Example suffix\n",
    "overall_metrics, per_group_metrics = calculate_error_metrics_grouped_hybrid(\n",
    "    true_ATE,\n",
    "    estimated_ATE_subset,\n",
    "    accumulated_p_values,\n",
    "    suffix=simulation_suffix # Pass the suffix here\n",
    ")\n",
    "\n",
    "print(\"\\n--- Overall Metrics (Dictionary) ---\")\n",
    "print(overall_metrics)\n",
    "\n",
    "print(\"\\n--- Per Group Metrics (Dictionary) ---\")\n",
    "for group_idx, metrics in per_group_metrics.items():\n",
    "    print(f\"Group {group_idx}:\")\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 100/100 [00:07<00:00, 12.93iteration/s]\n",
      "/tmp/ipykernel_15953/394917134.py:81: RuntimeWarning: Mean of empty slice\n",
      "  overall_mape = np.nanmean(abs_perc_errors) * 100\n",
      "/home/hugo.souto/.local/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_15953/394917134.py:125: RuntimeWarning: Mean of empty slice\n",
      "  group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
      "/tmp/ipykernel_15953/394917134.py:125: RuntimeWarning: Mean of empty slice\n",
      "  group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
      "/home/hugo.souto/.local/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_15953/394917134.py:125: RuntimeWarning: Mean of empty slice\n",
      "  group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
      "/home/hugo.souto/.local/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-iteration metrics and p-values successfully saved to 'OLS_GATE_and_PValues_linearity=2.xlsx'\n",
      "\n",
      "--- Overall Metrics (Dictionary) ---\n",
      "{'Overall_RMSE': np.float64(0.31158569965636795), 'Overall_MAE': np.float64(0.24527226958490586), 'Overall_MAPE': np.float64(nan), 'Overall_Std_RMSE': np.float64(0.12193785080026887), 'Overall_Std_MAE': np.float64(0.11218357558120047), 'Overall_Std_MAPE': np.float64(nan)}\n",
      "\n",
      "--- Per Group Metrics (Dictionary) ---\n",
      "Group 0:\n",
      "{'Group_0_RMSE': np.float64(0.31158569965636795), 'Group_0_MAE': np.float64(0.24527226958490586), 'Group_0_MAPE': np.float64(nan), 'Group_0_Std_RMSE': np.float64(0.12193785080026888), 'Group_0_Std_MAE': np.float64(0.11218357558120047), 'Group_0_Std_MAPE': np.float64(nan)}\n",
      "Group 1:\n",
      "{'Group_1_RMSE': np.float64(0.31158569965636795), 'Group_1_MAE': np.float64(0.24527226958490586), 'Group_1_MAPE': np.float64(nan), 'Group_1_Std_RMSE': np.float64(0.12193785080026888), 'Group_1_Std_MAE': np.float64(0.11218357558120047), 'Group_1_Std_MAPE': np.float64(nan)}\n",
      "Group 2:\n",
      "{'Group_2_RMSE': np.float64(0.31158569965636795), 'Group_2_MAE': np.float64(0.24527226958490586), 'Group_2_MAPE': np.float64(nan), 'Group_2_Std_RMSE': np.float64(0.12193785080026888), 'Group_2_Std_MAE': np.float64(0.11218357558120047), 'Group_2_Std_MAPE': np.float64(nan)}\n"
     ]
    }
   ],
   "source": [
    "linearity_degree=2\n",
    "num_x_covariates = 5\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "num_pre_periods=4\n",
    "\n",
    "num_post_periods=4\n",
    "\n",
    "number_of_groups=3\n",
    "true_ATE=np.zeros([num_iterations,num_post_periods,number_of_groups])\n",
    "estimated_ATE_subset=np.zeros([num_iterations,num_post_periods,number_of_groups])\n",
    "accumulated_p_values=np.zeros([num_iterations,num_post_periods,number_of_groups])\n",
    "\n",
    "epsilon_scale=1\n",
    "\n",
    "for i in tqdm(range(num_iterations), desc=\"Progress\", unit=\"iteration\"):\n",
    "    # Generate a random seed for each iteration.\n",
    "    seed_val = i\n",
    "\n",
    "    # Generate data with specified hyperparameters.\n",
    "    data_linear = generate_staggered_did_data(\n",
    "        n_units=200,\n",
    "        linearity_degree=linearity_degree,\n",
    "        num_pre_periods=num_pre_periods,\n",
    "        num_post_periods=num_post_periods,\n",
    "        pre_trend_bias_delta=0,\n",
    "        num_x_covariates=num_x_covariates,\n",
    "        epsilon_scale=epsilon_scale,\n",
    "        seed=seed_val\n",
    "    )\n",
    "    indexes = find_first_treatment_indexes_array(data_linear)\n",
    "\n",
    "    # Define the complex regression formula.\n",
    "    formula_complex = \"Y ~ eventually_treated + C(time) + C(time):eventually_treated + X_1 + X_2+X_3+X_4+ X_5 + X_6+X_7\"\n",
    "    \n",
    "    # Fit the model.\n",
    "    model_complex = smf.ols(formula=formula_complex, data=data_linear).fit()\n",
    "\n",
    "    for j in range(len(indexes)):\n",
    "      true_ATE[i,:,j]=np.array(data_linear[(data_linear['time'] >= num_pre_periods) & data_linear['eventually_treated'] == 1][\"CATE\"].loc[indexes[j]:indexes[j]+num_post_periods])\n",
    "      estimated_ATE_subset[i,:,j]=np.array(model_complex.params[[f'C(time)[T.{t}]:eventually_treated' for t in range(num_pre_periods, num_pre_periods + num_post_periods)]])\n",
    "      accumulated_p_values[i,:,j]=np.array(model_complex.pvalues[[f'C(time)[T.{t}]:eventually_treated' for t in range(num_pre_periods, num_pre_periods + num_post_periods)]])\n",
    "\n",
    "\n",
    "simulation_suffix = \"_linearity=2\" # Example suffix\n",
    "overall_metrics, per_group_metrics = calculate_error_metrics_grouped_hybrid(\n",
    "    true_ATE,\n",
    "    estimated_ATE_subset,\n",
    "    accumulated_p_values,\n",
    "    suffix=simulation_suffix # Pass the suffix here\n",
    ")\n",
    "\n",
    "print(\"\\n--- Overall Metrics (Dictionary) ---\")\n",
    "print(overall_metrics)\n",
    "\n",
    "print(\"\\n--- Per Group Metrics (Dictionary) ---\")\n",
    "for group_idx, metrics in per_group_metrics.items():\n",
    "    print(f\"Group {group_idx}:\")\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 100/100 [00:07<00:00, 12.65iteration/s]\n",
      "/tmp/ipykernel_15953/394917134.py:81: RuntimeWarning: Mean of empty slice\n",
      "  overall_mape = np.nanmean(abs_perc_errors) * 100\n",
      "/home/hugo.souto/.local/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_15953/394917134.py:125: RuntimeWarning: Mean of empty slice\n",
      "  group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
      "/tmp/ipykernel_15953/394917134.py:125: RuntimeWarning: Mean of empty slice\n",
      "  group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
      "/home/hugo.souto/.local/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_15953/394917134.py:125: RuntimeWarning: Mean of empty slice\n",
      "  group_mape = np.nanmean(abs_perc_errors_g) * 100\n",
      "/home/hugo.souto/.local/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:2035: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-iteration metrics and p-values successfully saved to 'OLS_GATE_and_PValues_linearity=3.xlsx'\n",
      "\n",
      "--- Overall Metrics (Dictionary) ---\n",
      "{'Overall_RMSE': np.float64(0.4122548413548967), 'Overall_MAE': np.float64(0.32337641509665166), 'Overall_MAPE': np.float64(nan), 'Overall_Std_RMSE': np.float64(0.15786963983787594), 'Overall_Std_MAE': np.float64(0.1452984562657175), 'Overall_Std_MAPE': np.float64(nan)}\n",
      "\n",
      "--- Per Group Metrics (Dictionary) ---\n",
      "Group 0:\n",
      "{'Group_0_RMSE': np.float64(0.4122548413548967), 'Group_0_MAE': np.float64(0.3233764150966516), 'Group_0_MAPE': np.float64(nan), 'Group_0_Std_RMSE': np.float64(0.15786963983787594), 'Group_0_Std_MAE': np.float64(0.1452984562657175), 'Group_0_Std_MAPE': np.float64(nan)}\n",
      "Group 1:\n",
      "{'Group_1_RMSE': np.float64(0.4122548413548967), 'Group_1_MAE': np.float64(0.3233764150966516), 'Group_1_MAPE': np.float64(nan), 'Group_1_Std_RMSE': np.float64(0.15786963983787594), 'Group_1_Std_MAE': np.float64(0.1452984562657175), 'Group_1_Std_MAPE': np.float64(nan)}\n",
      "Group 2:\n",
      "{'Group_2_RMSE': np.float64(0.4122548413548967), 'Group_2_MAE': np.float64(0.3233764150966516), 'Group_2_MAPE': np.float64(nan), 'Group_2_Std_RMSE': np.float64(0.15786963983787594), 'Group_2_Std_MAE': np.float64(0.1452984562657175), 'Group_2_Std_MAPE': np.float64(nan)}\n"
     ]
    }
   ],
   "source": [
    "linearity_degree=3\n",
    "num_x_covariates = 5\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "num_pre_periods=4\n",
    "\n",
    "num_post_periods=4\n",
    "\n",
    "number_of_groups=3\n",
    "true_ATE=np.zeros([num_iterations,num_post_periods,number_of_groups])\n",
    "estimated_ATE_subset=np.zeros([num_iterations,num_post_periods,number_of_groups])\n",
    "accumulated_p_values=np.zeros([num_iterations,num_post_periods,number_of_groups])\n",
    "\n",
    "epsilon_scale=1\n",
    "\n",
    "for i in tqdm(range(num_iterations), desc=\"Progress\", unit=\"iteration\"):\n",
    "    # Generate a random seed for each iteration.\n",
    "    seed_val = i\n",
    "\n",
    "    # Generate data with specified hyperparameters.\n",
    "    data_linear = generate_staggered_did_data(\n",
    "        n_units=200,\n",
    "        linearity_degree=linearity_degree,\n",
    "        num_pre_periods=num_pre_periods,\n",
    "        num_post_periods=num_post_periods,\n",
    "        pre_trend_bias_delta=0,\n",
    "        num_x_covariates=num_x_covariates,\n",
    "        epsilon_scale=epsilon_scale,\n",
    "        seed=seed_val\n",
    "    )\n",
    "    indexes = find_first_treatment_indexes_array(data_linear)\n",
    "\n",
    "    # Define the complex regression formula.\n",
    "    formula_complex = \"Y ~ eventually_treated + C(time) + C(time):eventually_treated + X_1 + X_2+X_3+X_4+ X_5 + X_6+X_7\"\n",
    "    \n",
    "    # Fit the model.\n",
    "    model_complex = smf.ols(formula=formula_complex, data=data_linear).fit()\n",
    "\n",
    "    for j in range(len(indexes)):\n",
    "      true_ATE[i,:,j]=np.array(data_linear[(data_linear['time'] >= num_pre_periods) & data_linear['eventually_treated'] == 1][\"CATE\"].loc[indexes[j]:indexes[j]+num_post_periods])\n",
    "      estimated_ATE_subset[i,:,j]=np.array(model_complex.params[[f'C(time)[T.{t}]:eventually_treated' for t in range(num_pre_periods, num_pre_periods + num_post_periods)]])\n",
    "      accumulated_p_values[i,:,j]=np.array(model_complex.pvalues[[f'C(time)[T.{t}]:eventually_treated' for t in range(num_pre_periods, num_pre_periods + num_post_periods)]])\n",
    "\n",
    "\n",
    "simulation_suffix = \"_linearity=3\" # Example suffix\n",
    "overall_metrics, per_group_metrics = calculate_error_metrics_grouped_hybrid(\n",
    "    true_ATE,\n",
    "    estimated_ATE_subset,\n",
    "    accumulated_p_values,\n",
    "    suffix=simulation_suffix # Pass the suffix here\n",
    ")\n",
    "\n",
    "print(\"\\n--- Overall Metrics (Dictionary) ---\")\n",
    "print(overall_metrics)\n",
    "\n",
    "print(\"\\n--- Per Group Metrics (Dictionary) ---\")\n",
    "for group_idx, metrics in per_group_metrics.items():\n",
    "    print(f\"Group {group_idx}:\")\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
