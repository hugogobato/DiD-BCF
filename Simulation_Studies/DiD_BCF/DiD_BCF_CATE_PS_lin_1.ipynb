{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p76dXXNQWYB",
        "outputId": "787b18f8-a74f-4c1d-c827-9e0ac587deb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/StochasticTree/stochtree.git\n",
            "  Cloning https://github.com/StochasticTree/stochtree.git to /tmp/pip-req-build-um7km36v\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/StochasticTree/stochtree.git /tmp/pip-req-build-um7km36v\n",
            "  Resolved https://github.com/StochasticTree/stochtree.git to commit f55bbb47b57ef6160964084650ab81f557c9559c\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stochtree\n",
            "  Building wheel for stochtree (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stochtree: filename=stochtree-0.1.0-cp311-cp311-linux_x86_64.whl size=871294 sha256=f353cca6d8e55387749f6cc1688d0c4f9b7ada33e12ad902460cb2ed94197ca8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gam3q_55/wheels/6b/16/bb/b09e1d07fb9c44bfd19200620859a0fdda75287afaa4a076bf\n",
            "Successfully built stochtree\n",
            "Installing collected packages: stochtree\n",
            "Successfully installed stochtree-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/StochasticTree/stochtree.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VgjqGpE_QWYH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hg0viKgUQWYI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_did_data(\n",
        "    n_units=200,\n",
        "    num_pre_periods=5,\n",
        "    num_post_periods=5,\n",
        "    linearity_degree=1, # 1: fully linear, 2: half X non-linear, 3: treatment + all X non-linear\n",
        "    propensity_coeffs={'intercept': 0.0, 'X1': 0.5, 'X7': -0.5}, # Coefficients for propensity score (using static X1 and X7)\n",
        "    pre_trend_bias_delta=0.2,\n",
        "    epsilon_scale=1,\n",
        "    seed=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates panel data for Difference-in-Differences analysis with controllable pre-trends,\n",
        "    non-linearity, conditional treatment effects, and propensity score based treatment assignment.\n",
        "\n",
        "    Generates exactly 8 covariates:\n",
        "    - X1: Bernoulli(p=0.66) - Static (unit-level)\n",
        "    - X2: Bernoulli(p=0.45) - Time-varying\n",
        "    - X3: Normal(0,1) - Time-varying (Used in CATE)\n",
        "    - X4: Normal(0,1) - Time-varying\n",
        "    - X5: Normal(0,1) - Time-varying\n",
        "    - X6: Normal(0,1) - Time-varying\n",
        "    - X7: Normal(0,1) - Static (unit-level, Used in Propensity Score)\n",
        "    - X8: Categorical{1,2,3,4} probs {0.3, 0.1, 0.2, 0.4} - Time-varying (Used in CATE)\n",
        "\n",
        "    Args:\n",
        "        n_units (int): Number of units (e.g., individuals, firms).\n",
        "        num_pre_periods (int): Number of periods before treatment.\n",
        "        num_post_periods (int): Number of periods after treatment.\n",
        "        linearity_degree (int): Degree of linearity in the DGP:\n",
        "            1: Fully linear Y, conditional treatment effect based on X3/X8.\n",
        "            2: X1,X2,X3,X4 non-linear, conditional treatment effect based on X3/X8.\n",
        "            3: All X covariates non-linear, conditional treatment effect based on X3/X8.\n",
        "        propensity_coeffs (dict): Dictionary with coefficients for the propensity score calculation.\n",
        "                                  Expected keys: 'intercept', 'X1', 'X7'.\n",
        "                                  p = sigmoid(intercept + X1*coeff_X1 + X7*coeff_X7)\n",
        "        pre_trend_bias_delta (float): Bias parameter to induce pre-trends in the treated group.\n",
        "        epsilon_scale (float): Scale (standard deviation) of the error term.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Generated panel data in long format with 'Y', covariates (X1-X8), 'treated_group',\n",
        "                      'post_treatment', 'CATE', and 'propensity_score'.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    total_periods = num_pre_periods + num_post_periods\n",
        "    total_observations = n_units * total_periods\n",
        "    unit_ids = np.arange(n_units)\n",
        "    time_periods = np.arange(total_periods)\n",
        "\n",
        "    # --- Fixed Covariate Betas ---\n",
        "    beta_x = np.array([-0.75, 0.5, -0.5, -1.30, 1.8, 2.5, -1.0, 0.3])\n",
        "    if len(beta_x) != 8:\n",
        "        raise ValueError(\"beta_x must have exactly 8 elements.\")\n",
        "\n",
        "    # --- Set base treatment_effect_beta based on linearity_degree ---\n",
        "    if linearity_degree == 1 or linearity_degree == 2:\n",
        "        treatment_effect_beta = 3.0\n",
        "    elif linearity_degree == 3:\n",
        "        treatment_effect_beta = 5.0\n",
        "    else:\n",
        "        raise ValueError(f\"linearity_degree ({linearity_degree}) must be 1, 2, or 3.\")\n",
        "\n",
        "    # --- Generate Static Covariates (Unit-Level) ---\n",
        "    X1_unit = np.random.binomial(n=1, p=0.66, size=n_units) # Static Bernoulli for Propensity\n",
        "    X7_unit = np.random.normal(0, 1, size=n_units)          # Static Numerical for Propensity\n",
        "\n",
        "    # --- Propensity Score Calculation and Treatment Assignment ---\n",
        "    z_unit = (propensity_coeffs['intercept'] +\n",
        "              propensity_coeffs['X1'] * X1_unit +\n",
        "              propensity_coeffs['X7'] * X7_unit)\n",
        "    propensity_scores_unit = 1 / (1 + np.exp(-z_unit))\n",
        "    treatment_assignment_random = np.random.uniform(0, 1, size=n_units)\n",
        "    treated_units_mask = treatment_assignment_random < propensity_scores_unit\n",
        "    treated_units = unit_ids[treated_units_mask]\n",
        "\n",
        "    # --- Create Base DataFrame and Map Static Data ---\n",
        "    data = pd.DataFrame({\n",
        "        'unit_id': np.repeat(unit_ids, total_periods),\n",
        "        'time': np.tile(time_periods, n_units)\n",
        "    })\n",
        "    unit_map = data['unit_id'].values # Index mapper from panel row to unit\n",
        "\n",
        "    data['X1'] = X1_unit[unit_map]\n",
        "    data['X7'] = X7_unit[unit_map]\n",
        "    data['propensity_score'] = propensity_scores_unit[unit_map]\n",
        "    data['treated_group'] = np.isin(data['unit_id'], treated_units).astype(int)\n",
        "\n",
        "    # --- Generate Time-Varying Covariates (Panel-Level) ---\n",
        "    data['X2'] = np.random.binomial(n=1, p=0.45, size=total_observations) # Time-varying Bernoulli\n",
        "    data['X3'] = np.random.normal(0, 1, size=total_observations)         # Time-varying Numerical (for CATE)\n",
        "    data['X4'] = np.random.normal(0, 1, size=total_observations)         # Time-varying Numerical\n",
        "    data['X5'] = np.random.normal(0, 1, size=total_observations)         # Time-varying Numerical\n",
        "    data['X6'] = np.random.normal(0, 1, size=total_observations)         # Time-varying Numerical\n",
        "\n",
        "    # Time-varying Categorical (for CATE)\n",
        "    cat_categories = [1, 2, 3, 4]\n",
        "    cat_probabilities = [0.3, 0.1, 0.2, 0.4]\n",
        "    data['X8'] = np.random.choice(cat_categories, size=total_observations, p=cat_probabilities)\n",
        "\n",
        "    # --- Time indicators ---\n",
        "    treatment_period = num_pre_periods\n",
        "    data['post_treatment'] = np.where(data['time'] >= treatment_period, 1, 0)\n",
        "    data['time_trend'] = data['time']\n",
        "\n",
        "    # --- Generate error term ---\n",
        "    data['epsilon'] = np.random.normal(scale=epsilon_scale, size=total_observations)\n",
        "\n",
        "    # --- DGP parameters ---\n",
        "    beta_0 = -0.5 # Intercept\n",
        "    beta_treated = 0.75 # Main effect of treated group (alpha_i)\n",
        "    beta_time = 0.2 # Main effect of time trend (gamma_t)\n",
        "\n",
        "    # --- Calculate Conditional Treatment Effect (CATE) ---\n",
        "    # Depends on X3 (first numerical, time-varying) and X8 (categorical, time-varying)\n",
        "    sqrt_abs_X3 = np.sqrt(np.abs(data['X3']))\n",
        "    cate_conditions = [\n",
        "        (data['X8'] == 1) | (data['X8'] == 3),\n",
        "        (data['X8'] == 2),\n",
        "        (data['X8'] == 4)\n",
        "    ]\n",
        "    cate_choices = [\n",
        "        treatment_effect_beta + 1.5 * sqrt_abs_X3,\n",
        "        treatment_effect_beta,\n",
        "        treatment_effect_beta - 0.5 * sqrt_abs_X3\n",
        "    ]\n",
        "    potential_cate = np.select(cate_conditions, cate_choices, default=treatment_effect_beta)\n",
        "\n",
        "\n",
        "\n",
        "    actual_cate_contribution = potential_cate * data['treated_group'] * data['post_treatment']\n",
        "    data['CATE'] = potential_cate # Store potential effect magnitude\n",
        "\n",
        "    # --- Calculate Outcome Y based on Linearity Degree ---\n",
        "    covariate_names = [f'X{i+1}' for i in range(8)]\n",
        "\n",
        "    # Define non-linear functions for flexibility\n",
        "    def nl_func1(x): return x**2\n",
        "    def nl_func2(x): return np.exp(x / 2) # Scaled exp\n",
        "    def nl_func3(x): return np.abs(x)\n",
        "    def nl_func4(x): return np.sqrt(np.abs(x))\n",
        "\n",
        "    cov_effect = 0\n",
        "\n",
        "    if linearity_degree == 1: # Fully linear\n",
        "        for i in range(8):\n",
        "            cov_effect += beta_x[i] * data[covariate_names[i]]\n",
        "        time_term = beta_time * data['time_trend']\n",
        "\n",
        "    elif linearity_degree == 2: # X1, X2, X3, X4 non-linear\n",
        "        # Apply specific non-linear functions to first 4 covariates\n",
        "        cov_effect += beta_x[0] * nl_func1(data['X1']) # X1^2 (still 0 or 1)\n",
        "        cov_effect += beta_x[1] * nl_func2(data['X2']) # exp(X2/2) (more distinct for 0/1)\n",
        "        cov_effect += beta_x[2] * nl_func3(data['X3']) # abs(X3)\n",
        "        cov_effect += beta_x[3] * nl_func4(data['X4']) # sqrt(abs(X4))\n",
        "        # Linear term for remaining covariates\n",
        "        for i in range(4, 8):\n",
        "            cov_effect += beta_x[i] * data[covariate_names[i]]\n",
        "        time_term = beta_time * data['time_trend']\n",
        "\n",
        "    elif linearity_degree == 3: \n",
        "        # Apply different non-linear functions across all covariates\n",
        "        nl_funcs = [nl_func1, nl_func2, nl_func3, nl_func4, nl_func1, nl_func2, nl_func3, nl_func4] # Example pattern\n",
        "        for i in range(8):\n",
        "            cov_effect += beta_x[i] * nl_funcs[i](data[covariate_names[i]])\n",
        "        # Non-linear time trend\n",
        "        time_term = beta_time * (data['time_trend']**2)\n",
        "\n",
        "    # Combine all components for Y\n",
        "    data['Y'] = (beta_0 + beta_treated * data['treated_group'] +\n",
        "                 time_term + cov_effect +\n",
        "                 actual_cate_contribution)\n",
        "\n",
        "    # --- Add pre-trend bias ---\n",
        "    if pre_trend_bias_delta != 0:\n",
        "        pre_trend_effect = pre_trend_bias_delta * data['treated_group'] * (data['time'] - treatment_period) * (1 - data['post_treatment'])\n",
        "        data['Y'] += pre_trend_effect\n",
        "\n",
        "    # --- Add final error term ---\n",
        "    data['Y'] += data['epsilon']\n",
        "\n",
        "    # --- Final Touches ---\n",
        "    # Reorder columns for clarity\n",
        "    cols_order = ['unit_id', 'time', 'treated_group', 'post_treatment', 'propensity_score'] + \\\n",
        "                 covariate_names + ['CATE', 'Y']\n",
        "    data = data[cols_order]\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R9xVDSM3cnE7"
      },
      "outputs": [],
      "source": [
        "def find_first_treatment_indexes_array(df, min_time=4, eventually_treated=1):\n",
        "    \"\"\"\n",
        "    Finds the indexes of the first row for each treatment group (0, 1, 2, 3)\n",
        "    after filtering the DataFrame by time and eventually_treated, and returns them as a NumPy array.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame.\n",
        "        min_time: The minimum time value.\n",
        "        eventually_treated: The desired eventually_treated value.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array containing the first row indexes for each treatment group (0, 1, 2, 3),\n",
        "        or None if no rows meet the criteria. Returns -1 if a treatment group does not appear in the filtered data.\n",
        "    \"\"\"\n",
        "\n",
        "    filtered_df = df[(df['time'] >= min_time) & (df['eventually_treated'] == eventually_treated)]\n",
        "\n",
        "    if filtered_df.empty:\n",
        "        return None  # Return None if no rows match the time and eventually_treated criteria.\n",
        "\n",
        "    indexes = []\n",
        "    for group in [1, 2, 3]:\n",
        "        group_df = filtered_df[filtered_df['treatment_group'] == group]\n",
        "        if not group_df.empty:\n",
        "            indexes.append(group_df.index[0])  # Get the first index\n",
        "        else:\n",
        "            indexes.append(-1) #Return -1 if the treatment group does not appear in the filtered data.\n",
        "\n",
        "    return np.array(indexes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mRsSJjwMQWYK"
      },
      "outputs": [],
      "source": [
        "from stochtree import BCFModel\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E42jM5QRQFpd"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j01Gayr1XLfR",
        "outputId": "8529164f-d963-47fa-e597-f3d06886077d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Progress: 100%|██████████| 100/100 [2:16:38<00:00, 81.99s/iteration]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean RMSE for 100 simulations: 0.3623817774477199\n",
            "Standard Deviation RMSE for 100 simulations: 0.044681926464501205\n",
            "Mean MAE for 100 simulations: 0.28490797952675967\n",
            "Standard Deviation MAE for 100 simulations: 0.03904711264206922\n",
            "Mean MAPE for 100 simulations: 0.08507463617978843\n",
            "Standard Deviation MAPE for 100 simulations: 0.012614810761742553\n",
            "Mean RMSE for 100 simulations for post-treatment period 1: 0.3635853323792121\n",
            "Standard Deviation RMSE for 100 simulations for post-treatment period 1: 0.05353350618866636\n",
            "Mean MAE for 100 simulations for post-treatment period 1: 0.28688009030577183\n",
            "Standard Deviation MAE for 100 simulations for post-treatment period 1: 0.048859787484134415\n",
            "Mean MAPE for 100 simulations for post-treatment period 1: 0.08542889211922201\n",
            "Standard Deviation MAPE for 100 simulations for post-treatment period 1: 0.01582641682927777\n",
            "Mean RMSE for 100 simulations for post-treatment period 2: 0.3619681335101614\n",
            "Standard Deviation RMSE for 100 simulations for post-treatment period 2: 0.05349473115229942\n",
            "Mean MAE for 100 simulations for post-treatment period 2: 0.2851685595853511\n",
            "Standard Deviation MAE for 100 simulations for post-treatment period 2: 0.04557884822722133\n",
            "Mean MAPE for 100 simulations for post-treatment period 2: 0.0851249580585472\n",
            "Standard Deviation MAPE for 100 simulations for post-treatment period 2: 0.014180153600571989\n",
            "Mean RMSE for 100 simulations for post-treatment period 3: 0.35726673784949303\n",
            "Standard Deviation RMSE for 100 simulations for post-treatment period 3: 0.049896322133956346\n",
            "Mean MAE for 100 simulations for post-treatment period 3: 0.2817413328798524\n",
            "Standard Deviation MAE for 100 simulations for post-treatment period 3: 0.04230485100741821\n",
            "Mean MAPE for 100 simulations for post-treatment period 3: 0.08446457349686247\n",
            "Standard Deviation MAPE for 100 simulations for post-treatment period 3: 0.013862368950620301\n",
            "Mean RMSE for 100 simulations for post-treatment period 4: 0.3631620240735666\n",
            "Standard Deviation RMSE for 100 simulations for post-treatment period 4: 0.04805769351130802\n",
            "Mean MAE for 100 simulations for post-treatment period 4: 0.28584193533606345\n",
            "Standard Deviation MAE for 100 simulations for post-treatment period 4: 0.04207089347895499\n",
            "Mean MAPE for 100 simulations for post-treatment period 4: 0.08528012104452197\n",
            "Standard Deviation MAPE for 100 simulations for post-treatment period 4: 0.013368613599079858\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "num_x_covariates = 6\n",
        "linearity_degree=1\n",
        "\n",
        "# Set the number of iterations and initialize the counter.\n",
        "num_iterations = 100\n",
        "count_at_least_two_non_significant = 0\n",
        "\n",
        "num_pre_periods=4\n",
        "\n",
        "num_post_periods=4\n",
        "num_mcmc=500\n",
        "\n",
        "epsilon_scale=1\n",
        "\n",
        "RMSE_per_period=np.zeros([num_iterations,num_post_periods])\n",
        "MAE_per_period=np.zeros([num_iterations,num_post_periods])\n",
        "MAPE_per_period=np.zeros([num_iterations,num_post_periods])\n",
        "\n",
        "list_accumulated_p_values=[]\n",
        "\n",
        "RMSE_overall=np.zeros([num_iterations])\n",
        "MAE_overall=np.zeros([num_iterations])\n",
        "MAPE_overall=np.zeros([num_iterations])\n",
        "\n",
        "for i in tqdm(range(num_iterations), desc=\"Progress\", unit=\"iteration\"):\n",
        "    # Generate a random seed for each iteration.\n",
        "    seed_val = i\n",
        "\n",
        "    # Generate data with specified hyperparameters.\n",
        "    data_linear = generate_did_data(\n",
        "        n_units=200,\n",
        "        linearity_degree=linearity_degree,\n",
        "        num_pre_periods=num_pre_periods,\n",
        "        num_post_periods=num_post_periods,\n",
        "        pre_trend_bias_delta=0,\n",
        "        epsilon_scale=epsilon_scale,\n",
        "        seed=seed_val\n",
        "    )\n",
        "\n",
        "    data_linear['pi_hat']=0.5\n",
        "\n",
        "\n",
        "    data_linear['D']=data_linear['post_treatment']*data_linear['treated_group']\n",
        "\n",
        "    x_columns = [f\"X{i}\" for i in range(1, num_x_covariates + 1+2)]\n",
        "    X = np.array(data_linear[[\"treated_group\"] + x_columns +[\"time\"]])\n",
        "\n",
        "    Z=np.array(data_linear[\"D\"])\n",
        "    y=np.array(data_linear[\"Y\"])\n",
        "\n",
        "\n",
        "    bcf_model = BCFModel()\n",
        "    general_params = {\"keep_every\": 5, \"num_chains\": 3}\n",
        "    prognostic_forest_params = {\"keep_vars\": np.array([0, 1] + list(range(2, num_x_covariates + 3))+[num_x_covariates + 3])}\n",
        "    treatment_effect_forest_params = {\"keep_vars\": np.array([3,8]+[num_x_covariates + 3])}\n",
        "    bcf_model.sample(X_train=X, Z_train=Z, y_train=y, pi_train=np.array(data_linear['pi_hat']), num_gfr=50, num_mcmc=num_mcmc, general_params=general_params, prognostic_forest_params=prognostic_forest_params,\n",
        "                treatment_effect_forest_params=treatment_effect_forest_params)\n",
        "\n",
        "    CATE_indexes=data_linear[(data_linear['treated_group']==1) & (data_linear['post_treatment']==1)]['CATE'].index\n",
        "    Pre_indexes=data_linear[(data_linear['treated_group']==1) & (data_linear['post_treatment']==0)]['CATE'].index\n",
        "    true_CATE=data_linear[(data_linear['treated_group']==1) & (data_linear['post_treatment']==1)]['CATE']\n",
        "    estimated_CATE=bcf_model.tau_hat_train.mean(axis=1)\n",
        "    final_CATE=np.zeros([int(len(Pre_indexes)/4),num_post_periods])\n",
        "\n",
        "    for k in range(int(len(Pre_indexes)/4)):\n",
        "        estimated_CATE_individual=estimated_CATE[CATE_indexes[4*k:4*(k+1)]]\n",
        "        debias_term=0\n",
        "        final_CATE[k,:]=estimated_CATE_individual-debias_term\n",
        "\n",
        "    true_CATE=np.array(true_CATE).reshape(final_CATE.shape)\n",
        "\n",
        "    num_mcmc=500\n",
        "    final_CATE_pvalues=np.zeros([int(len(Pre_indexes)/4),num_post_periods,num_mcmc])\n",
        "\n",
        "    accumulated_p_values=np.zeros([num_post_periods,int(len(Pre_indexes)/4)])\n",
        "\n",
        "    for k in range(int(len(Pre_indexes)/4)):\n",
        "      estimated_CATE_individual=bcf_model.tau_hat_train[CATE_indexes[4*k:4*(k+1)],:]\n",
        "      debias_term=0\n",
        "      final_CATE_pvalues[k,:,:]=estimated_CATE_individual-debias_term\n",
        "      for h in range(num_post_periods):\n",
        "        mean_values=final_CATE_pvalues[k,h,:]\n",
        "        above_zero = np.sum(mean_values > 0)\n",
        "        below_zero = np.sum(mean_values < 0)\n",
        "        total_points = mean_values.size\n",
        "        percentage_above_zero = (above_zero / total_points)\n",
        "        percentage_below_zero = (below_zero / total_points)\n",
        "        accumulated_p_values[h,k]=min(percentage_above_zero, percentage_below_zero)\n",
        "\n",
        "    accumulated_p_values=accumulated_p_values.reshape(accumulated_p_values.shape[0]*accumulated_p_values.shape[1])\n",
        "    list_accumulated_p_values.append(accumulated_p_values)\n",
        "\n",
        "\n",
        "    RMSE_overall[i]=np.sqrt(np.mean((final_CATE-true_CATE)**2))\n",
        "    MAE_overall[i]=np.mean(np.abs(final_CATE-true_CATE))\n",
        "    MAPE_overall[i]=np.mean(np.abs((final_CATE-true_CATE)/true_CATE))\n",
        "\n",
        "    for h in range(num_post_periods):\n",
        "        RMSE_per_period[i,h]=np.sqrt(np.mean((final_CATE[:,h]-true_CATE[:,h])**2))\n",
        "        MAE_per_period[i,h]=np.mean(np.abs(final_CATE[:,h]-true_CATE[:,h]))\n",
        "        MAPE_per_period[i,h]=np.mean(np.abs((final_CATE[:,h]-true_CATE[:,h])/true_CATE[:,h]))\n",
        "\n",
        "\n",
        "mean_RMSE_overall=np.mean(RMSE_overall)\n",
        "mean_MAE_overall=np.mean(MAE_overall)\n",
        "mean_MAPE_overall=np.mean(MAPE_overall)\n",
        "std_RMSE_overall=np.std(RMSE_overall)\n",
        "std_MAE_overall=np.std(MAE_overall)\n",
        "std_MAPE_overall=np.std(MAPE_overall)\n",
        "\n",
        "print(f\"Mean RMSE for {num_iterations} simulations: {mean_RMSE_overall}\")\n",
        "print(f\"Standard Deviation RMSE for {num_iterations} simulations: {std_RMSE_overall}\")\n",
        "print(f\"Mean MAE for {num_iterations} simulations: {mean_MAE_overall}\")\n",
        "print(f\"Standard Deviation MAE for {num_iterations} simulations: {std_MAE_overall}\")\n",
        "print(f\"Mean MAPE for {num_iterations} simulations: {mean_MAPE_overall}\")\n",
        "print(f\"Standard Deviation MAPE for {num_iterations} simulations: {std_MAPE_overall}\")\n",
        "\n",
        "for h in range(num_post_periods):\n",
        "    print(f\"Mean RMSE for {num_iterations} simulations for post-treatment period {h+1}: {np.mean(RMSE_per_period[:,h])}\")\n",
        "    print(f\"Standard Deviation RMSE for {num_iterations} simulations for post-treatment period {h+1}: {np.std(RMSE_per_period[:,h])}\")\n",
        "    print(f\"Mean MAE for {num_iterations} simulations for post-treatment period {h+1}: {np.mean(MAE_per_period[:,h])}\")\n",
        "    print(f\"Standard Deviation MAE for {num_iterations} simulations for post-treatment period {h+1}: {np.std(MAE_per_period[:,h])}\")\n",
        "    print(f\"Mean MAPE for {num_iterations} simulations for post-treatment period {h+1}: {np.mean(MAPE_per_period[:,h])}\")\n",
        "    print(f\"Standard Deviation MAPE for {num_iterations} simulations for post-treatment period {h+1}: {np.std(MAPE_per_period[:,h])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63RBlLp5ME6-",
        "outputId": "9d319941-3893-4f65-9d27-2927f9ed516f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics data saved to sheet 'Metrics' in adapted_BCF_CATE_PS_and_PValues_linearity=1.xlsx\n",
            "P-Values data saved to sheet 'P_Values' in adapted_BCF_CATE_PS_and_PValues_linearity=1.xlsx\n"
          ]
        }
      ],
      "source": [
        "# === Sheet 1: Metrics Data ===\n",
        "\n",
        "# 1. Prepare data dictionary for metrics (overall and per_period)\n",
        "metrics_data_dict = {\n",
        "    'RMSE_overall': RMSE_overall,\n",
        "    'MAE_overall': MAE_overall,\n",
        "    'MAPE_overall': MAPE_overall,\n",
        "}\n",
        "\n",
        "# 2. Flatten the _per_period arrays into columns\n",
        "for i in range(num_post_periods):\n",
        "    metrics_data_dict[f'RMSE_period_{i}'] = RMSE_per_period[:, i]\n",
        "    metrics_data_dict[f'MAE_period_{i}'] = MAE_per_period[:, i]\n",
        "    metrics_data_dict[f'MAPE_period_{i}'] = MAPE_per_period[:, i]\n",
        "\n",
        "# 3. Create the first DataFrame for metrics\n",
        "df_metrics = pd.DataFrame(metrics_data_dict)\n",
        "\n",
        "# === Sheet 2: P-Values Data (Handling Variable Lengths) ===\n",
        "\n",
        "df_p_values = None # Initialize in case the list is empty\n",
        "\n",
        "if not list_accumulated_p_values:\n",
        "    print(\"Warning: 'list_accumulated_p_values' is empty. P-Value sheet will not be created.\")\n",
        "else:\n",
        "    # 1. Find the maximum length of the p-value vectors\n",
        "    max_len = 0\n",
        "    for vec in list_accumulated_p_values:\n",
        "        # Check if the element is actually a numpy array or list-like\n",
        "        if hasattr(vec, '__len__'):\n",
        "             max_len = max(max_len, len(vec))\n",
        "        # else: handle potential non-iterable elements if necessary\n",
        "\n",
        "    if max_len == 0 and list_accumulated_p_values:\n",
        "         print(\"Warning: list_accumulated_p_values contains elements but none have length > 0.\")\n",
        "         # Decide how to handle this - maybe create an empty df?\n",
        "\n",
        "    # 2. Create padded data\n",
        "    padded_p_values = []\n",
        "    for i, vec in enumerate(list_accumulated_p_values):\n",
        "         # Ensure vec is treated as an iterable, default to empty if not applicable\n",
        "        current_vec = []\n",
        "        if hasattr(vec, '__len__'):\n",
        "            current_vec = list(vec) # Convert numpy array to list for easy padding\n",
        "\n",
        "        # Create a padded row with NaN for missing values\n",
        "        padded_row = current_vec + [np.nan] * (max_len - len(current_vec))\n",
        "        padded_p_values.append(padded_row)\n",
        "\n",
        "    # 3. Create column names for the p-values sheet\n",
        "    p_value_columns = [f'p_value_{i}' for i in range(max_len)]\n",
        "\n",
        "    # 4. Create the second DataFrame for p-values\n",
        "    df_p_values = pd.DataFrame(padded_p_values, columns=p_value_columns)\n",
        "\n",
        "# === Save to Excel File with Multiple Sheets ===\n",
        "\n",
        "output_filename_excel = 'DiD_BCF_CATE_PS_and_PValues_linearity=1.xlsx'\n",
        "\n",
        "# Use ExcelWriter to write multiple DataFrames to different sheets\n",
        "try:\n",
        "    with pd.ExcelWriter(output_filename_excel, engine='openpyxl') as writer:\n",
        "        # Write the metrics DataFrame to the first sheet\n",
        "        df_metrics.to_excel(writer, sheet_name='Metrics', index=False, float_format='%.6f')\n",
        "        print(f\"Metrics data saved to sheet 'Metrics' in {output_filename_excel}\")\n",
        "\n",
        "        # Write the p-values DataFrame to the second sheet (if it exists)\n",
        "        if df_p_values is not None:\n",
        "            df_p_values.to_excel(writer, sheet_name='P_Values', index=False, float_format='%.6f')\n",
        "            print(f\"P-Values data saved to sheet 'P_Values' in {output_filename_excel}\")\n",
        "        else:\n",
        "             # Optionally create an empty sheet or just skip\n",
        "             print(\"P-Values sheet was not created as the source list was empty or contained no vectors.\")\n",
        "\n",
        "\n",
        "except ImportError:\n",
        "    print(\"\\nError: Cannot write Excel file. Please install the 'openpyxl' library.\")\n",
        "    print(\"You can install it using: pip install openpyxl\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while writing the Excel file: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9Uc6edAPlcR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
